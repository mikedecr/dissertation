## Testing the Model with Simulated Data {#sec:test-static}

<!-- in chapter: model -->


```{r stopper, eval = FALSE, cache = FALSE, include = FALSE}
knitr::knit_exit()
```

```{r knitr-02-1_model-sim, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

```{r r-02-1_model-sim, cache = FALSE}
library("here")
library("magrittr")
library("tidyverse")
# library("boxr"); box_auth()

library("scales")
library("english")
library("latex2exp")
library("ggforce")

library("broom")
library("tidybayes")
```

```{r sim-dirs}
input_dir <- file.path("data", "mcmc", "2-dgirt", "test", "input")
mcmc_dir <- file.path("data", "mcmc", "2-dgirt", "test", "samples")
```

```{r 021-helper-vars}
# helper variables
sim_seed <- 01110011
```



```{r read-simulation-data}
# (1) read objects as list
# (2) save in GlobalEnv
# (3) remove list

# box_read(524570617700)
list.files(input_dir)
here(input_dir, "sim-params.RDS") %>%
  read_rds() %>% 
  list2env(envir = .GlobalEnv)

ls()
```

Because the model is custom-built using Stan, it comes with no off-the-shelf quality assurances. 
To test the model's ability to recover unknown parameters, I estimate the model on simulated data. 
I intentionally stress-test the model by building a dataset that is smaller than the real dataset that I use for estimation.
The fake data contain just `r n_states` with `r english(n_districts)` districts per state.
With two parties per district, this totals `r n_states * n_districts * n_parties` groups instead of the $435 \times 2 = 870$ groups in the real data.
Individuals in each group offer responses to `r n_items` items.
For simplicity, the simulation assumes that each individual in each group answers only one question.
When the model is estimated on real data, this assumption is relaxed by the weighting scheme laid out in Section \@ref(sec:model-weights).
Because the weighting scheme downweights respondents who answer multiple survey items, I generate a small number of item responses for each item in each group: just `r n_cases` independent responses per item. 





The model is designed to estimate group-level parameters from an individual-level data generating process. As such, I begin by simulating individual-level item response data before aggregating the data to the group level for estimation. I simulate a universe containing `r n_states` states nested within `r english(n_regions)` regions, allocated so each region contains `r english(n_states / n_regions)` states. Each state contains `r english(n_districts)` districts containing voters belonging to `r english(n_parties)` parties, totaling `r n_states * n_districts * n_parties` groups across `r n_states * n_districts` districts in the whole "country." Data for each district-party group contain responses to `r n_items` items that are answered by `r n_cases` unique individuals apiece.^[
  While a single survey will not contain `r n_items` policy items, this study will pool items from several surveys into a final dataset.
]

```{r party-params}
party_params <- params %>% split(.$party) %>% print()
```

I draw item parameters from independent Normal distributions: $\text{Normal}(0, `r difficulty_sd`)$ for the unadjusted midpoint parameters and $\text{Normal}\left(0, `r discrimination_sd`\right)$ for the unadjusted log discrimination parameters.
The hierarchical model does not contain a lot of data: I generate just `r english(n_distcov)` district-level covariates and `r english(n_statecov)` state covariate.
District and state covariates are random draws from a Normal distribution with standard deviation `r sd_beta_d`, with separate coefficients for "Democrats" and "Republicans."
Intercept values for Democrats and Republicans are `r party_params[[1]]$const` and `r party_params[[2]]$const`, respectively.
The random effects for districts, states, and `r english(n_regions)` regions are Normal draws with standard deviations of `r resid_theta_d`, `r resid_theta_s`, and `r resid_theta_r`.


```{r read-test-stanfit}
# box_read(507153233559)
stanfit <- read_rds(here("data", "mcmc", "2-dgirt", "test", "samples", "SMOL_probit-lkj-2k.RDS"))
```

```{r tidybayes-tidy}
df_stanfit <- tidy_draws(stanfit)
```

```{r read-test-stanpars, cache = FALSE}
here("data", "mcmc", "2-dgirt", "test", "input", "mcmc-params.RDS") %>%
  read_rds() %>% 
  list2env(envir = .GlobalEnv)

total_iterations <- (n_chains * (n_iterations - n_warmup) / n_thin)
```

I estimate the model with MCMC, running `r english(n_chains)` for `r n_iterations`, using the first `r n_warmup` iterations for an adaptive warmup period.
This results in `r comma(total_iterations)` samples for each parameter in total.

```{r read-true-thetas}
group_level <- read_rds(
  here("data", "mcmc", "2-dgirt", "test", "input", "group-level-data.RDS")
)
```

```{r tidy-conf-level}
# saving this to print it later in the text
conf_level <- 0.9
```

```{r tidy-stanfit}
tidy_stanfit <- tidy(stanfit, conf.int = TRUE, conf.level = conf_level)
```

```{r save-thetas}
thetas <- tidy_stanfit %>%
  filter(
    str_detect(term, "theta\\[") == TRUE, 
    str_detect(term, "idtheta") == FALSE
  ) %>%
  mutate(group = parse_number(term)) %>%
  left_join(group_level %>% select(group, theta_g, sigma_g, party)) %>% 
  print()

# make a square plot, save abs_value of "biggest" theta
axis_limit <- thetas %$% max(max(theta_g), max(estimate)) %>% ceiling() %>% abs()
```

```{r plot-theta-scatters}
ggplot(thetas) +
  aes(x = theta_g, y = estimate, color = as.factor(party)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_vline(xintercept = 0, color = "gray") +
  facet_wrap(
    ~ as.factor(party),
    labeller = 
      c("1" = "Simulated Democrats",
        "2" = "Simulated Republicans") %>%
      as_labeller()
  ) +
  geom_pointrange(
    aes(ymin = (conf.low), ymax = conf.high),
    fatten = 2
  ) +
  geom_abline(color = "black") +
  labs(
    y = str_glue(
      "Model Estimate ({percent(conf_level, accuracy = 1)} intervals)"
    ),
    x = '"True" Ideal Point',
    color = NULL,
    title = 'Estimated vs. "True" Ideal Points',
    subtitle = "Results from fake data simulation"
  ) +
  scale_color_manual(values = party_factor_colors) +
  theme(legend.position = "none") +
  coord_fixed() +
  NULL
```

Figure \@ref(fig:plot-theta-scatters) compares each group's "true" ideal point to its estimated ideal point from the IRT model.
We observe a strong relationship between the true and estimated values, indicating that the model does a good job recovering the ideal points. 
We do observe a little bit of shrinkage in the ideal point estimates; the most and least conservative ideal points fall closer to zero compared to the $45^{\circ}$ line. 
This is likely a result of partial pooling in the modelâ€”without strong information about ideal point estimates themselves, the hierarchical prior hedges more extreme ideal points toward the center of the estimated distribution. 

<!-- Another source of attenuation could be the fact that the data are generated from a heteroskedastic model, but the model does not generate that heteroskedasticity.  -->
<!-- Unmodeled sources of variation add dispersion to the data, pulling the response probability toward $0.5$, which will pull ideal point estimates toward $0$ and increase the item dispersion parameters.  -->

```{r plot-theta-scatters, include = TRUE, fig.height = 5, fig.width = 8, out.width = "100%", fig.scap = "Ideal point estimates from fake data simulation.", fig.cap = "Ideal point estimates from fake data simulation. Estimates are plotted with posterior means and 90 percent compatibility intervals."}
```

```{r extract-icc-draws}
n_draws <- 50

draws <- df_stanfit %>%
  spread_draws(cutpoint[item], dispersion[item], n = n_draws, seed = sim_seed) %>%
  print()

# beepr::beep(2)
icc_draws <- draws %>%
  group_by(item) %>%
  crossing(theta = seq(-axis_limit, axis_limit, .05)) %>%
  mutate(
    pp = plogis((theta - cutpoint) / dispersion)
  ) %>%
  print()
```


```{r read-ij-data}
ij_level <- read_rds(
  here("data", "mcmc", "2-dgirt", "test", "input", "ij-level-data.RDS")
)
```


```{r sample_items}
n_items_sampled <- 10
sampled_items <- 
  sample(ij_level$item, size = n_items_sampled, replace = FALSE) %>%
  round(1) %>%
  as.integer()
```

```{r plot-sim-ICCs}
icc_draws %>%
  filter(item %in% sampled_items) %>%
  ggplot() +
  aes(x = theta, y = pp) +
  geom_vline(xintercept = 0, color = "gray", size = 0.25) +
  geom_hline(yintercept = 0.5, color = "gray", size = 0.25) +
  geom_line(aes(group = .draw), color = primary, size = 0.2) +
  geom_line(
    data = filter(ij_level, item %in% sampled_items), 
    color = "black", 
    size = 1.5,
    aes(x = theta_i, y = pi_ij)
  ) +
  facet_wrap(
    ~ item,
    labeller = as_labeller(function(x) str_glue("Item {x}")),
    ncol = 5
  ) +
  coord_cartesian(xlim = c(-1 * axis_limit, axis_limit), ylim = c(0, 1)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  labs(
    x = "Ideal Point", 
    y = "Conservative Response Probability",
    title = 'Item Response Functions from Fake Data Simulation',
    subtitle = str_glue("{n_draws} posterior samples vs. true item response function")
  )
```

The other important result from the fake data simulation is the item response functions (IRFs).
Figure \@ref(fig:plot-sim-ICCs) plots estimated IRFs from `r n_draws` MCMC draws (light orange) against the true IRF (black). 
These results also testify to the model's ability to recover key parameters even under weak data settings.
In the few cases where the estimated IRF departs slightly from the true IRF, this again appears to be due to partial pooling.
In particular, the model is pooling the largest-magnitude midpoint parameters toward the mean of the distribution, which is restricted to be zero. 

```{r plot-sim-ICCs, include = TRUE, fig.height = 6, fig.width = 9, out.width = "100%", fig.scap = "Item response functions from fake data simulation.", fig.cap = "Item response functions from fake data simulation. Comparison between estimated (yellow) and true (black) item parameters for a random sample of items."}
```

It is important to remember that the bias introduced by partial pooling is intentional.
Partial pooling, like all forms of regularization, is intended to stabilize parameters that are estimated with weak signals from data.
Without partial pooling, the model must estimate each ideal point or item parameter with no "memory" of the other parameters in the model.
Although less regularization leads to less bias (all else equal), estimates will have higher variance especially in highly parameterized measurement models.
Pooling toward hierarchical distributions can be particularly valuable for item response models, where each additional observation introduces additional parameters for items or for respondents [@bailey:2001:ranef-ideal].

```{r item-hyper-prepost, eval = FALSE}
df_stanfit %>%
  spread_draws(item_scales[dim], item_rho) %>% 
  gather(key = param, value = value, contains('item_')) %>%
  mutate(
    param = 
      case_when(
        dim == 1 & param == "item_scales" ~ "sd_cut",
        dim == 2 & param == "item_scales" ~ "sd_logdisc",
        TRUE ~ "item_rho"
      )
  ) %>%
  ggplot() +
  aes(x = value) +
  facet_grid(~ param, scales = "free_x") +
  geom_ribbon(
    data = tibble(
      x = seq(-1, 3, .05),
      dbeta = dbeta((x + 1) / 2, 2, 2) / 2,
      dhalfnorm = dnorm(x) * 2
    ) %>%
    crossing(param = c("item_rho", "sd_cut", "sd_logdisc")) %>%
    mutate(
      density = case_when(
        param == "item_rho" ~ dbeta,
        str_detect(param, "sd") & x > 0 ~ dhalfnorm
      ) 
    ) %>%
    filter(
      (x > 0 & param != "item_rho") |
      (x <= 1 & param == "item_rho")
    ),
    aes(ymax = density, ymin = 0, x = x),
    fill = primary, alpha = 0.5
  ) +
  geom_histogram(
    aes(y = ..density..), 
    bins = 100,
    fill = primary, color = NA
  )
```


```{r item-hyper-scatter, eval = FALSE}
df_stanfit %>%
  spread_draws(dispersion[item], cutpoint[item], n = 500) %>%
  ggplot() +
  aes(x = cutpoint, y = log(1 / dispersion), fill = as.factor(item)) +
  geom_mark_ellipse(
    expand = unit(3, "mm"),
    color = NA,
    alpha = 0.1,
    label.family = font_fam,
    label.fontface = "plain"
  ) +
  ggpointdensity::geom_pointdensity() +
  scale_fill_viridis_d(option = "plasma") +
  scale_color_viridis_c(option = "plasma") +
  NULL
```


<!-- item parameters
covariate values
residual values -->



<!------- TO DO ---------
- still need to compare the hierarchical distributions
------------------------->


<!-- Distill the simulation stuff here 

- Static, homoskedastic
- Static and heteroskedastic
- Heteroskedastic and dynamic

-->



