## Testing the Model with Simulated Data {#sec:test-static}

<!-- in chapter: model -->


```{r stopper, eval = FALSE, cache = FALSE, include = FALSE}
knitr::knit_exit()
```

```{r knitr-02-1_model-sim, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

```{r r-02-1_model-sim, cache = FALSE}
library("here")
library("magrittr")
library("tidyverse")
# library("boxr"); box_auth()

library("scales")
library("english")
library("latex2exp")
library("ggforce")

library("broom")
library("tidybayes")
```

```{r sim-dirs}
input_dir <- file.path("data", "mcmc", "2-dgirt", "test", "input")
mcmc_dir <- file.path("data", "mcmc", "2-dgirt", "test", "samples")
```

```{r 021-helper-vars}
# helper variables
sim_seed <- 01110011
```



```{r read-simulation-data}
# (1) read objects as list
# (2) save in GlobalEnv
# (3) remove list

# box_read(524570617700)
list.files(input_dir)
here(input_dir, "sim-params.RDS") %>%
  read_rds() %>% 
  list2env(envir = .GlobalEnv)

ls()
```

Because the model is custom-built using Stan, it comes with no off-the-shelf quality assurances. 
To test the model's ability to recover unknown parameters, I estimate the model on simulated data [@gelman-et-al:2013:BDA p. 270].
I intentionally stress-test the model by building a simulated dataset that is smaller than the real dataset that I use for estimation.
The fake data contain just `r n_states` states with `r english(n_districts)` districts per state.
With two parties per district, this totals `r n_states * n_districts * n_parties` groups instead of the $435 \times 2 = 870$ groups in the real data.
Individuals in each group offer responses to `r n_items` items, which is fewer than the number of items I collect from real data.
For simplicity, the simulation assumes that each individual in each group answers only one question.
When the model is estimated on real data, this assumption is relaxed by the weighting scheme laid out in Section \@ref(sec:model-weights).
Because the weighting scheme downweights respondents who answer multiple survey items, I generate a small number of independent responses for each item in each group: just `r n_cases` independent responses per item.

```{r party-params}
party_params <- params %>% split(.$party) %>% print()
```

I draw item parameters from independent Normal distributions: $\text{Normal}(0, `r difficulty_sd`)$ for the unadjusted midpoint parameters and $\text{Normal}\left(0, `r discrimination_sd`\right)$ for the unadjusted log discrimination parameters.
The hierarchical model does not contain a lot of data: I generate just `r english(n_distcov)` district-level covariates and `r english(n_statecov)` state covariate.
District and state covariates are random draws from a Normal distribution with standard deviation `r sd_beta_d`, with separate coefficients for the fake "Democrats" and "Republicans."
Intercept values for Democrats and Republicans are fixed at `r party_params[[1]]$const` and `r party_params[[2]]$const`, respectively.
The random effects for districts, states, and `r english(n_regions)` regions are Normal draws with standard deviations of `r resid_theta_d`, `r resid_theta_s`, and `r resid_theta_r`.


```{r read-test-stanfit}
# box_read(507153233559)
stanfit <- read_rds(here(mcmc_dir, "SMOL_probit-lkj-2k.RDS"))
```

```{r tidybayes-tidy}
df_stanfit <- tidy_draws(stanfit)
```

```{r read-test-stanpars, cache = FALSE}
here("data", "mcmc", "2-dgirt", "test", "input", "mcmc-params.RDS") %>%
  read_rds() %>% 
  list2env(envir = .GlobalEnv)

total_iterations <- (n_chains * (n_iterations - n_warmup) / n_thin)
```

I estimate the model with MCMC, running `r english(n_chains)` chains for `r n_iterations` iterations apiece, using the first `r n_warmup` iterations in each chain for an adaptive warmup period.
This results in `r comma(total_iterations)` saved samples for each parameter.

```{r read-true-thetas}
group_level <- read_rds(
  here("data", "mcmc", "2-dgirt", "test", "input", "group-level-data.RDS")
)
```

```{r tidy-conf-level}
# saving this to print it later in the text
conf_level <- 0.9
```

```{r tidy-stanfit}
tidy_stanfit <- tidy(stanfit, conf.int = TRUE, conf.level = conf_level)
```

```{r save-thetas}
thetas <- tidy_stanfit %>%
  filter(
    str_detect(term, "theta\\[") == TRUE, 
    str_detect(term, "idtheta") == FALSE
  ) %>%
  mutate(group = parse_number(term)) %>%
  left_join(group_level %>% select(group, theta_g, sigma_g, party)) %>% 
  mutate(
    across(
      .cols = c(estimate, conf.low, conf.high),
      .fns = ~ (. - mean(estimate)) / sd(estimate)
    ),
    theta_g_id = (theta_g - mean(theta_g)) / sd(theta_g)
  ) %>%
  print()
```

```{r plot-theta-scatters}
ggplot(thetas) +
  aes(x = theta_g_id, y = estimate, color = as.factor(party)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_vline(xintercept = 0, color = "gray") +
  facet_wrap(
    ~ as.factor(party),
    labeller = 
      c("1" = "Simulated Democrats",
        "2" = "Simulated Republicans") %>%
      as_labeller()
  ) +
  geom_pointrange(
    aes(ymin = (conf.low), ymax = conf.high),
    fatten = 2
  ) +
  geom_abline(color = "black") +
  labs(
    y = str_glue(
      "Estimated ideal point (standardized)"
    ),
    x = '"True" ideal point (standardized)',
    color = NULL,
    title = 'Estimated vs. "True" Ideal Points',
    subtitle = "Results from fake data simulation"
  ) +
  scale_color_manual(values = party_factor_colors) +
  theme(legend.position = "none") +
  coord_fixed() +
  NULL
```

Figure \@ref(fig:plot-theta-scatters) compares each group's "true" ideal point to its estimated ideal point from the IRT model.
Because the latent ideological space is not identified, I facilitate the comparison of ideal points by standardizing both sets to be mean zero and variance of $1$. 
The model recovers a strong correspondence between the true and estimated ideal point values, underscoring the computational fidelity of the model and the statistical precision to detect meaningful variation in the underlying data.

```{r plot-theta-scatters, include = TRUE, fig.height = 5, fig.width = 10, out.width = "100%", fig.scap = "Ideal point estimates from fake data simulation.", fig.cap = "Ideal point estimates from fake data simulation. Estimates are plotted with posterior means and 90 percent compatibility intervals."}
```

```{r extract-icc-draws}
n_draws <- 50

draws <- df_stanfit %>%
  spread_draws(cutpoint[item], dispersion[item], n = n_draws, seed = sim_seed) %>%
  print()

# beepr::beep(2)
icc_draws <- draws %>%
  group_by(item) %>%
  crossing(theta = seq(min(thetas$theta_g), max(thetas$theta_g), .05)) %>%
  mutate(
    pp = plogis((theta - cutpoint) / dispersion)
  ) %>%
  print()
```


```{r read-ij-data}
ij_level <- read_rds(
  here("data", "mcmc", "2-dgirt", "test", "input", "ij-level-data.RDS")
)
```


```{r sample_items}
n_items_sampled <- 10
sampled_items <- 
  sample(ij_level$item, size = n_items_sampled, replace = FALSE) %>%
  round(1) %>%
  as.integer()
```

```{r plot-sim-ICCs}
icc_draws %>%
  filter(item %in% sampled_items) %>%
  ggplot() +
  aes(x = theta, y = pp) +
  geom_vline(xintercept = 0, color = "gray", size = 0.25) +
  geom_hline(yintercept = 0.5, color = "gray", size = 0.25) +
  geom_line(aes(group = .draw), color = primary, size = 0.2) +
  geom_line(
    data = filter(ij_level, item %in% sampled_items), 
    color = "black", 
    aes(x = theta_i, y = pi_ij)
  ) +
  facet_wrap(
    ~ item,
    labeller = as_labeller(function(x) str_glue("Item {x}")),
    ncol = 5
  ) +
  scale_x_continuous(limits = c(-3, 3)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  labs(
    x = "Ideal Point", 
    y = "Conservative Response Probability",
    title = 'Item Response Functions from Fake Data Simulation',
    subtitle = str_glue("{n_draws} posterior samples of {n_items_sampled} random items vs. true IRFs")
  )
```

The other important result from the fake data simulation is the item response functions (IRFs).
Figure \@ref(fig:plot-sim-ICCs) plots IRFs from `r n_items_sampled` randomly chosen items, showing `r n_draws` MCMC draws of the estimated IRFs (light orange) against the true IRF (black). 
These results also testify to the model's ability to recover key parameters even under weak data settings.
In the few cases where the estimated IRF departs slightly from the true IRF, this appears to be due to partial pooling: regularization of item parameters toward their estimated distribution.

```{r plot-sim-ICCs, include = TRUE, fig.height = 6, fig.width = 9, out.width = "100%", fig.scap = "Item response functions from fake data simulation.", fig.cap = "Item response functions from fake data simulation. Comparison between estimated (orange) and true (black) item parameters for a random sample of items."}
```

It is important to remember that the bias introduced by partial pooling is intentional.
Partial pooling, like all forms of regularization, is intended to stabilize parameters that are estimated with weak signals from data.
Without partial pooling, the model must estimate each item parameter with no "memory" of the other item parameters in the model.
Although less regularization leads to less bias (all else equal), estimates will have higher variance especially in measurement models that have many parameters.
Pooling toward hierarchical distributions can be particularly valuable for item response models, for which each additional observation introduces either two additional item parameters or an additional ideal point parameter [@bailey:2001:ranef-ideal].

```{r item-hyper-prepost, eval = FALSE}
df_stanfit %>%
  spread_draws(item_scales[dim], item_rho) %>% 
  gather(key = param, value = value, contains('item_')) %>%
  mutate(
    param = 
      case_when(
        dim == 1 & param == "item_scales" ~ "sd_cut",
        dim == 2 & param == "item_scales" ~ "sd_logdisc",
        TRUE ~ "item_rho"
      )
  ) %>%
  ggplot() +
  aes(x = value) +
  facet_grid(~ param, scales = "free_x") +
  geom_ribbon(
    data = tibble(
      x = seq(-1, 3, .05),
      dbeta = dbeta((x + 1) / 2, 2, 2) / 2,
      dhalfnorm = dnorm(x) * 2
    ) %>%
    crossing(param = c("item_rho", "sd_cut", "sd_logdisc")) %>%
    mutate(
      density = case_when(
        param == "item_rho" ~ dbeta,
        str_detect(param, "sd") & x > 0 ~ dhalfnorm
      ) 
    ) %>%
    filter(
      (x > 0 & param != "item_rho") |
      (x <= 1 & param == "item_rho")
    ),
    aes(ymax = density, ymin = 0, x = x),
    fill = primary, alpha = 0.5
  ) +
  geom_histogram(
    aes(y = ..density..), 
    bins = 100,
    fill = primary, color = NA
  )
```


```{r item-hyper-scatter, eval = FALSE}
df_stanfit %>%
  spread_draws(dispersion[item], cutpoint[item], n = 500) %>%
  ggplot() +
  aes(x = cutpoint, y = log(1 / dispersion), fill = as.factor(item)) +
  geom_mark_ellipse(
    expand = unit(3, "mm"),
    color = NA,
    alpha = 0.1,
    label.family = font_fam,
    label.fontface = "plain"
  ) +
  ggpointdensity::geom_pointdensity() +
  scale_fill_viridis_d(option = "plasma") +
  scale_color_viridis_c(option = "plasma") +
  NULL
```




<!------- TO DO ---------
- still need to compare the hierarchical distributions
------------------------->





