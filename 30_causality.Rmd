# Bayesian Causal Models for Political Science {#ch:causality}

<!------- TO DO ---------
- why no build with tufte?
------------------------->

```{r knitr-03-causality, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

<!-- who knows if this works on next latex build -->
$\renewcommand{\ind}[0]{\perp \!\!\! \perp}$
$\renewcommand{\doop}[1]{\mathit{do}\left(#1\right)}$
$\renewcommand{\diff}[1]{\, \mathrm{d}#1}$
$\renewcommand{\E}[1]{\mathbb{E}\left[#1\right]}$
$\renewcommand{\p}[1]{p\left(#1\right)}$



```{r r-03-causality, cache = FALSE}
library("knitr")
library("here")
library("ggdag")
library("magrittr")
library("tidyverse")
library("extrafont")
library("latex2exp")
library("scales")
library("patchwork")
```


<!------- TO DO ---------
This might need to be chapter 2!!!! 

- The model has a discussion of priors that might feel out of place
  without first couching it in a viewpoint of Bayes for the project.
- Is there only one viewpoint? No? 
- The model is a "pragmatic Bayes" (MCMC for fitting, structural priors),
  whereas the causal inf stuff is mixed in its view.
  Sometimes its "normalizing Bayes" (information for sensitivity testing),
  other times is merely "structural" WIPs. 
  Maybe one thing to articulate (to myself, to others) is a position on how
  structural priors make us think about properties of Bayes estimators.
  (in the RDD case, the OLS model is consistent but we aren't in asymptopia.
   How else do we evaluate the properties of the Bayes estimator
   when we include structural information?)
------------------------->


<!------- TO DO ---------
- is the intro too specific?
- Have an abstract and then an outline of the chapter?
- missing right now: connect to political science!
------------------------->

Before I employ the estimates of party-public ideology obtained in Chapter \@ref(ch:model), this chapter discusses a Bayesian framework for causal inference in political science. 
This framework addresses two major themes in the empirical problems that I confront later in the project, among several other minor themes.

First, this project views causal inference as a problem of posterior predictive inference.
Causal models are tools that enable inferences about missing data: the data that would be observed if a key independent variable could be set to a different value. 
The unobserved data are "unobserved potential outcomes" in the Rubin causal framework or "counterfactual observations" in the Pearl framework.
<!------- TO DO ---------
- Rubin and Pearl cites
------------------------->
Regardless of the notational/semantic conventions employed, "Bayesian causal inference" is a modeling framework for structuring the inferences about the probability distribution of unobserved potential outcomes, having observed one set of potential outcomes.
In other words, which counterfactual data are plausible, given the observed data and a model relating the data to causal queries of interest? In this sense, the Bayesian approach to causal inference confronts our uncertainties about the "left-hand side" of our model.


Second, the Bayesian approach confronts our uncertainties about the "right-hand side" of the model as well.
Causal estimands (to use the Rubin terminology) are comparisons of potential outcomes are two hypothetical values of a treatment.
In many cases, these estimands are comparisons of a unit's outcome under an observed treatment against an unobserved treatment.
For this project, the treatment of interest---ideology in district-party-publics---is not directly observed.
It is instead only observed up to a probability distribution from a measurement model.
(To use potential outcomes notation, instead of observing $Y(\pi)$, we observe $Y(p(\pi))$).
Bayesian probabilistic models provide machinery to describe causal queries when we have only a probabilistic understanding of the observed data as well as the unobserved data.

This chapter unpacks these issues according to the following outline.
First, I review the notation and terminology for causal modeling in empirical research, where data and causal estimands are posed in terms of "potential outcomes" or "counterfactual" observations. 
I then describe a Bayesian reinterpretation of these models, which uses probability distributions to quantify uncertainty about causal effects and counterfactual data, conditional on the observed data. 
Because Bayesian modeling remains largely foreign to political science,
I confront several issues surrounding prior distributions for causal models.
I discuss how priors are inescapable for many causal claims, how priors can provide important structure to improve causal inferences, and practical advice for constructing and evaluating priors.
Finally, I provide examples of the Bayesian causal framework at work, highlighting how priors add value to causal inference at different levels of abstraction.






## Overview of Key Concepts


### Causal Models {#sec:causal-inf}

As an area of scientific development, _causal inference_ refers to the formal modeling of causal effects, the assumptions required to identify causal effects, and research designs that make these assumptions plausible.
The rigorous accounting of the causal model and identifying assumptions distinguishes causal inference from informal, verbal discussions of causal effects.

The current movement in causal inference spans several fields, most notably in statistics, biostatistics, epidemiology, computer science, and economics.
The dominant modeling approach to causal inference in political science is rooted in a notation of _potential outcomes_ [@rubin:1974:potential-outcomes; @rubin:2005:potential-outcomes].
This "Rubin model" formalizes the concept of a causal effect by first defining a space of hypothetical outcomes.
The outcome variable $Y$ for unit $i$ is a function of a treatment variable $A$.
"Treatment" refers only to a causal factor of interest, regardless of whether the treatment is randomly assigned.^[
  Some causal inference literatures refer to treatments as "exposures," which may feel more broadly applicable to settings beyond experiments. For this project, I make no distinction between treatments and exposures.
]
For a binary treatment assignment where $A = 1$ represents treatment and $A = 0$ represents control, unit $i$'s outcome under treatment is represented as $Y_{i}(A = 1)$ or $Y_{i}(1)$, and the outcome under control would be $Y_{i}(A = 0)$ or $Y_{i}(0)$.
The benefit of expressing $Y$ in terms of hypothetical values of $A$ allows the causal model to describe, with formal exactitude, the entire space of possible outcomes that result from treatment assignment.
If $Y_{i}(A = 1)$ differs from $Y_{i}(A = 0)$, then the treatment has a nonzero causal effect on unit $i$, denoted $\tau_{i}$.
\begin{align}
  (\#eq:tau-i)
  \tau_{i} &= Y_{i}(A = 1) - Y_{i}(A = 0)
\end{align}
Defining the causal model in terms of unit-level effects provides an exact, minimally sufficient definition of a causal effect: $A$ affects $Y$ treatment has a nonzero effect for any unit.

By establishing this baseline model, the researcher can derive exact theoretical statements about causal effects by manipulating the equations that define the model.
A causal model may describe more complex causal effects, such as whether $Y_{i}$ it is observed at all, whether the treatment effect is indirectly mediated by another variable, whether the effect depends on other baseline characterics of units, and so on. 

The entire space of potential outcomes is a hypothetical device. 
Although a causal model defines potential outcomes for every unit under every treatment assignment, it is not possible to observe all of these potential outcomes, since a unit can only receive one treatment, and thus can only take a single outcome value. 
This implies that the individual causal effect ($\tau_{i}$), while a valid feature of the hypothetical causal model, is never actually observed be for any unit.
This "fundamental problem of causal inference" is the core philosophical problem in causal inference; because the researcher never observes a unit under more than one treatment status, she can never make any causal claims based on the observed data alone [@holland:1986:causal-inf].
Causal claims are possible only by imposing assumptions on the data that allow the researcher to predict what the data would look like if units received treatments that they did not actually receive.
These "identification assumptions" specify the conditions under which data from one treatment group can be used to make generalizations about data from other treatment groups [@keele:2015:causal-inf].

<!------- TO DO ---------
- identification assumptions (oganisian and roy style)
------------------------->

A related feature of causal models is that the causal effect $\tau_{i}$ is defined at the unit level, so it can be different for each unit.
This low-level causal heterogeneity is often an important feature of research designs, since the identifiable causal quantities will depend on which assumptions the researcher is willing to make about the structure of that heterogeneity.
The body of work that we call "causal inference" focuses on (1) which the causal quantities are described by a causal model, (2) the assumptions required to identify those quantities in observed data and the research designs that invoke them, and (3) estimation methods for those causal quantities.

A separate but increasingly popular approach to causal modeling focuses on _structural causal models_ or "SCMs" [@pearl:1995:causal-diagrams; @pearl:2009:causality] that describe networks of causally related variables.
Some treatment $A$ causally affects the outcome $Y$ if the treatment is a component of the function that assigns values to the outcome: $Y_{i} := f(A_{i}, \ldots)$.
The notion of _assignment_ is important to distinguish from _equality_, since it distinguishes a directional causal effect from a bidirectional correlation.
This is a central feature of the SCM approach, which is premised on the difference between _observing_ relationships as they appear in the world and _obtaining_ relationships by intervening on the world.
We can observe a relationship between outcome $Y$ and treatment $A$ by observing that the distribution of $Y$ given $A$, $p(Y \mid A = a)$, changes as $A$ takes different values, but we would observe a different relationship by intervening on the world to set the value of $A$ ourselves.
\begin{align}
  \begin{split}
    \text{Observational Distribution:} &\; & p(Y \mid A = a) \\
    \text{Interventional Distribution:} &\; & p(Y \mid \do{A = a}) 
  \end{split}
  (\#eq:obs-int-distributions)
\end{align}
Like the potential outcomes model, $A$ has a causal effect on $Y$ if it changes at least one unit's $Y$ value, since it only takes one unit to change the probability distribution $p(Y \mid \do{A = a})$.
The SCM framework comes to life by implementing _do_-calculus [@pearl:1995:causal-diagrams], rules for conditioning on variables in the causal system to nonparametrically identify causal effects.
The SCM approach is important for this dissertation because its use of probability distributions to convey causal effects lends itself to Bayesian modeling more readily than the potential outcomes notation.

```{r confounding-dag}
problem_dag <- 
  dagify(
    Y ~ A + U,
    A ~ U,
    exposure = "X",
    outcome = "Y",
    coords = tribble(
      ~ name,      ~ x,    ~ y,
      "Y", 1, 0,
      "A", 0, 0,
      "U", 0, 1
    )
  ) %>%
  tidy_dagitty() %>%
  # node_parents("C") %>%
  print()
```

```{r plot-confounding-dag}
ggplot(problem_dag) +
  aes(x = x, y = y, xend = xend, yend = yend) +
  geom_dag_edges() +
  geom_dag_point(aes(color = (name == "U"))) +
  geom_dag_text(
    aes(label = name), 
    parse = TRUE, 
    family = font_fam
  ) +
  scale_color_manual(values = c("TRUE" = secondary, "FALSE" = "black")) +
  annotate(
    geom = "text",
    label = "Effect of treatment (A → Y) is identified\nconditioning on confounder (U)",
    x = 0.1, y = 1.2,
    size = 3,
    hjust = 0
  ) +
  theme_dag(legend.position = "none") +
  expand_plot(
    expand_x = expand_scale(c(0.2, 0.2)), 
    expand_y = expand_scale(c(0.2, 0.2))
  ) +
  NULL  
```

The most recognizable feature of the SCM framework is that every structural model can be visualized as a graphical model, where nodes represent variables and directed edges represent causal relationships. Many researchers refer to these graphs as "directed acyclic graphs," but I refer to them simply as "causal graphs."^[
  This is an effort to reduce jargon. 
  DAGs can be used to represent many systems that have nothing to do with causal inference, since "directed" and "acyclic" refer only to the mathematical properties of the graph.
  Formally, any graph is a DAG if two nodes can only be first-order connected by just one directed edge, and no path through the graph begins and ends with the same node.
  This entails that nodes cannot be immediately connected by multiple edges, undirected edges, or bidirectional edges.
  Furthermore, the graph contains no "loops": edges that connect a node to itself.
]
These graphs are valuable because they reveal which variables must be adjusted for in order to satisfy certain identification criteria in _do_-calculus.
Most notably, the _back-door criterion_ states that a causal effect is identified by "blocking any back-door paths" between treatment $A$ and outcome $Y$—paths that connect $A$ to $Y$ that begin with an arrow flowing into $A$ and that pass through other variables.
For instance, Figure \@ref(fig:plot-confounding-dag) shows a causal system where the effect of $A$ on $Y$ is not identified because we can connect $A$ to $Y$ through the back-door path $A \rightarrow U \rightarrow Y$.
This means that $U$ confounds the relationship between $A$ and $Y$, but we can identify the relationship by conditioning on $U$ and "blocking" the back-door path.
Graphs are valuable for this project because I use them to convey important identification assumptions in later chapters.

```{r plot-confounding-dag, include = TRUE, fig.width = 5, fig.height = 2,out.width = "70%", fig.cap = "A causal graph where the effect of $A$ on $Y$ is confounded by $U$."}
```

<!-- 
\begin{align}
  P(Y = y \mid \do{A = a}) &= \int\limits_{\mathcal{U}}P(Y \mid A = a, U = u)P(U = u) \diff{u}
  (\#eq:adjustment-formula)
\end{align} -->

<!-- how each expresses confounding, correlation ≠ causation, and identification by conditioning -->


As with the potential outcomes framework, _do_-calculus (and causal graphs in turn) describe minimally sufficient conditions for _nonparametric_ causal identification. There is no guarantee that linear regression models, or any parametric models, adequately control for confounders and isolate identifying variation in the treatment. For this reason, it can be helpful to lay out a hierarchy of modeling concerns for any causal inference problem.

1. Causal modeling: the definition of potential outcomes or structural equations that define the space of causal effects, and the declaration of relevant causal estimands.
2. Identification assumptions: the assumptions required to identify causal estimands using only the observed data. 
These assumptions are usually posed as statements about expectations of $Y$ and when we can ignore the fact that counterfactuals are unobserved.
For instance, conditioning on a confounder $U$ identifies the conditional causal effect of $A$ because it satisfies an assumption that $A$ is independent of the potential outcomes after conditioning on $U$. 
\begin{align}
  Y_{i}(A = 1), \, Y_{i}(A = 0) \ind A_{i} \mid U_{i} = u
  (\#eq:conditional-independence)
\end{align}
Conditional independence between treatment and potential outcomes lets the researcher make inferences about potential outcomes, which we can't always observe because they are hypothetical entities in the causal model, having only observed the real-world data. For instance, if potential outcomes are conditionally independent of treatment status $A$, then we can conclude that the $Y$ actually observed at $A = a$ is equal to the $Y$ that we _would have observed_ if we could set $A = a$ ourselves, within strata $U = u$.
\begin{align}
  \begin{split}
    \E{Y_{i} \mid A_{i} = a, U_{i} = u} 
      &= \E{Y_{i}(A = a) \mid A_{i} = a, U_{i} = u} \\
      &= \E{Y_{i}(A = a)} \mid U_{i} = u
  \end{split}
  (\#eq:CIA-implication)
\end{align}
This conditional independence assumption (CIA) is the same assumption invoked by the back-door criterion. By conditioning on $U$, we subtract any variation in $A$ that can be explained by $U$, so the remaining relationship between $A$ and $Y$ represents the relationship we would observe by setting $\do{A = a}$ within stratum $U = u$.
3. Statistical assumptions: What estimators are we using to estimate $\E{Y_{i} \mid A_{i}, U_{i}}$, and do they introduce additional assumptions that interfere with causal inference? 
Do they suffer additional biases due to misspecification, functional form assumptions, that jeopardize key identification assumptions?

I lay out this hierarchy for two reasons.
First, it clarifies why researchers use certain research designs or statistical approaches to overcome certain problems with their data.
Statistical assumptions, we will see, can undermine identification assumptions, which is why causal inference scholars tend to promote estimation strategies that rely on as few additional assumptions as possible [@keele:2015:causal-inf].
One way to avoid these assumptions is to use research designs that eliminate confounding "by design" rather than through statistical adjustment, such as randomized experiments, instrumental variables, regression discontinuity, and difference-in-differences [for instance, @angrist-pischke:2008:mostly-harmless].
If researchers cannot design away these difficult assumptions, other methods are available to adjust for confounders without as many strict assumptions about the functional form of the causal model as are commonly invoked in parametric regression models.
Causal inference is not synonymous with the new "agnostic statistics" [@aronow-miller:2019:agnostic-statistics], but it is animated by a similar motivation to implement statistical methods that relies on fewer fragile assumptions.
For causal inference problems, these methods include matching, propensity score weighting, and even machine learning methods for estimating flexible conditional expectation functions that are robust to nonlinearities and non-additivities in the data generating process [@sekhon:2009:opiates; @aronow-samii:2016:regression; @hill:2011:bart; @green-kern:2012:bart; @samii-et-al:2016:retrospective-causal-inference-ML].
This dissertation will use machine learning methods, in particular Bayesian neural networks (BNNs), to estimate flexible expectation functions without relying on strict assumptions about unknown functional forms. 

Second, this three-party hierarchy of causal clarifies where my contributions around Bayesian causal estimation will be focused.
As I discuss below, a Bayesian view changes the interpretation of the causal model (level 1) by invoking a _probability distribution_ over the space of potential outcomes. This probability distribution allows the researcher to say which counterfactual data are more plausible than others, which is a desirable property in a causal modell that conventional inference methods nonetheless do not readily enable.
A Bayesian approach to causal inference also has the potential to extend the meaning of identification assumptions (level 2) by construing them also as probabilistic features of a model rather than fixed features. 
Finally, and most practically, the Bayesian framework offers a greater variety of statistical tools for estimating causal effects (level 3).



### Bayesian Inference

Bayesian reasoning is a contentious and misunderstood topic in empirical political science, so it is important to establish some essential tenets to the approach before melding it with causal modeling.
Bayesian analysis is the application of conditional probability for statistical inference.
Its mechanical underpinnings are uncontroversial, essential building blocks of probability theory: how the probability of an event changes by conditioning on other known information.
The controversy surrounding Bayesian methods in political science is better understood as a disagreement over which modeling constructs we choose to describe using probabilities.

Whereas many statistical methods begin with a model of data given fixed parameters, Bayesian inference is premised on a joint modeling for all components in a system. 
For example, suppose that we are interested in jointly describing age and vote choice in a population:
We could write the joint distribution of these two variables as $p(\mathit{Age}, \mathit{Vote})$. This distribution can be equivalently expressed by factoring it in two ways:
\begin{align}
  p(\mathit{Vote} \mid \mathit{Age})p(\mathit{Age}) 
    &= p(\mathit{Age} \mid \mathit{Vote})p(\mathit{Vote})
  (\#eq:condition-prob-vote-age)
\end{align}
Supposing that we observed an individual's vote choice, how would this information affect our information about their likely age? In order to condition on known information, probability theory says that we divide the joint probability by the probability of the conditioning event.
\begin{align}
\begin{split}
  \frac{p(\mathit{Vote} \mid \mathit{Age})p(\mathit{Age})}{p(\mathit{Vote})}
   &= \frac{p(\mathit{Age} \mid \mathit{Vote})p(\mathit{Vote})}{p(\mathit{Vote})} \\
  \frac{p(\mathit{Vote} \mid \mathit{Age})}{p(\mathit{Age})}
  &= p(\mathit{Age} \mid \mathit{Vote})
\end{split}
(\#eq:updated-vote-age)
\end{align}
This simple operation reveals a statement that is conventionally understood as Bayes' theorem: the conditional probability of $A$ given $B$, expressed in terms of $B$ given $A$.
The machinery underpinning Bayesian inference is nothing but a by-the-book application of probability theory.
It provides, through no magic whatsoever, a formal method for rationally updating a joint probability distribution by conditioning on known information.

This machinery can be applied to a more general modeling scenario containing data $\mathbf{y}$ and parameters $\boldsymbol{\pi}$.
The joint model for the data and the parameters takes the form
\begin{align}
  p(\mathbf{y}, \boldsymbol{\pi}) = p(\mathbf{y} \cap \boldsymbol{\pi}) = p(\mathbf{y} \mid \boldsymbol{\pi})p(\boldsymbol{\pi}),
  (\#eq:joint-model)
\end{align}
where $p(\mathbf{y} \mid \boldsymbol{\pi})$ represents the probability distribution of the data, conditioning on parameters, and $p(\boldsymbol{\pi})$ represents a _prior distribution_ for parameters, marginal of (or unconditioned on) the data.
The prior distribution is commonly described as the researcher's "prior information" (or, controversially, prior "beliefs") about the parameter, an important but misunderstood component of Bayesian modeling.  
The joint model provides machinery for learning about parameters by conditioning the parameters on the data,
\begin{align}
  p(\boldsymbol{\pi} \mid \mathbf{y}) &= \frac{p(\mathbf{y} \mid \boldsymbol{\pi})p(\boldsymbol{\pi})}{p(\mathbf{y})}
  (\#eq:joint-model-post)
\end{align}
also known as obtaining the _posterior distribution_ of the parameters.

These basic concepts of conditional probability and Bayesian updating are not foreign to political science, but it will be important to establish an interpretation of Bayesian thinking and Bayesian modeling that is productive for causal inference. 
This project invokes what @mcelreath:2017:decolonized-bayes calls an "inside view" of Bayesian statistics---Bayesian statistics on its own terms.
This implies a rejection of an "outside view" that construes Bayesian statistics as a penalized variation of maximum likelihood.
Under the outside view, heavily influenced by the prominence of non-Bayesian likelihood approaches to statistical inference, data follow probability distributions, while parameters are fixed constants.
Bayesian estimation uses the original likelihood model and attaches a penalty in the form of a prior distribution for model parameters.
Prior distributions, in turn, represent the subjective beliefs of the researcher.
Bayesian modeling can be reduced to a biased estimation procedure that downweights data in favor of arbitrary subjectivity.
This view of Bayesian statistics is admittedly confusing, and if we take it at face value, it is no wonder that causal inference in political science has largely avoided Bayesian tools other than for computational convenience.^[
  I return to the distinction between "Bayes for the sake of MCMC" and "Bayes for the sake of Bayes" in a later discussion of prior distributions.
].

The inside view, as mentioned above, construes a Bayesian model as a joint model for all variables in a system.
The use of the word "variables" to encompass both data and parameters is crucial.
The technology of a Bayesian model does not regard data and parameters as distinct from one another.
They follow the same rules, just as age and vote choice followed the same rules in the above example.
Data and parameters are both instantiations of uncertain processes, with the only semantic difference between the two being that observed variables are called "data" while unobserved variables are called "parameters" [@mcelreath:2017:decolonized-bayes].^[
  The semantic conventions are often sloppier in practice than many researchers would like to think.
  Many analyses use data that summarize lower-level processes, such as per-capita income in U.S or the percentage of women who vote for the Democratic presidential candidate, which behave like random variables in that their values could differ under repeated sampling.
  The semantic distinction between data and parameters has a similar spirit to the @blackwell-et-al:2017:measurement-error view of measurement uncertainty, where "measurement error" falls on a spectrum between fully observed data and missing data.
]
Prior distributions and likelihood functions are the same thing: probability distributions that quantify uncertainty about a variable.
If I were to observe a new data point from a model, I would be unable to predict its value exactly, but some values are more probable than others, given the parameters that condition the data.
The same premise holds for parameters: if I could observe a parameter, I would have been unable to anticipate it exactly, but I could bet that some parameters are more likely than others, given the data that I have already seen.
The joint model for all variables encapsulates the probabilistic relationships between data and parameters.
Starting with the prior model, $p(\mathbf{y}, \boldsymbol{\pi})$, we can condition the model on chosen parameters to obtain a rationalizable distribution of data, $p(\mathbf{y} \mid \boldsymbol{\pi})$.
Or we can condition the model on data to obtain a rationalizable distribution of parameters, $p(\boldsymbol{\pi} \mid \mathbf{y})$.^[
  @mcelreath:2020:bayes-counting calls these maneuvers "running the model forward" (data given parameters) or "running the model backward" (parameters given data).
  It can be helpful to think of the model as a machine with many interlocking parts, where if you crank the machine, the pieces that you choose to constrain will affect which pieces do and do not move.
]

<!------- TO DO ---------
- plot a prior, predictive, and inferential distribution
------------------------->

From the inside view, Bayesian updating proceeds by considering a variety of possible scenarios that create data and evaluating which scenarios are consistent with the data.
The joint prior model, $p(\mathbf{y} \mid \boldsymbol{\pi})p(\boldsymbol{\pi})$ describes an overly broad set of possible configurations of the world. This configurations are the combination of possible parameters, $p(\boldsymbol{\pi})$, and possible data given parameters, $p(\mathbf{y} \mid \boldsymbol{\pi})$.
Bayesian updating decides which configurations of the world are more plausible in light of data actually observed.
The plausibility, or posterior probability, of a parameter value is greater if the observed data are more likely to occur under that parameter value versus another value: larger $p(\mathbf{y} \mid \boldsymbol{\pi})$.
Conditioning on the data downweights the configurations of the world that are inconsistent with the observed data, resulting in a distribution of scenarios that reflect more plausible configurations of the world [@mcelreath:2020:rethinking-2, chapter 2].
This is an important distinction from non-Bayesian statistical inference, since there can be no formal notion of "plausible parameters" without a posterior distribution, which necessitates a prior distribution.
For causal inference, this means there can be no formal notion of "plausible counterfactuals" without a probability distribution over the counterfactuals, which necessitates a probability distribution over causal effects.
The mission in the remainder of this chapter is to establish a framework for causal inference in terms of plausible effects and plausible counterfactuals.

The inside view of Bayesian modeling, and the philosophical unity that it brings to statistical machinery and inference, is possible even if using uninformative prior distributions that are indifferent to possible parameters  _a priori_.
This is how Bayesian methods tend to appear in political science to date, with noninformative priors that exist primarily to facilitate Bayesian computation for difficult estimation problems.
The infamy of Bayesian methods, however, is owed to the ability of the researcher to specify "informative" priors that concentrate probability density on model configurations that are thought to be more plausible even before data are analyzed.
There are many modeling scenarios where this concentration of probability delivers results that are almost unthinkable without prior structure: multilevel models that allocate variance to different layers of hierarchy, highly parameterized models with correlated parameters such as spline regression, and sparse regressions where regularizing priors are used to shrink coefficients and preserve degrees of freedom to overcome the "curse of dimensionality" [e.g. @gelman-et-al:2013:BDA].
At the same time, many researchers are skeptical of Bayesian methods because supplying a model with non-data information can be spun as data falsification [@garcia-perez:2019:bayes-data-falsification].
As I elaborate in Section \@ref(sec:bayes-how-to), the mere non-flatness of a prior is often less consequential than the prior's _structure_. 
For instance, whether a prior has variance of $10$ or $100$ often matters less than whether the prior specifies a hierarchical structure for borrowing information from other groups in the data.
It is no coincidence the Bayesian contributions in this dissertation and in other areas of applied statistics are concerned with different prior structures more than the precision of otherwise equal prior structures.


<!------- TO DO ---------
- no "histomancy" but rather a prior about the stochastic process that generates each data point
------------------------->





### Shared Goals, Different Tactics

Fux with GGK:

- measure = truth + bias + variance
- causal inference: reduce bias "by design"
- bayesian models: reduce variance using prior information (or quantify potential biases [all uncertainties] using priors)


Not _inherently_ the same as "agnostic" inference though we fux with both.





## Bayesian Causal Modeling {#sec:intro-bcm}

Having reviewed the basics of causal models and Bayesian inference, we now turn to a framework for Bayesian causal modeling.
The distinguishing feature of a Bayesian causal model is that the elemental units of the model, the potential outcomes, are given probability distributions.
This probability distribution reflects available causal information that exists outside the current dataset.
Bayesian inference proceeds updating our information about counterfactual potential outcomes in light of the observed data.
The headings under \@ref(sec:intro-bcm) introduce this modeling framework at a high level.
I provide a probabilistic interpretation and notation for potential outcomes models (\@ref(sec:po-distributions)), a connection between causal parameters and model parameters (\@ref(sec:what-params)), and a broad justification for the Bayesian interpretation of causal effects (\@ref(sec:why-bcm)).




### Probabilistic Model for Potential Outcomes {#sec:po-distributions}

As with other causal models, we begin at the unit level.
Unit $i$ receives a treatment $A_{i} = a$, with potential outcomes $Y_{i}\left(A_{i} = a\right)$.
Suppose a binary treatment case where $A_{i}$ can take values $0$ or $1$, so the unit-level causal effect is $\tau_{i} - Y_{i}\left(1\right) - Y_{i}\left(0\right)$.

The unit-level causal effect $\tau_{i}$ is unidentified, but it is possible to estimate to estimate population-level causal quantities by invoking identification assumptions.
For instance, the conditional average treatment effect at $U = u$, $\bar{\tau}(U = u) = \E{Y_{i}(1) - Y_{i}(0) \mid U_{i} = u}$, can be estimated from observed data assuming consistency, non-interference, conditional ignorability, and positivity,
\begin{align}
\begin{split}
  \bar{\tau}(U = u) 
      &= \E{Y(A = 1) - Y(A = 0)\mid U = u} \\
      &= \E{Y(A = 1) \mid U = u} - \E{Y(A = 0) \mid U = u} \\
      &= \E{Y \mid A = 1, U = u} - \E{Y \mid A = 0, U = u}
\end{split}
(\#eq:CACE-proof)
\end{align}
where the third line is obtained by the identification assumptions.
The identification assumptions connect _causal estimands_ and what I will call _observable estimands_.
Causal estimands are the true causal quantities stated in terms of potential outcomes, which makes them unobservable.
Observable estimands are the equivalent quantities that could be recovered from observable data under the identification assumptions.
Other literature refers to observable estimands as "nonparametric estimators" [@keele:2015:causal-inf], but I steer clear of this language because the gap between observable estimands and estimators is an essential distinction for understanding the Bayesian causal approach.

The transition to a Bayesian probabilistic model begins with an acknowledgment that no estimator for the observable estimand, $\E{Y \mid A = a, U = u}$, is exact. 
The assumptions identify causal effects only in an infinite data regime, where the observable estimand can be known exactly.
Inference about causal effects from finite samples, however, requires further statistical assumptions that link the observable estimand to an estimator or model.
<!------- TO DO ---------
- redo with an expectation-level model
------------------------->
Let $f(Y \mid A, U, \boldsymbol{\pi})$ be a prediction function for the mean of $Y$ that depends on treatment $A$, confounders $U$, and parameters $\boldsymbol{\pi}$. This model's expectation is assumed to be equal to the observable estimand.
\begin{align}
  \E{Y \mid A = a, U = u} &= f(Y \mid A = a, U = u, \boldsymbol{\pi})
\end{align}
This assumption is similar to any modeling assumption that appears in observational causal inference to link an estimator to the observable estimand, including parametric models for covariate adjustment, propensity models, matching, and more [@acharya-blackwell-sen:2016:direct-effects; @sekhon:2009:opiates].^[
  Although researchers are focusing more attention on estimation methods that focus on these statistical assumptions themselves, either by model ensembles/averages or "robust" models for propensity and response.
]
<!------- TO DO ---------
- cites for parametric adjustment, propensity, matching
- robust
- ensembles in causal inf
------------------------->
The conditional average treatment effect $\bar{\tau}(U = u)$ is obtained by differencing these model predictions over the treatment.
\begin{align}
  \bar{\tau}(U = u) 
    &= \E{Y \mid A = 1, U = u} - \E{Y \mid A = 0, U = u} \\
    &= f(Y \mid A = 1, U = u, \boldsymbol{\pi})
       - f(Y \mid A = 0, U = u, \boldsymbol{\pi})
\end{align}

These expectations are infinite-data results, but in any finite sample, we are uncertain about the conditional average treatment effect because we are uncertain about the model predictions, and we are uncertainty about model predictions because we don't estimate model parameters $\boldsymbol{\pi}$ with certainty.
The Bayesian framework provides a natural ability to incorporate uncertainty into the causal estimate by way of probabilistic information about $\boldsymbol{\pi}$.
\begin{align}
  p(\bar{\tau}(U = u))
    &= \int \left[
         f(Y \mid A = 1, U = u, \boldsymbol{\pi}) 
         - f(Y \mid A = 0, U = u, \boldsymbol{\pi})
       \right] 
       p(\boldsymbol{\pi})
       \diff{\boldsymbol{\pi}}
  (\#eq:post-cace)
\end{align}
<!------- TO DO ---------
- should the p(pi) be there?
------------------------->
Equation \@ref(eq:post-cace) provides a probabilistic expression for a causal estimand, which is the key feature of a Bayesian causal model.
These probabilistic expressions are significant because they lets us directly characterize plausible and implausible magnitudes of causal effects.
Statements as banal as "the data suggest that the causal effect is most likely 3 percentage points or greater" are simply not possible under non-Bayesian methods, which I explain further in Section \@ref(sec:why-bcm).
This is valuable because causal inference is interested in knowing what the causal effect of treatment _is_, not what the causal effect is not.

Works without repeated sampling.
<!------- TO DO ---------
- design uncertainty
------------------------->


This is possible by having a joint model for the data and parameters...

With the joint model, we can write a probabilistic expression for unobserved potential outcomes directly.
This is possible because, under the generative model, our model for counterfactual data is nothing more than a missing data model. (Rubin)
We are simply churning out new data points, tweaking the treatment status. 






Guidelines:

- we have a probability distribution for an unknown potential outcome
- this gives us a probability distribution for a unit causal effect
- and a potential outcome for a mean
- This is _missing data problem_ (Rubin 1978). Assumptions let us model E[Y | A] for all A.

Causal model: $y_{i}(1)$ and $y_{i}(0)$

Observed data: $y_{i} = z_{i}y_{i}(1) + (1 - z_{i})y_{i}(0)$

Unobserved data: $\tilde{y}_{i}$

Inference about $\tilde{y}_{i}$: $p(\tilde{y}_{i} \mid \mathbf{y}) \propto p(\tilde{y}_{i} \mid \pi, \mathbf{y})p(\pi)$


This fits an ML approach:

- ML approach to causal inference recasts the problem as a prediction problem for the treatment assignment.
- Treatment effects are then integrated over the uncertainty in the propensity
- This propagates uncertainty using the exact model machinery, rather than a post-hoc computational workaround like bootstrapping
    - Bootstrapping does have an appealing feature that it isn't making a parametric assumption about errors in the model. It simply uses the "sampling uncertainty" intuition to build intuitive bounds on effect uncertainty.
    - However in many real-world cases the data we have cannot be resampled, so the framework for inference under bootstrapping doesn't make theoretical sense without appealing to a "superpopulation" philosophy. 
    - Bayes naturally deals with this by assigning a probability distribution to the unmodeled features, which mechanistically follows a similar assumption as a parametric assumption about errors in any typical regression case: the prior isn't really extra. 
    - If this assumption isn't something you want, it is possible to generalize the model by specifying an overdispersion parameter and letting the data inform what which model estimates can be rationalized against plausible overdispersion parameters. For example, Kruske's BEST approach to difference-in-means testing. 
- In a Bayesian framework, we have posterior More of a "priors to facilitate posteriors" approach, not a prior information view (since there is some formal data peeking involved)


Probabilistic treatment


### What "Parameters" Mean in Modeling and Estimation {#sec:what-params}

What was I trying to say here? 

A causal parameter is a feature of the _causal model_. 

- The causal model is the model if you knew everything (level 1). 
- Individual causal effect is only a transformation of _data_. if I knew all potential outcomes, I could calculate this with only data.
- Estimands are different forms of causal effects, individual or aggregate.
- Assumptions let us state the conditions under which an individual or aggregate parameter can be estimated from observed data only (level 2).

Estimands are framed in terms of individual effects or aggregate expectations.

- The estimator (3) of an expectation or a counterfactual is thus a different thing from the hypothetical estimand.
- Some causal models imply nonparametric estimators, so no Bayesian anything will be required. However, estimates themselves are in-sample quantities, and any generalization about the in-sample effect can be augmented with priors.
- Bayes is an _estimation_ framework that doesn't change the causal model itself. It is a different way of estimating the quantities in an in-sample causal estimand.
- Oganisian and Roy (2020) "identification" assumptions vs. "statistical" assumptions. Identification assumptions get you to a place where you can express causal effects in terms of expectations about $Y$ given treatment, within confounding strata (perhaps then averaging over confounders). "Statistical" assumptions define how we think we can build $E[Y]$

Bayes is 
- Bayesian inference _begins_ as technology for thinking about statistical assumptions.
- It eventually becomes technology for experimenting with identification assumptions.


- For nonparametric estimators, structural priors can be helpful for concentrating probability mass in sensible areas of space, since nonparametric estimators may be lower-powered than estimators that get a power boost from parametric assumptions.
- Parametric models, in term, are obvious areas where Bayesian estimates can go, but they are stacking more assumptions on top of the parametric assumptions.
- Semi-parametric models are an interesting middle ground, where we want to be flexible about the exact nature of the underlying relationships, but we want to impose some stabilization to prevent the model from behaving like crazy.






Do battle with the "implied nonparametric estimator" framework.

- Causal models are manipulated to express causal _estimands_.
- the "nonparametric estimators" are usually just averages of treatment and control group outcomes, under ignorability assumptions.
- Parametric models for estimands are usually a byproduct of thinking within an MLE/regression framework. Many causal inf techniques don't do that. Instead they try to simply predict E[y(1) - y(0)] and don't care about the interpretability of the other RHS terms.
- Thinking outside the "typical econometric" framework it's actually kinda easy to see how one flavor of fitting Bayesian models for causal effects. If the technology for prediction is Bayesian, you just get the prediction and the posterior distribution. You then deal with the bias/variance properties of $\hat{y}$ or $\tilde{y}$ (unobserved counterfactuals) but at least your focus is on the predictions rather than coefficients (which, from this view, who cares about those?)
- This actually fits quite nicely with the machine learning approach to causal inference



We can think bigger about Bayesian _inference_ for a parameter as distinct from Bayesian _estimation_ of the in-sample quantity. 
This lets us use a nonparametric data-driven estimator for the data, but the "inference" or "generalization" still has a prior.
For instance a sample mean estimates a population mean without a likelihood model for the data, but inference about the population mean often follows a parametric assumption from the Central Limit Theorem that the sampling distribution from the mean is asymptotically normal (but doesn't have to, c.f. bootstrapping).
<!------- TO DO ---------
- Read more about sharp null and an RI framework for inference, 
  since this runs up against the idea of asymptotic normality of the means.
------------------------->
Even if the point estimator we use for a mean is unbiased, we can assimilate external information during the interpretation of the estimator (biasing the inference without biasing the point estimator).
Restated: the posterior distribution is a weighted average of the raw point estimator and external information, rather than biasing the data-driven estimate directly.

Even bigger: Bayesian inference about _models_ (Baldi and Shahbaba 2019). 
This is probably where I have to start my justification for this? 
The _entire point_ of causal inference is to make inferences about counterfactuals given data (Rubin 1978?).
Invoking Bayesian inference is really the only way to say what we _want_ to say about causal effects: what are the plausible causal effects given the model/data. We do _not_ care about the plausibility of data given the null (as a primary QOI). 
- Probably want to use Harrell-esque language? Draw on intuition from clinical research, or even industry. We want our best answer, not a philosophically indirect weird jumble.
- This probably also plays into the Cox/Jeffreys/Jaynes stuff I have open on my computer.
<!------- TO DO ---------
- Corey Yanofsky recommended reading
- Gelman/Shalizi philosophy of Bayes
- Induction/Deduction in Bayes
- etc.
- symposium about "objective" Bayes
- And then also Gelman/Simpson/Betancourt "context of the likelihood"
  => we have priors about the WORLD, not about model parameters.
------------------------->

<!------- TO DO ---------
- soul searching about a philosophy of Bayes to stake out for the project?
- "pragmatic bayesian workflow" - priors are "just a part of the model" that need to be checked w/ prior predictive checking and so on.
- vs. the "philosophical coherence" view.
- damnit don't spend too much time on this because it just needs to be an orientation to stick decisions to.
- We're already leaning in the "pragmatic" direction from the IRT model text.
------------------------->

This presumes an m-closed world(?right?), which maybe we don't like (Navarro, "Devil and Deep Blue Sea").
<!------- TO DO ---------
- Read Navarro CLOSELY!
- Read ARONOW and MILLER: how to square "intra-model agonisticism" with 
  some philosophical view of what Bayes is.
- Maybe this is terminology I should work on! Draw some helpful lines that 
  don't already exist for defining subtypes of agnosticism or other
  epistemic bundles of things.
  (there is also some paper on "types of Bayesians"?
   mentioned in a McElreath talk)
------------------------->
Me debating with myself: how to think about Bayesian model selection vs "doubly robust" estimation ideas...
Maybe some hybrid view in the "quasi-experimental" approach to causal model selection. We estimate two models from the same data—one with a treatment parameter and one where we impose no treatment effect—and compare models using some likelihoodist or Bayes factor measure of evidence.
<!------- TO DO ---------
 - articulate a good beef with the paper's terminology of "quasi-experimental"
 -------------------------> 

What is THIS project going to do?

- Pragmatic view of priors?
- Are we doing more flexible covariate adjustment?
- Maybe we should decide this AFTER we experiment with Ch 4 and 5 data/methods
<!------- TO DO ---------
- pronouncement of project values
------------------------->


### Bayesian Inferences Make More Sense {#sec:why-bcm}


Why would we want this? Inference makes more sense.

- What's the probability of a model/hypothesis, given the data
- vs. What's the probability of "more extreme data" (?) given a model that I don't believe.
- posterior probabilities mean what they say they mean
    - conditioning on data (and implicitly the model), this is the distribution of parameters
    - p-values are the "probability of more extreme results." They condition on the model, but they're only useful if they don't.

Proper frequentist analysis is violated as soon as you look at the data.

Are we in a repeated sampling framework at all?

Frequency properties are still possible for Bayesian estimators, but we view frequency properties as a byproduct of something more essential (MSE).





## Making Sense of Priors in Causal Inference


### Priors as Data Falsification

<!------- TO DO ---------
- figure this out
DATA FALSIFICATION digression
------------------------->

Data falsification versus unavoidable choice:
imagine a study with a posterior distribution $p(\mu \mid \mathbf{y})$ that is proportional to the likelihood times the prior. 
  \begin{align}
    (\#eq:propto)
    p(\mu \mid \mathbf{y}) &\propto p(\mathbf{y} \mid \mu)p(\mu)
  \end{align}
Rewrite the right side in product notation for $n$ observations of $y_{i}$ for units indexed by $i$, letting $l(y_{i}) = p(y_{i} \mid \mu)$
  \begin{align}
    p(\mu \mid \mathbf{y}) &\propto \prod\limits_{i = 1}^{n}l(y_{i})p(\mu)
    (\#eq:propto-index)
  \end{align}
Suppose that we express this proportionality on the log scale, where the log posterior is proportional to the log likelihood plus the log posterior.
  \begin{align}
    \begin{split}
      \log p(\mu \mid \mathbf{y}) 
        &\propto \sum\limits_{i = 1}^{n} \log l(y_{i}) 
          + \sum\limits_{i = 1}^{n} \log p(\mu) \\[12pt]
        &\propto \log l(y_{1}) + \log p(\mu)
                 + \log l(y_{2}) + \log p(\mu)
                 + \ldots
                 + \log l(y_{n}) + \log p(\mu)
    \end{split}
  (\#eq:log-propto-index)
  \end{align}


The setup in \@ref(eq:log-propto-index) highlights a few appealing intuitions.
First, it shows how each observation "adds information" to the log posterior distribution.
Data that are more likely to be observed given the parameter (larger $l(y_{i})$ values) increase the posterior probability that parameter. 
We also see that the prior probability "adds information" to the posterior in the same way data add information, captured by the addition of each $p(\mu)$ term. 
Parameters that are more probable in the prior are more probable in the posterior.

The proportionality \@ref(eq:log-propto-index) also reveals how the posterior "learns" from flat priors.
A flat priors implies that prior probability $p(\mu)$ is constant for all potential values of $\mu$.
Because \@ref(eq:log-propto-index) is a proportionality, this lets us disregard $p(\mu)$ entirely by factoring it out of the proportionality, leaving us with an expression that the log posterior is proportional to the likelihood of the data only if the prior is flat.
\begin{align}
  \log p(\mu \mid \mathbf{y}) 
        &\propto
        \log l(y_{1}) + \log l(y_{2}) + \ldots + \log l(y_{n})
  (\#eq:log-propto-flat)
\end{align}
If $p(\mu)$ is not flat, however, and $p(\mu)$ varies across values of $\mu$, we can no longer ignore $p(\mu)$ in \@ref(eq:log-propto-index) shows that $p(\mu)$ varies across values $\mu$.
Not only does this prevent us from dropping $p(\mu)$ from the proportionality, but it also reveals how the prior "adds information" to the posterior by the same mechanism that observations do: adding to the log posterior distribution. 
This general expression where both data and priors contribute to the posterior distribution has led some researchers to argue the Bayesian inference with non-flat priors is analytically indistinguishable from data falsification [@garcia-perez:2019:bayes-data-falsification].
We can highlight this behavior by obscuring each $p(\mu)$ term with a square $\square$.
\begin{align}
  \log p(\mu \mid \mathbf{y}) 
        &\propto
        \log l(y_{1}) + \square + \log l(y_{2}) + \square \ldots + \log l(y_{n}) + \square
  (\#eq:log-propto-falsification)
\end{align}
Behind each square is _some contribution_ to the log posterior.
The fact that it adds information to the log posterior is unaffected by whether the hidden term is the probability of an additional observation $l(y_{i})$ or the prior probability of a parameter value $p(\mu)$.
<!------- TO DO ---------
- END FALSIFICATION
------------------------->


<!------- TO DO ---------
- but where was I going with this?
------------------------->



### Priors and Model Parameterization

<!------- TO DO ---------
- this needs reverse outline
------------------------->

Priors are defined with respect to a model of the data (the likelihood). 
<!------- TO DO ---------
- discuss Betancourt, Simpson, Gelman
------------------------->
We may have priors about the way the world works, but we rarely have priors about model parameters. This is because parameters are an invention in the model. They are mathematical abstractions similar to points and lines, so they only exist when we translate the world into a mathematical language. This means that the mathematical representation of the world is in direct dialog with the choices available to a researcher about how to encode prior information. In the real world, the prior information that I have about the world isn't affected by a mathematical representation of the world. As a researcher, the way I encode prior information depends on the choices I make about that mathematical representation.

One essential feature for understanding prior choices in practice is the _parameterization_ of the data model, $p(y \mid \phi)$, for some generic parameter $\phi$.^[
  Bayesian practitioners sometimes refer to the data model as the "likelihood." 
  This can be confusing because the "likelihood function" more traditionally refers to the _product_ of the data probabilities under the data model.
  References to the "parameterization of the likelihood" should be understood as interchangeable with "parameterization of the data model," since the former is determined entirely by the latter. 
]
We say that a data model has an "equivalent reparameterization" if for some transformed parameter $\psi = f(\phi)$, the function that defines the data model can be rewritten in terms of $\psi$ and return an equivalent likelihood of the data. More formally, the parameterization is equivalent if $p(y \mid \phi) = p(y \mid \psi)$ for all possible $y$.

In a maximum likelihood framework, equivalent parameterizations are a more benign feature of the modeling framework. Reparameterization may result in likelihood surfaces that have easier geometries for optimization algorithms to explore, but the _value_ of the likelihood function is unaffected by the algebraic definition or parameterization of the likelihood function. For instance, a Normally distributed variable $x$ with mean $\mu$ could be parameterized in terms of standard deviation $\sigma$ or in terms of precision $\tau = \frac{1}{\sigma^{2}}$, but the resulting density is unaffected.
\begin{align}
  \frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{x - \mu}{2\sigma}\right)^{2}}
  &=
  \sqrt{\frac{\tau}{2\pi}}e^{-\frac{\tau\left(x - \mu\right)^{2}}{2}}
  (\#eq:normal-sigma-equal-tau)
\end{align}
The consequence for Bayesian analysis, however, is that the parameterization of the data model determines the set of parameters and their functional relationship to the data.


<!------- TO DO ---------
- linear reparameterization: easy to understand
- decide if non-centering is worth it to discuss here
- nonlinear reparameterization: a much bigger problem
------------------------->



One example of equivalent reparameterization arises with the different possible ways to write a linear regression model. 
The first form specifies $y_{i}$ for unit $i$ as a linear function of $x_{i}$ with a random error that is mean $0$ and standard deviation $\sigma$.
\begin{align}
  (\#eq:ols-noncenter)
  y_{i} &= \alpha + \beta x_{i} + \varepsilon_{i}, & \text{where } \varepsilon_{i} &\sim \mathrm{Normal}\left(0, \sigma \right)
\end{align}
The second form, more common when viewing linear regression in the framework of generalized linear models, is to express $y_{i}$ directly as the random variable, with a conditional mean defined by the regression function and standard deviation $\sigma$.
\begin{align}
  (\#eq:ols-center)
  y_{i} &\sim \mathrm{Normal}\left( \alpha + \beta x_{i},\sigma\right)
\end{align}
Algebraically, these two models are identical.
The difference is only a matter of which component has the distributional assumption.
In Equation \@ref(eq:ols-noncenter), the distribution is assigned to $\varepsilon_{i}$, so $y_{i}$ is a random variable only by way of $\varepsilon_{i}$.
In Equation \@ref(eq:ols-center), we assign the distributional assumption directly to $y_{i}$, bringing the regression function into the mean rather than "factoring it out" of the distribution.

```{r normal-factoring}
normal_n <- 500
normal_mu <- 4
normal_sd <- 2
```

```{r plot-normal-factoring, include = TRUE, fig.cap = "Demonstration of centered and non-centered parameterizations for a Normal distribution. The non-centered parameterization is statistically equivalent, but the location and scale are factored out of the distribution."}
set.seed(0987)
tibble(
  y1 = rnorm(n = normal_n, mean = normal_mu, sd = normal_sd),
  y2 = normal_mu + normal_sd * rnorm(n = normal_n, mean = 0, sd = 1)
) %>%
  pivot_longer(cols = everything()) %>%
  ggplot() +
  aes(x = value, color = name) +
  stat_ecdf(size = 1) +
  coord_cartesian(
    xlim = c(normal_mu - 2 * normal_sd, normal_mu + 2 * normal_sd)
  ) +
  scale_x_continuous(
    breaks = seq(-10, 10, normal_sd)
  ) +
  labs(
    title = str_glue(
      "Empirical CDF of Normal({normal_mu}, {normal_sd}) Samples"
    ),
    subtitle = str_glue(
      "{normal_n} Draws from Centered and Non-Centered Distributions"
    ),
    x = TeX("Random Variable $\\nu$"),
    y = "ECDF"
  ) +
  scale_color_manual(values = c(primary, secondary)) +
  theme(legend.position = "none")
```

The linear regression context is one context where the choice of parameterization appears. These two parameterizations are typically called the "centered" and "non-centered" parameterization for a Normal distribution. In the centered parameterization, the random variable is drawn from a distribution "centered" on a systematic component, whereas the non-centered distribution factors out any location and scale information from the distribution, such that the only remaining random variable is a standardized variate. The equations below describe a Normal variable $\nu$ with mean $`r normal_mu`$ and standard deviation $`r normal_sd`$.
\begin{align}
  \text{Centered Parameterization:} && \nu &\sim \mathrm{Normal}(`r normal_mu`, `r normal_sd`) \\
  \text{Non-Centered Parameterization:} && \nu &= `r normal_mu` + `r normal_sd`z, & \text{where } z &\sim \mathrm{Normal}(0, 1)
\end{align}
To demonstrate that these parameterizations are equivalent, I simulate $`r normal_n`$ simulations from each parameterization and plot their empirical cumulative distribution functions alongside each other in Figure \@ref(fig:plot-normal-factoring). Because the distributions are the same, the empirical CDFs are identical except for random sampling error. 

```{r, include = TRUE, fig.width = 10, fig.height = 2, out.width = "100%", fig.cap = "A spectrum of attitudes toward priors. "}
tibble(
  x = c(0.5, 2, 3.5),
  y = 1,
  label_head = c(
    "Minimalist Prior",
    "Pragmatic Prior",
    "Principled Prior"
  ),
  label_body = c(
    "Priors are nuisances,\nmerely facilitate inference/MCMC",
    "Priors are helpful:\nstructural info, regularization",
    "Priors ensure coherence,\nencode beliefs & hypotheses"
  )
) %>%
  ggplot() +
  aes(x = x, y= y) +
  geom_text(
    aes(y = 0.75, label = label_head),
    vjust = 0,
    fontface = "bold"
  ) +
  geom_text(
    aes(y = 0.75, label = label_body),
    vjust = 1.5
  ) +
  annotate(
    geom = "segment",
    arrow = arrow(
      angle = 30, length = unit(0.25, "inches"), ends = "both", type = "open"
    ),
    x = 0, xend = 4, y = 1, yend = 1
  ) +
  annotate(
    geom = "text",
    x = 0, y = 1, hjust = 1.1,
    label = "Less Informative"
  ) +
  annotate(
    geom = "text",
    x = 4, y = 1, hjust = -0.1,
    label = "More Informative"
  ) +
  coord_cartesian(ylim = c(0.25, 1.25), xlim = c(-1, 5)) +
  theme_void()
```





Problems of beliefs:

- No degree of belief.
- Parameterization makes this too challenging.
- Prior might change depending on what I ate for lunch. 
- "Elicitation" of priors satisfying the wrong audience, or at the very least can be easily misused. We don't want to elicit priors about arcane model parameters. We want to elicit priors about the _world_ (Gill Walker)d


Problems of nuisance prior

- parameterization gets you again
- the MLEs are unstable, overfit
- make the regularization argument in-sample




Pragmatic view of priors

- we're between full information and nuisance prior
- Weak information: structure, regularization, identification
- Structural information about parameters
- regularization toward zero (L1, L2), learning by pooling
- stabilizing weakly identified parameters, separation, etc.


Parameters are a _choice_. 

- They are part of the _rhetoric_ of a model. Sometimes we make pragmatic choices (something is easy to give an independent prior to, but independence isn't always valuable per se). Sometimes we make principled choices (normality, laplace, etc). 
- They deserve scrutiny (else just "excrete your posterior") and are a part of the model that you should check and diagnose. 
- They aren't merely a nuisance because we can use them to our benefit, 
- sometimes when we parameterize a problem to reveal easier things to place convenient priors on
<!------- TO DO ---------
- Gelman "Parameterization" paper: parameterization for convenience often reveals a new family of models. For Bayesian inference, it reveals a new family of priors. For _causal_ inference, we should pay careful attention to the way parameterization bears on the priors that we do and do not feel comfortable using, and the way that parameterization presents opportunities to improve on other work.
------------------------->








### Flatness is a Relative, Not an Absolute, Property of Priors

<!------- TO DO ---------
- flat priors over effects imply non-flat priors for potential outcomes
- flat priors for potential outcomes imply mode-zero priors for causal effects
------------------------->

<!------- TO DO ---------
- does this go in previous section?
------------------------->

The primary resistance to Bayesian inference in applied research is the need to set a prior at all. To many researchers, the prior distribution is an additional assumptions that is never feels justified because it is external to the data. Often researchers wish to sidestep this choice altogether, preferring a "flat" prior that prefers all parameters equally.

We have seen so far that the parameterization of a model has consequences for prior specification. Reparameterization may result in an algebraically equivalent likelihood
<!------- TO DO ---------
- define "equivalent reparameterization"
------------------------->


The incoherence of flatness: 

- no universally valid strategy for specifying flat priors because it is always possible to rearrange the data model either by transforming a parameter or otherwise rearranging the likelihood. 

Consider an experiment with a binary treatment $Z$ and a binary outcome variable $Y$. We want to determine the effect of $Z$ by comparing the success probability in the treatment group, $\pi_{1}$, to the success probability in the control group, $\pi_{0}$.



- "no way to conceptualize an uninformative prior because you can always rearrange the problem through a reparameterization or transformation of a parameter"
- examples of transformations having crazy implications/MLE being wild (logit).
- Jeffreys prior: actually a very limited range of priors that satisfy an "invariance" property. My words: such that the "amount of information obtained from data about is invariant to parameterization of the likelihood, for all possible values of the parameter," or, "the only way for the posterior distribution to be exactly the same, given the same data, for all true parameter values (?), is the Jeffreys prior," or, regardless of the data, I will learn the same thing about the generative model regardless of which equivalent parameterization of the generative model is used.  
    - is it worth it to think about the theoretical meaning of information
    - how does flatness reflect information in nonlinear scales?


Suppose we have some posterior distribution which relies on some parameter vector $\overrightarrow{\alpha}$. 
\begin{align}
  p\left(\overrightarrow{\alpha} \mid y\right) &\propto 
    p(y \mid \overrightarrow{\alpha} )
    p( \overrightarrow{\alpha} )
\end{align}
Consider some alternate parameterization of the likelihood parameterized by $\overrightarrow{\beta}$. 

Nonlinear transformation of $\pi$ does not preserve a uniform density over parameters.





Alex meeting takeaways:

- every prior has a "covariant" prior in a different parameterization
- the posteriors will be covariant as well.
- The way you get between them is by transforming the parameter and doing the appropriate Jacobian transformation to the density.
- Jeffrey's priors are a special case of this where the prior is proportional to the determinant of the information matrix. This has the beneficial property of "optimal learning" from the data. For example, flat Beta prior doesn't "hedge toward 50" in quite the same way.


### Generalization, Big and Small




Models (likelihoods) are priors

- we restrict the space of all other models
- could think about "flexible" models, but these are just priors over more spaces

Identification assumptions are priors

Generalization to any population is a prior

- priors are not MY data, but ANY data
- parameters describe ANY data
- Lampinen Vehtari 2001: likelihood as "prior for the data" is the basis for all generalization from any finite model
<!-- Describing the prior information explicitly distinguishes the Bayesian approach from the maximum likelihood (ML) methods. It is important to notice, however, that the role of prior knowledge is equally important in any other approach, including the ML. Basically, all generalization is based on the prior knowledge, as discussed in Lemm (1996, 1999); the training samples provide information only at those points, and the prior knowledge provides the necessary link between the training samples and the not yet measured future samples.  -->

We are always doing violence, but the framework lets us build out more and more general models to structure our uncertainties

Email to David:

> My first reaction to this is like this: it's probably correct to say that Bayesians may have some shared ideas about how to think about generalizing that might differ systematically from non-Bayesians, but I'm not sure how much of that is because of Bayes per se so much as just...the types of modeling someone is willing to do. By "modeling" I mean, functional form assumptions are you willing to make about data, which is different from "Bayes" in that the former is something required of all modeling and the latter is only what you can say about parameters. For example, a functional modeling thing you might do is specify (using some weights or something) how an estimate in one sample might map to another sample, but whether you do that with Bayesian estimation is a separate choice aside from the functional model itself. That being said, even though functional modeling things can be done with Bayes/non-Bayes point of view, it does seem right to say that certain modeling approaches may feel more natural in a Bayesian framework, or that Bayesians might construe a problem slightly differently because they are more used to hierarchical modeling. 
> 
> That's a pragmatic view of things, but I can give you a more theoretically abstract view, and think of situations where Bayesian lets you do things that non-Bayes can't. It all comes down to how "seriously" you want to take the tenets of Bayesian work and what kind of generalization-based claims you want to make. So I will lay out a series of vignettes that start from a world where Bayes is "not unique" and then gets into worlds where Bayes gets more and more necessary to say what you want to say about the out-of-sample world. I will use the example of estimating some parameter, but you can translate this into learning about a "mechanism" however Erica and Nick are defining that, and you can think about it non-statistically as well even if I'll use statistical modeling language.
> 
> I estimate some parameter $\mu$ in a study, but it's one instance of a more general phenomenon. If the study were representative of the world, you can imagine that the effect is one instance of the "population effect" and give it a hierarchical prior as such: $\mu ~ D(\theta, \sigma)$ where D() is some distribution, $\teta$ is the "general" effect. If I learn about $\mu$ (the estimate and standard error $\sigma$), I learn about $\teta$! Choice of distribution depends on your assumptions about the stochastic process at work. naturally, but that logic works basically just like a likelihood function choice. (In fact, Bayes sees priors for parameters as mechanistically no different from likelihoods for data. Which is to say, MLE models like logit are simply hierarchical priors on the data, and regression is estimating the hierarchical parameters of the prior.) This is basically a meta-analysis setup using Bayesian language: if you want tangible examples you can look at the Don Rubin "eight schools experiment" which is about generalizing from parallel studies in schools, or Rachael Meager (sp) has a paper applying this setup to micro-credit experiments in the development econ context: what do we learn about the "overall effect of microcredit expansion" by assimilating information from different studies. In this sense the priors are just ways to structure the meta-analysis.
> 
> If the study is unrepresentative or "not externally valid" then it's up to the researcher to specify some approach to modeling the invalidity: $\theta ~ D(f(\theta), \sigma)$, where f() is some function that distorts the representativeness of the study. Which is to say, $f(\theta)$ is the expectation for a study with these distortions. Researcher's task is then to learn the form of f(). These distortions might be like sample bias, the country where the study was performed, or whatever, and all you're really doing is reweighting or adjusting the estimate to make more sense for the target population. If you can estimate parameters that determine f(), viola you can infer the posterior distribution for the true $\theta$. But this is what I mean when I say that none of this is really EXCLUSIVELY Bayesian. Reweighting/adjusting happens in non-Bayes world all the time; the main thing that is different is how you write the model and your ability to say that the population estimate is a "posterior of the true parameter, given the information learned from the data." One example of this kind of thing is maybe the multilevel regression and poststratification: we use national surveys to model the attitudes of different demographic groups, and then we use those model predictions plus census information to project estimates for smaller units, for example states or counties, based on the demographic composition of those units! This example goes the other direction from where Erica and Nick want to go (from representative to unrepresentative) but the technology sounds similar: estimate something in the data that you have, and map it into a space where you don't have data. MRP in political science comes from Bayes world and feels natural there, but nothing saying that it HAS to be Bayesian in its overall approach.
> 
> Now we get to a world where Bayes is more necessary. If there's something TRULY Bayesian that really makes no sense withoutBayes, it is the fact that I actually don't need data about f() in order to estimate $\theta$. This is because I have priors even in the absence of data. If all I have is priors about f(), then even if I collect data about ONLY $\mu$ and NOTHING about f(), I nonetheless update my information about $\theta$. This is because the parameters are functionally related through f(), so if I learn about the subsample then I learn about the population. Stated differently, learning about $\mu$ restricts the space of $\theta$ because I can basically "solve backward" using my priors about f. This is the stuff that is very natural in Bayes, and I can think of basically no analog in non-Bayes that lets you do something similar (other than picking point estimates for unknowns and simulating, which doesn't have the same theoretical coherence as a prior/posterior distribution). Of course, this means that inference on f() is subject to the priors that go into f(), which is exactly the kind of thing that non-Bayesians are super afraid of despite majorly misconstruing how this works (IMO). For one, the functional form of f() is the kind of thing non-Bayesians would make assumptions about anyway, so that's not unique to Bayes at all. And secondly, the priors that would go into f() would usually be generic enough that researchers aren't "picking their hypothesis" (a common and frankly stupid stereotype) so much as restricting the space of f() to rule out stuff that's frankly impossible. Happy to give you more concrete examples of the kind of "weakly informative priors" that someone would use in a situation like that if it's a route you want to dig into more. It's this kind of stuff that I think non-Bayesians are under-utilizing: how much extrapolation power you get by being willing to place even weak priors on stuff you can't exactly identify with data. And if you REALLY want to give a nod to Bayesian views of extrapolation, this is the area you'd want to dig into, because it's the stuff that doesn't really make sense without Bayes. You can sort of see Ken and I do this in our voter ID paper (which you can find on my website) though we kind of wimp out of fully placing priors on f().
> 
> Here's where things get really abstract because, gun to my head, we can be really scorched-earth and say that all extrapolation falls apart without a Bayesian notion of priors. Think about any model for data: $y ~ D(\theta)$, I think my data come from this distribution, and if I were to go out into the world and collect new data, my estimate for a new data point is characterized by this distribution assumption i.e. this prior for the data. If you try to lay out a formal definition of what "generalization" is, I would say that there is no such thing as generalization without an implicit prior that links your observed data to unobserved things that you want to project into. There are stats theorems out there called "no free lunch" theorems that basically say "all statistical inference is limiting the space of models that link parameters to data, and there is no way to improve your guess for a new data point using a model except to impose prior information on the system by way of the model." So this would be a hard line view of what priors mean in a philosophy of science (not necessarily quantitative or statistical, mind you), but that if you accept that view it trickles down into the more minor examples in a very natural way: the only way to generalize is by using priors to structure the connection between what I do observe and what I don't observe.





## Pragmatic Bayesian Modeling {#sec:bayes-how-to}


### Orientations Toward Prior Distributions

Various roles that priors take on

- merely facilitate posterior inference
- structural information
- weak information
- regularization / stabilization
- prior knowledge

A general orientation toward priors in this dissertation:

- Not about "stacking the deck" or hazy notions of "prior beliefs"
- information, not belief
- inference about the thing we care about (counterfactuals)
- structural information when we have it
- Causal inference: "agnosticism" is something valuable generally

Priors are not de-confounders

- downweighting, not upweighting



### Modeling Cultures in Political Science: Complexity and Agnosticism

Sidestepping priors

complexity of bayes vs. parsimony of causal inference NOT A RULE

Causal don't mean nonparametric

Bayesian don't mean Complex

At any rate: 

- stabilizing model-heavy designs (but priors in high dimensions are scary!)
    - that hierarchical conjoint thing
- sensitivity testing for noisy circumstances


### Principled Approaches to Model Parameterization

Models are a tool, set it up so that it works.






```{r diff-means-example}
means_data <- 
  tibble(
    x = seq(-1, 1, .05),
    unif = ifelse(x >= 0, dunif(x, 0, 1), NA),
    triangle = 
      ifelse(
        x < 0, 
        (2 * (x - -1)) / ((1 - -1) * (0 - -1)), 
        (2 * (1 - x)) / ((1 - -1) * (1 - 0))
      )
  ) 

plot_prior <- function(data, x, y) {
  ggplot(data) +
  aes(x = {{x}}, y = {{y}}) +
  geom_ribbon(
    aes(ymin = 0, ymax = {{y}}),
    fill = primary_light
  ) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1.5))
}
```

```{r plot-diff-means-example}
plot_prior(data = means_data, x = x, y = unif) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(
    x = TeX("Group Mean: $\\mu_{z}$"),
    y = "Prior Density",
    title = "Prior Densities for Difference in Means",
    subtitle = "If means have Uniform(0, 1) priors"
  ) +
plot_prior(data = means_data, x = x, y = triangle) +
  labs(x = TeX("Treatment Effect: $\\mu_{1} - \\mu_{0}$"), y = NULL) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) 
```



For instance, consider a simple experiment with a binary outcome variable $y_{i}$ and binary treatment assignment $z_{i} \in \{0, 1\}$. Suppose that the treatment effect of interest is a difference-in-means, $\bar{y}_{z = 1} - \bar{y}_{z = 0}$, estimated from a linear probability model. This linear probability model might be parameterized in two ways. First is a conventional regression setup,
\begin{align}
  y_{i} &= \alpha + \beta z_{i} + \epsilon_{i}
  (\#eq:diff-means-example)
\end{align}
where $\alpha$ is the control group mean, $\alpha + \beta$ is the treatment group mean, $\beta$ represents the difference in means, and $\epsilon_{i}$ is a symmetric error term for unit $i$. With the model parameterized in this way, the researcher must specify priors for $\alpha$ and $\beta$. Suppose that the researcher gives $\beta$ a flat prior to represent ignorance about the treatment effect. An equivalent _likelihood model_ for the data would be to treat each observation as a function of its group mean $\mu_{z}$.
\begin{align}
  y_{i} &= z_{i}\mu_{1} + \left(1 - z_{i}\right)\mu_{0}  + \epsilon_{i}
  (\#eq:separate-means-example)
\end{align}
Although the treatment effect $\beta$ from Equation \@ref(eq:diff-means-example) is equivalent to the difference in means $\mu_{1} - \mu_{0}$ from Equation \@ref(eq:separate-means-example), the parameterization of the model affects the implied prior for the difference in means. If the researcher gives a flat prior to both $\mu_{\mathbf{z}}$ terms, the implied prior for the difference in means will not be flat. Instead, it will be triangular, as shown in Figure \@ref(fig:plot-diff-means-example). The underlying mechanics of this problem are well-known in applied statistics---if we continue adding parameters, the Central Limit Theorem describes how the resulting distribution will converge to Normality---but it takes the explicit specification of priors to shine a light on the consequences of default prior choices in a particular case. In particular it shows how even flat priors, which are popularly regarded as "agnostic" priors because of their implicit connection to maximum likelihood estimators, do not necessarily imply flat priors about the researcher's key quantities of interest. Rather, flat priors can create a variety of unintended prior distributions that do not match the researcher's expectations. I return to this important idea in the discussion about setting priors for a probit model in Section \@ref(sec:probit-prior).

<!------- TO DO ---------
- probit in the model chapter?
- logit in this chapter?
------------------------->

```{r plot-diff-means-example, include = TRUE, fig.height = 5, fig.width = 8, out.width = "100%", fig.cap = "Model parameterization affects prior distributions for quantities of interest that are functions of multiple parameters or transformed parameters. The left panel shows that the difference between two means does not have a flat prior if the two means are given flat priors. Note that the $x$-axes are not fixed across panels."}
```

- equivalent parameterizations



```{r}
tibble(
  r = 1:50000
) %>%
  mutate(
    prior_control = runif(n = n(), 0, 1),
    treat_lower = 0 - prior_control,
    treat_upper = 1 - prior_control,
    prior_effect = rnorm(n = n(), sd = 10),
    prior_treat = prior_control + prior_effect
  ) %>%
  pivot_longer(cols = starts_with("prior_")) %>%
  ggplot(aes(x = value)) +
  facet_wrap(~ name) +
  geom_histogram(boundary = 0)
```


### Structural Priors and Weak Information

Structure (bounds), regularization (L1, L2), hierarchy


### Understanding Log Prior _Shape_

This is low-key pretty big


### Regularization-Induced Confounding

This is a huge, underappreciated problem in the broad ML-for-causal-inference world


### Priors for Imperfect Identifiability

"The Bayesian approach also clarifies what can be learned in the noncompliance problem when causal estimands are intrinsically not fully “identified.” In par- ticular, issues of identification are quite different from those in the frequen- tist perspective because with proper prior distributions, posterior distribu- tions are always proper." (Imbens and Rubin 1997)

This is where Imbens and Rubin push (also Horiuchi et al. example)

Randomization limits the impact of the Bayesian assumptions 

- "Classical random- ized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis." (Rubin 1978)




## Bayesian Opportunities



### Full Posterior Uncertainty

Multi-stage models

- propensity models (Zigler, Plummer/cutting)
- structural models
- to bootstrap or not to bootstrap?


Matching


Predictive/ML models

Multiple comparisons and regularization



### Priors for Key Assumptions

Relatedly: priors over models






### Regression Discontinuity with a Binary Outcome

Parameterization, structural information


### Meta-Analysis of Nonparametric Treatment Effects





## Other Frontiers of Bayesian Causal Inference 



### Beyond Estimation: Inferences About Models and Hypotheses

Inherit material from earlier section

### Priors are the Basis for all Generalization

No-Free-Lunch theorems

### Agnostic Causal Inference

<!------- TO DO ---------
- should this go into generalization, agnosticism sections above?
------------------------->

Conventional:
\begin{align}
  p(\theta \mid \mathbf{y}) &= \frac{p(\mathbf{y} \mid \theta)p(\theta)}{p(\mathbf{y})} \\
  p(\theta \mid \mathbf{y}) &\propto p(\mathbf{y} \mid \theta)p(\theta)
\end{align}

Implied:
\begin{align}
  p(\theta \mid \mathbf{y}, \mathcal{H}) &= \frac{p(\mathbf{y} \mid \theta, \mathcal{H})p(\theta \mid \mathcal{H})}{p(\mathbf{y} \mid \mathcal{H})}
\end{align}





