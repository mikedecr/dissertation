# A Bayesian Framework for Causal Inference in Political Science {#ch:causality}

<!------- TO DO ---------
- This might need to be chapter 2!!!! 
- The model has a discussion of priors that might feel out of place
  without first couching it in a viewpoint of Bayes for the project.
- Is there only one viewpoint? No? 
  The model is a "pragmatic Bayes" (MCMC for fitting, structural priors),
  whereas the causal inf stuff is mixed in its view.
  Sometimes its "normalizing Bayes" (information for sensitivity testing),
  other times is merely "structural" WIPs. 
  Maybe one thing to articulate (to myself, to others) is a position on how
  structural priors make us think about properties of Bayes estimators.
  (in the RDD case, the OLS model is consistent but we aren't in asymptopia.
   How else do we evaluate the properties of the Bayes estimator
   when we include structural information?)
------------------------->

<!------- TO DO ---------
- is the intro too specific?
- Have an abstract and then an outline of the chapter?
- missing right now: connect to political science!
------------------------->

Before I employ the estimates of party-public ideology obtained in Chapter&nbsp;\@ref(ch:model), this chapter discusses a Bayesian framework for causal inference in political science. 
This framework addresses two major themes in the empirical problems that I confront later in the project, among several other minor themes.

First, this project views causal inference as a problem of posterior predictive inference
Causal models are tools that enable inferences about missing data: the data that would be observed if a key independent variable could be set to a different value
The unobserved data are "unobserved potential outcomes" in the Rubin causal framework or "counterfactual observations" in the Pearl framework.
<!------- TO DO ---------
- Rubin and Pearl cites
------------------------->
Regardless of the notational/semantic conventions employed, "Bayesian causal inference" is a modeling framework for structuring the inferences about the probability distribution of unobserved potential outcomes, having observed one set of potential outcomes
In other words, which counterfactual data are plausible, given the observed data and a model relating the data to causal queries of interest? In this sense, the Bayesian approach to causal inference confronts our uncertainties about the "left-hand side" of our model


Second, the Bayesian approach confronts our uncertainties about the "right-hand side" of the model as well
Causal estimands (to use the Rubin terminology) are comparisons of potential outcomes are two hypothetical values of a treatment
In many cases, these estimands are comparisons of a unit's outcome under an observed treatment against an unobserved treatment
For this project, the treatment of interest---ideology in district-party-publics---is not directly observed
It is instead only observed up to a probability distribution from a measurement model
(To use potential outcomes notation, instead of observing $Y(\theta)$, we observe $Y(p(\theta))$)
Bayesian probabilistic models provide machinery to describe causal queries when we have only a probabilistic understanding of the observed data as well as the unobserved data.

This chapter unpacks these issues according to the following outline
First, I provide a quick preview to the notation and terminology of two dominant approaches to causal inference in applied statistics: the "Rubin model" of potential outcomes and the "Pearl model" of _do_-calculus
I then offer a Bayesian reinterpretation of these models, where unobserved potential outcomes/counterfactuals are characterized by probability distributions that are conditioned on the observed data
This raises the daunting issue of prior specification for model parameters: how priors are inescapable for many causal claims, how priors can provide important structure to improve causal inferences, and practical advice for constructing and evaluating priors
Finally, I provide examples of the Bayesian causal framework at work, highlighting how priors contribute to a causal study at different levels of abstraction.



```{r knitr-03-causality, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

```{r r-03-causality, cache = FALSE}
library("knitr")
library("here")
library("magrittr")
library("tidyverse")
library("extrafont")
library("latex2exp")
library("scales")
library("patchwork")
```



## Essential Concepts

### Bayesian Inference

Conventional:
\begin{align}
  p(\theta \mid \mathbf{y}) &= \frac{p(\mathbf{y} \mid \theta)p(\theta)}{p(\mathbf{y})} \\
  p(\theta \mid \mathbf{y}) &\propto p(\mathbf{y} \mid \theta)p(\theta)
\end{align}

Implied:
\begin{align}
  p(\theta \mid \mathbf{y}, \mathcal{H}) &= \frac{p(\mathbf{y} \mid \theta, \mathcal{H})p(\theta \mid \mathcal{H})}{p(\mathbf{y} \mid \mathcal{H})}
\end{align}



Joint model of the system:

- likelihood :: prior
- Lampinen Vehtari 2001: likelihood as "prior for the data" is the basis for all generalization from any finite model
<!-- Describing the prior information explicitly distinguishes the Bayesian approach from the maximum likelihood (ML) methods. It is important to notice, however, that the role of prior knowledge is equally important in any other approach, including the ML. Basically, all generalization is based on the prior knowledge, as discussed in Lemm (1996, 1999); the training samples provide information only at those points, and the prior knowledge provides the necessary link between the training samples and the not yet measured future samples.  -->


Better parameters with priors (reduce variance)


### Causal Models

Intro Rubin and Pearl models.

Strictly speaking, causality is never observed. It is only inferred with the help of assumptions.

Better parameters "by design" (reduce bias)

Not _inherently_ the same as "agnostic" inference though we fux with both.


### Shared Goals, Different Tactics



## Modeling Potential Outcomes with Posterior Distributions

Causal model: $y_{i}(1)$ and $y_{i}(0)$

Observed data: $y_{i} = z_{i}y_{i}(1) + (1 - z_{i})y_{i}(0)$

Unobserved data: $\tilde{y}_{i}$

Inference about $\tilde{y}_{i}$: $p(\tilde{y}_{i} \mid \mathbf{y}) \propto p(\tilde{y}_{i} \mid \theta, \mathbf{y})p(\theta)$


This fits an ML approach:

- ML approach to causal inference recasts the problem as a prediction problem for the treatment assignment.
- Treatment effects are then integrated over the uncertainty in the propensity
- This propagates uncertainty using the exact model machinery, rather than a post-hoc computational workaround like bootstrapping
    - Bootstrapping does have an appealing feature that it isn't making a parametric assumption about errors in the model. It simply uses the "sampling uncertainty" intuition to build intuitive bounds on effect uncertainty.
    - However in many real-world cases the data we have cannot be resampled, so the framework for inference under bootstrapping doesn't make theoretical sense without appealing to a "superpopulation" philosophy. 
    - Bayes naturally deals with this by assigning a probability distribution to the unmodeled features, which mechanistically follows a similar assumption as a parametric assumption about errors in any typical regression case: the prior isn't really extra. 
    - If this assumption isn't something you want, it is possible to generalize the model by specifying an overdispersion parameter and letting the data inform what which model estimates can be rationalized against plausible overdispersion parameters. For example, Kruske's BEST approach to difference-in-means testing. 
- In a Bayesian framework, we have posterior More of a "priors to facilitate posteriors" approach, not a prior information view (since there is some formal data peeking involved)




### What "Parameters" Mean in Modeling and Estimation

What was I trying to say here? 

The causal parameter is a feature of the _causal model_. The causal model is the model if you knew everything. When we impose assumptions for estimating, we can say under which conditions this parameter (or an average of the parameter in a finite sample) can be _estimated_. However, the estimator for the parameter is a different thing.

Bayesian estimation is a different _estimation_ framework given the same causal model. This is easy to see when we think about MLE models as special cases of Bayesian models with all log-likelihoods equally weighted. We have parameters given by some "ideal" model, but the parameters are estimated with a pragmatic penalty term.

Oganisian and Roy (2020) "identification" assumptions vs. "statistical" assumptions. Identification assumptions get you to a place where you can express causal effects in terms of expectations about $Y$ given treatment and confounders, integrating over confounders. "Statistical" assumptions define how we think we can build $E[Y]$

- Bayesian inference _begins_ as technology for thinking about statistical assumptions.
- It eventually becomes technology for experimenting with identification assumptions.

Do battle with the "implied nonparametric estimator" framework.

- Causal models are manipulated to express causal _estimands_.
- the "nonparametric estimators" are usually just averages of treatment and control group outcomes, under ignorability assumptions.
- Parametric models for estimands are usually a byproduct of thinking within an MLE/regression framework. Many causal inf techniques don't do that. Instead they try to simply predict E[y(1) - y(0)] and don't care about the interpretability of the other RHS terms.
- Thinking outside the "typical econometric" framework it's actually kinda easy to see how one flavor of fitting Bayesian models for causal effects. If the technology for prediction is Bayesian, you just get the prediction and the posterior distribution. You then deal with the bias/variance properties of $\hat{y}$ or $\tilde{y}$ (unobserved counterfactuals) but at least your focus is on the predictions rather than coefficients (which, from this view, who cares about those?)
- This actually fits quite nicely with the machine learning approach to causal inference



We can think bigger about Bayesian _inference_ for a parameter as distinct from Bayesian _estimation_ of the in-sample quantity. 
This lets us use a nonparametric data-driven estimator for the data, but the "inference" or "generalization" still has a prior.
For instance a sample mean estimates a population mean without a likelihood model for the data, but inference about the population mean often follows a parametric assumption from the Central Limit Theorem that the sampling distribution from the mean is asymptotically normal (but doesn't have to, c.f. bootstrapping).
<!------- TO DO ---------
- Read more about sharp null and an RI framework for inference, 
  since this runs up against the idea of asymptotic normality of the means.
------------------------->
Even if the point estimator we use for a mean is unbiased, we can assimilate external information during the interpretation of the estimator (biasing the inference without biasing the point estimator).
Restated: the posterior distribution is a weighted average of the raw point estimator and external information, rather than biasing the data-driven estimate directly.

Even bigger: Bayesian inference about _models_ (Baldi and Shahbaba 2019). 
This is probably where I have to start my justification for this? 
The _entire point_ of causal inference is to make inferences about counterfactuals given data (Rubin 1978?).
Invoking Bayesian inference is really the only way to say what we _want_ to say about causal effects: what are the plausible causal effects given the model/data. We do _not_ care about the plausibility of data given the null (as a primary QOI). 
- Probably want to use Harrell-esque language? Draw on intuition from clinical research, or even industry. We want our best answer, not a philosophically indirect weird jumble.
- This probably also plays into the Cox/Jeffreys/Jaynes stuff I have open on my computer.
<!------- TO DO ---------
- Corey Yanofsky recommended reading
- Gelman/Shalizi philosophy of Bayes
- Induction/Deduction in Bayes
- etc.
- symposium about "objective" Bayes
- And then also Gelman/Simpson/Betancourt "context of the likelihood"
  => we have priors about the WORLD, not about model parameters.
------------------------->

<!------- TO DO ---------
- soul searching about a philosophy of Bayes to stake out for the project?
- "pragmatic bayesian workflow" - priors are "just a part of the model" that need to be checked w/ prior predictive checking and so on.
- vs. the "philosophical coherence" view.
- damnit don't spend too much time on this because it just needs to be an orientation to stick decisions to.
- We're already leaning in the "pragmatic" direction from the IRT model text.
------------------------->

This presumes an m-closed world(?right?), which maybe we don't like (Navarro, "Devil and Deep Blue Sea").
<!------- TO DO ---------
- Read Navarro CLOSELY!
- Read ARONOW and MILLER: how to square "intra-model agonisticism" with 
  some philosophical view of what Bayes is.
- Maybe this is terminology I should work on! Draw some helpful lines that 
  don't already exist for defining subtypes of agnosticism or other
  epistemic bundles of things.
  (there is also some paper on "types of Bayesians"?
   mentioned in a McElreath talk)
------------------------->
Me debating with myself: how to think about Bayesian model selection vs "doubly robust" estimation ideas...
Maybe some hybrid view in the "quasi-experimental" approach to causal model selection. We estimate two models from the same data—one with a treatment parameter and one where we impose no treatment effect—and compare models using some likelihoodist or Bayes factor measure of evidence.
<!------- TO DO ---------
 - articulate a good beef with the paper's terminology of "quasi-experimental"
 -------------------------> 

What is THIS project going to do?

- Pragmatic view of priors?
- Are we doing more flexible covariate adjustment?
- Maybe we should decide this AFTER we experiment with Ch 4 and 5 data/methods
<!------- TO DO ---------
- pronouncement of project values
------------------------->



## Exposing Hidden Assumptions

### Priors and Model Parameterization

<!------- TO DO ---------
- this needs reverse outline
------------------------->

Priors are defined with respect to a model of the data (the likelihood). 
<!------- TO DO ---------
- discuss Betancourt, Simpson, Gelman
------------------------->
We may have priors about the way the world works, but we rarely have priors about model parameters. This is because parameters are an invention in the model. They are mathematical abstractions similar to points and lines, so they only exist when we translate the world into a mathematical language. This means that the mathematical representation of the world is in direct dialog with the choices available to a researcher about how to encode prior information. In the real world, the prior information that I have about the world isn't affected by a mathematical representation of the world. As a researcher, the way I encode prior information depends on the choices I make about that mathematical representation.

One essential feature for understanding prior choices in practice is the _parameterization_ of the data model, $p(y \mid \phi)$, for some generic parameter $\phi$.^[
  Bayesian practitioners sometimes refer to the data model as the "likelihood." 
  This can be confusing because the "likelihood function" more traditionally refers to the _product_ of the data probabilities under the data model.
  References to the "parameterization of the likelihood" should be understood as interchangeable with "parameterization of the data model," since the former is determined entirely by the latter. 
]
We say that a data model has an "equivalent reparameterization" if for some transformed parameter $\psi = f(\phi)$, the function that defines the data model can be rewritten in terms of $\psi$ and return an equivalent likelihood of the data. More formally, the parameterization is equivalent if $p(y \mid \phi) = p(y \mid \psi)$ for all possible $y$.

In a maximum likelihood framework, equivalent parameterizations are a more benign feature of the modeling framework. Reparameterization may result in likelihood surfaces that have easier geometries for optimization algorithms to explore, but the _value_ of the likelihood function is unaffected by the algebraic definition or parameterization of the likelihood function. For instance, a Normally distributed variable $x$ with mean $\mu$ could be parameterized in terms of standard deviation $\sigma$ or in terms of precision $\tau = \frac{1}{\sigma^{2}}$, but the resulting density is unaffected.
\begin{align}
  \frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{x - \mu}{2\sigma}\right)^{2}}
  &=
  \sqrt{\frac{\tau}{2\pi}}e^{-\frac{\tau\left(x - \mu\right)^{2}}{2}}
  (\#eq:normal-sigma-equal-tau)
\end{align}
The consequence for Bayesian analysis, however, is that the parameterization of the data model determines the set of parameters and their functional relationship to the data.


<!------- TO DO ---------
- linear reparameterization: easy to understand
- decide if non-centering is worth it to discuss here
- nonlinear reparameterization: a much bigger problem
------------------------->



One example of equivalent reparameterization arises with the different possible ways to write a linear regression model. The first form specifies $y_{i}$ for unit $i$ as a linear function of $x_{i}$ with a random error that is mean $0$ and standard deviation $\sigma$.
\begin{align}
  (\#eq:ols-noncenter)
  y_{i} &= \alpha + \beta x_{i} + \varepsilon_{i}, & \text{where } \varepsilon_{i} &\sim \mathrm{Normal}\left(0, \sigma \right)
\end{align}
The second form, more common when viewing linear regression in the framework of generalized linear models, is to express $y_{i}$ directly as the random variable, with a conditional mean defined by the regression function and standard deviation $\sigma$.
\begin{align}
  (\#eq:ols-center)
  y_{i} &\sim \mathrm{Normal}\left( \alpha + \beta x_{i},\sigma\right)
\end{align}
Algebraically, these two models are identical. The difference is only a matter of which component has the distributional assumption. In Equation&nbsp;\@ref(eq:ols-noncenter), the distribution is assigned to $\varepsilon_{i}$, so $y_{i}$ is a random variable only by way of $\varepsilon_{i}$. In Equation&nbsp;\@ref(eq:ols-center), we assign the distributional assumption directly to $y_{i}$, bringing the regression function into the mean rather than "factoring it out" of the distribution.

```{r normal-factoring}
normal_n <- 500
normal_mu <- 4
normal_sd <- 2
```

```{r plot-normal-factoring, include = TRUE, fig.cap = "Demonstration of centered and non-centered parameterizations for a Normal distribution. The non-centered parameterization is statistically equivalent, but the location and scale are factored out of the distribution."}
set.seed(0987)
tibble(
  y1 = rnorm(n = normal_n, mean = normal_mu, sd = normal_sd),
  y2 = normal_mu + normal_sd * rnorm(n = normal_n, mean = 0, sd = 1)
) %>%
  pivot_longer(
    cols = everything()
  ) %>%
  ggplot() +
  aes(x = value, color = name) +
  # geom_vline(
  #   xintercept = c(2, 4, 6),
  #   color = "gray"
  # ) +
  # geom_hline(
  #   yintercept = c(pnorm(-1), pnorm(0), pnorm(1)),
  #   color = "gray"
  # ) +
  stat_ecdf(size = 1) +
  coord_cartesian(
    xlim = c(normal_mu - 2 * normal_sd, normal_mu + 2 * normal_sd)
  ) +
  scale_x_continuous(
    breaks = seq(-10, 10, normal_sd)
  ) +
  labs(
    title = str_glue(
      "Empirical CDF of Normal({normal_mu}, {normal_sd}) Samples"
    ),
    subtitle = str_glue(
      "{normal_n} Draws from Centered and Non-Centered Distributions"
    ),
    x = TeX("Random Variable $\\nu$"),
    y = "ECDF"
  ) +
  scale_color_manual(values = c(primary, secondary)) +
  theme(legend.position = "none")
```

The linear regression context is one context where the choice of parameterization appears. These two parameterizations are typically called the "centered" and "non-centered" parameterization for a Normal distribution. In the centered parameterization, the random variable is drawn from a distribution "centered" on a systematic component, whereas the non-centered distribution factors out any location and scale information from the distribution, such that the only remaining random variable is a standardized variate. The equations below describe a Normal variable $\nu$ with mean $`r normal_mu`$ and standard deviation $`r normal_sd`$.
\begin{align}
  \text{Centered Parameterization:} && \nu &\sim \mathrm{Normal}(`r normal_mu`, `r normal_sd`) \\
  \text{Non-Centered Parameterization:} && \nu &= `r normal_mu` + `r normal_sd`z, & \text{where } z &\sim \mathrm{Normal}(0, 1)
\end{align}
To demonstrate that these parameterizations are equivalent, I simulate $`r normal_n`$ simulations from each parameterization and plot their empirical cumulative distribution functions alongside each other in Figure&nbsp;\@ref(fig:plot-normal-factoring). Because the distributions are the same, the empirical CDFs are identical except for random sampling error. 

```{r, include = TRUE, fig.width = 10, fig.height = 2, out.width = "100%", fig.cap = "A spectrum of attitudes toward priors. "}
tibble(
  x = c(0.5, 2, 3.5),
  y = 1,
  label_head = c(
    "Minimalist Prior",
    "Pragmatic Prior",
    "Principled Prior"
  ),
  label_body = c(
    "Priors are nuisances,\nmerely facilitate inference/MCMC",
    "Priors are helpful:\nstructural info, regularization",
    "Priors ensure coherence,\nencode beliefs & hypotheses"
  )
) %>%
  ggplot() +
  aes(x = x, y= y) +
  geom_text(
    aes(y = 0.75, label = label_head),
    vjust = 0,
    fontface = "bold"
  ) +
  geom_text(
    aes(y = 0.75, label = label_body),
    vjust = 1.5
  ) +
  annotate(
    geom = "segment",
    arrow = arrow(
      angle = 30, length = unit(0.25, "inches"), ends = "both", type = "open"
    ),
    x = 0, xend = 4, y = 1, yend = 1
  ) +
  annotate(
    geom = "text",
    x = 0, y = 1, hjust = 1.1,
    label = "Less Informative"
  ) +
  annotate(
    geom = "text",
    x = 4, y = 1, hjust = -0.1,
    label = "More Informative"
  ) +
  coord_cartesian(ylim = c(0.25, 1.25), xlim = c(-1, 5)) +
  theme_void()
```





Problems of beliefs:

- No degree of belief.
- Parameterization makes this too challenging.
- Prior might change depending on what I ate for lunch. 
- "Elicitation" of priors satisfying the wrong audience, or at the very least can be easily misused. We don't want to elicit priors about arcane model parameters. We want to elicit priors about the _world_ (Gill Walker)d


Problems of nuisance prior

- parameterization gets you again
- the MLEs are unstable, overfit
- make the regularization argument in-sample




Pragmatic view of priors

- we're between full information and nuisance prior
- Weak information: structure, regularization, identification
- Structural information about parameters
- regularization toward zero (L1, L2), learning by pooling
- stabilizing weakly identified parameters, separation, etc.


Parameters are a _choice_. 

- They are part of the _rhetoric_ of a model. Sometimes we make pragmatic choices (something is easy to give an independent prior to, but independence isn't always valuable per se). Sometimes we make principled choices (normality, laplace, etc). 
- They deserve scrutiny (else just "excrete your posterior") and are a part of the model that you should check and diagnose. 
- They aren't merely a nuisance because we can use them to our benefit, 
- sometimes when we parameterize a problem to reveal easier things to place convenient priors on
<!------- TO DO ---------
- Gelman "Parameterization" paper: parameterization for convenience often reveals a new family of models. For Bayesian inference, it reveals a new family of priors. For _causal_ inference, we should pay careful attention to the way parameterization bears on the priors that we do and do not feel comfortable using, and the way that parameterization presents opportunities to improve on other work.
------------------------->





### Priors as Data Falsification



<!------- TO DO ---------
- figure this out
DATA FALSIFICATION digression
------------------------->

Data falsification versus unavoidable choice:
imagine a study with a posterior distribution $p(\mu \mid \mathbf{y})$ that is proportional to the likelihood times the prior. 
  \begin{align}
    (\#eq:propto)
    p(\mu \mid \mathbf{y}) &\propto p(\mathbf{y} \mid \mu)p(\mu)
  \end{align}
Rewrite the right side in product notation for $n$ observations of $y_{i}$ for units indexed by $i$, letting $l(y_{i}) = p(y_{i} \mid \mu)$
  \begin{align}
    p(\mu \mid \mathbf{y}) &\propto \prod\limits_{i = 1}^{n}l(y_{i})p(\mu)
    (\#eq:propto-index)
  \end{align}
Suppose that we express this proportionality on the log scale, where the log posterior is proportional to the log likelihood plus the log posterior.
  \begin{align}
    \begin{split}
      \log p(\mu \mid \mathbf{y}) 
        &\propto \sum\limits_{i = 1}^{n} \log l(y_{i}) 
          + \sum\limits_{i = 1}^{n} \log p(\mu) \\[12pt]
        &\propto \log l(y_{1}) + \log p(\mu)
                 + \log l(y_{2}) + \log p(\mu)
                 + \ldots
                 + \log l(y_{n}) + \log p(\mu)
    \end{split}
  (\#eq:log-propto-index)
  \end{align}


The setup in \@ref(eq:log-propto-index) highlights a few appealing intuitions.
First, it shows how each observation "adds information" to the log posterior distribution.
Data that are more likely to be observed given the parameter (larger $l(y_{i})$ values) increase the posterior probability that parameter. 
We also see that the prior probability "adds information" to the posterior in the same way data add information, captured by the addition of each $p(\mu)$ term. 
Parameters that are more probable in the prior are more probable in the posterior.

The proportionality \@ref(eq:log-propto-index) also reveals how the posterior "learns" from flat priors.
A flat priors implies that prior probability $p(\mu)$ is constant for all potential values of $\mu$.
Because \@ref(eq:log-propto-index) is a proportionality, this lets us disregard $p(\mu)$ entirely by factoring it out of the proportionality, leaving us with an expression that the log posterior is proportional to the likelihood of the data only if the prior is flat.
\begin{align}
  \log p(\mu \mid \mathbf{y}) 
        &\propto
        \log l(y_{1}) + \log l(y_{2}) + \ldots + \log l(y_{n})
  (\#eq:log-propto-flat)
\end{align}
If $p(\mu)$ is not flat, however, and $p(\mu)$ varies across values of $\mu$, we can no longer ignore $p(\mu)$ in \@ref(eq:log-propto-index) shows that $p(\mu)$ varies across values $\mu$.
Not only does this prevent us from dropping $p(\mu)$ from the proportionality, but it also reveals how the prior "adds information" to the posterior by the same mechanism that observations do: adding to the log posterior distribution. 
This general expression where both data and priors contribute to the posterior distribution has led some researchers to argue the Bayesian inference with non-flat priors is analytically indistinguishable from data falsification [@garcia-perez:2019:bayes-data-falsification].
We can highlight this behavior by obscuring each $p(\mu)$ term with a square $\square$.
\begin{align}
  \log p(\mu \mid \mathbf{y}) 
        &\propto
        \log l(y_{1}) + \square + \log l(y_{2}) + \square \ldots + \log l(y_{n}) + \square
  (\#eq:log-propto-falsification)
\end{align}
Behind each square is _some contribution_ to the log posterior.
The fact that it adds information to the log posterior is unaffected by whether the hidden term is the probability of an additional observation $l(y_{i})$ or the prior probability of a parameter value $p(\mu)$.
<!------- TO DO ---------
- END FALSIFICATION
------------------------->


<!------- TO DO ---------
- but where was I going with this?
------------------------->



### Flatness is a Relative, Not an Absolute, Property of Priors

<!------- TO DO ---------
- does this go in previous section?
------------------------->

The primary resistance to Bayesian inference in applied research is the need to set a prior at all. To many researchers, the prior distribution is an additional assumptions that is never feels justified because it is external to the data. Often researchers wish to sidestep this choice altogether, preferring a "flat" prior that prefers all parameters equally.

We have seen so far that the parameterization of a model has consequences for prior specification. Reparameterization may result in an algebraically equivalent likelihood
<!------- TO DO ---------
- define "equivalent reparameterization"
------------------------->


The incoherence of flatness: 

- no universally valid strategy for specifying flat priors because it is always possible to rearrange the data model either by transforming a parameter or otherwise rearranging the likelihood. 

Consider an experiment with a binary treatment $Z$ and a binary outcome variable $Y$. We want to determine the effect of $Z$ by comparing the success probability in the treatment group, $\pi_{1}$, to the success probability in the control group, $\pi_{0}$.



- "no way to conceptualize an uninformative prior because you can always rearrange the problem through a reparameterization or transformation of a parameter"
- examples of transformations having crazy implications/MLE being wild (logit).
- Jeffreys prior: actually a very limited range of priors that satisfy an "invariance" property. My words: such that the "amount of information obtained from data about is invariant to parameterization of the likelihood, for all possible values of the parameter," or, "the only way for the posterior distribution to be exactly the same, given the same data, for all true parameter values (?), is the Jeffreys prior," or, regardless of the data, I will learn the same thing about the generative model regardless of which equivalent parameterization of the generative model is used.  
    - is it worth it to think about the theoretical meaning of information
    - how does flatness reflect information in nonlinear scales?


Suppose we have some posterior distribution which relies on some parameter vector $\overrightarrow{\alpha}$. 
\begin{align}
  p\left(\overrightarrow{\alpha} \mid y\right) &\propto 
    p(y \mid \overrightarrow{\alpha} )
    p( \overrightarrow{\alpha} )
\end{align}
Consider some alternate parameterization of the likelihood parameterized by $\overrightarrow{\beta}$. 

Nonlinear transformation of $\theta$ does not preserve a uniform density over parameters.





Alex meeting takeaways:

- every prior has a "covariant" prior in a different parameterization
- the posteriors will be covariant as well.
- The way you get between them is by transforming the parameter and doing the appropriate Jacobian transformation to the density.
- Jeffrey's priors are a special case of this where the prior is proportional to the determinant of the information matrix. This has the beneficial property of "optimal learning" from the data. For example, flat Beta prior doesn't "hedge toward 50" in quite the same way.




### Priors on Models and "No-Free-Lunch" Theorems

Models (likelihoods) are priors

Identification assumptions are priors

Generalization to any population is a prior

We are always doing violence, but the framework lets us build out more and more general models to structure our uncertainties


## Pragmatic Bayesian Modeling

### Orientations Toward Prior Distributions

Various roles that priors take on

- merely facilitate posterior inference
- structural information
- weak information
- regularization / stabilization
- prior knowledge

A general orientation toward priors in this dissertation:

- Not about "stacking the deck" or hazy notions of "prior beliefs"
- information, not belief
- inference about the thing we care about (counterfactuals)
- structural information when we have it
- Causal inference: "agnosticism" is something valuable generally

Priors are not de-confounders

- downweighting, not upweighting



### Modeling Cultures in Political Science: Complexity and Agnosticism

Sidestepping priors

complexity of bayes vs. parsimony of causal inference NOT A RULE

Causal don't mean nonparametric

Bayesian don't mean Complex

At any rate: 

- stabilizing model-heavy designs (but priors in high dimensions are scary!)
    - that hierarchical conjoint thing
- sensitivity testing for noisy circumstances


### Principled Approaches to Model Parameterization

Models are a tool, set it up so that it works.






```{r diff-means-example}
means_data <- 
  tibble(
    x = seq(-1, 1, .05),
    unif = ifelse(x >= 0, dunif(x, 0, 1), NA),
    triangle = 
      ifelse(
        x < 0, 
        (2 * (x - -1)) / ((1 - -1) * (0 - -1)), 
        (2 * (1 - x)) / ((1 - -1) * (1 - 0))
      )
  ) 

plot_prior <- function(data, x, y) {
  ggplot(data) +
  aes(x = {{x}}, y = {{y}}) +
  geom_ribbon(
    aes(ymin = 0, ymax = {{y}}),
    fill = primary_light
  ) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1.5))
}
```

```{r plot-diff-means-example}
plot_prior(data = means_data, x = x, y = unif) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(
    x = TeX("Group Mean: $\\mu_{z}$"),
    y = "Prior Density",
    title = "Prior Densities for Difference in Means",
    subtitle = "If means have Uniform(0, 1) priors"
  ) +
plot_prior(data = means_data, x = x, y = triangle) +
  labs(x = TeX("Treatment Effect: $\\mu_{1} - \\mu_{0}$")) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) 
```



For instance, consider a simple experiment with a binary outcome variable $y_{i}$ and binary treatment assignment $z_{i} \in \{0, 1\}$. Suppose that the treatment effect of interest is a difference-in-means, $\bar{y}_{z = 1} - \bar{y}_{z = 0}$, estimated from a linear probability model. This linear probability model might be parameterized in two ways. First is a conventional regression setup,
\begin{align}
  y_{i} &= \alpha + \beta z_{i} + \epsilon_{i}
  (\#eq:diff-means-example)
\end{align}
where $\alpha$ is the control group mean, $\alpha + \beta$ is the treatment group mean, $\beta$ represents the difference in means, and $\epsilon_{i}$ is a symmetric error term for unit $i$. With the model parameterized in this way, the researcher must specify priors for $\alpha$ and $\beta$. Suppose that the researcher gives $\beta$ a flat prior to represent ignorance about the treatment effect. An equivalent _likelihood model_ for the data would be to treat each observation as a function of its group mean $\mu_{z}$.
\begin{align}
  y_{i} &= z_{i}\mu_{1} + \left(1 - z_{i}\right)\mu_{0}  + \epsilon_{i}
  (\#eq:separate-means-example)
\end{align}
Although the treatment effect $\beta$ from Equation&nbsp;\@ref(eq:diff-means-example) is equivalent to the difference in means $\mu_{1} - \mu_{0}$ from Equation&nbsp;\@ref(eq:separate-means-example), the parameterization of the model affects the implied prior for the difference in means. If the researcher gives a flat prior to both $\mu_{\mathbf{z}}$ terms, the implied prior for the difference in means will not be flat. Instead, it will be triangular, as shown in Figure&nbsp;\@ref(fig:plot-diff-means-example). The underlying mechanics of this problem are well-known in applied statistics---if we continue adding parameters, the Central Limit Theorem describes how the resulting distribution will converge to Normality---but it takes the explicit specification of priors to shine a light on the consequences of default prior choices in a particular case. In particular it shows how even flat priors, which are popularly regarded as "agnostic" priors because of their implicit connection to maximum likelihood estimators, do not necessarily imply flat priors about the researcher's key quantities of interest. Rather, flat priors can create a variety of unintended prior distributions that do not match the researcher's expectations. I return to this important idea in the discussion about setting priors for a probit model in Section&nbsp;\@ref(sec:probit-prior).
<!------- TO DO ---------
- probit in the model chapter?
- logit in this chapter?
------------------------->

```{r plot-diff-means-example, include = TRUE, fig.height = 4, fig.width = 8, out.width = "100%", fig.cap = "Model parameterization affects prior distributions for quantities of interest that are functions of multiple parameters or transformed parameters. The left panel shows that the difference between two means does not have a flat prior if the two means are given flat priors. Note that the $x$-axes are not fixed across panels."}
```

- equivalent parameterizations
### Structural Priors and Weak Information

Structure (bounds), regularization (L1, L2), hierarchy


### Understanding Log Prior _Shape_

This is low-key pretty big


### Regularization-Induced Confounding







## Bayesian Opportunities



### Full Posterior Uncertainty

Multi-stage models

- propensity models
- structural models
- to bootstrap or not to bootstrap?

Predictive models

Multiple comparisons and regularization



### Identification Assumptions as Priors

Relatedly: priors over models




### Regression Discontinuity with a Binary Outcome

Parameterization, structural information


### Meta-Analysis of Nonparametric Treatment Effects





## Other Frontiers of Bayesian Causal Inference 



### Beyond Estimation: Inferences About Models and Hypotheses

Inherit material from earlier section

### Priors are the Basis for all Generalization

No-Free-Lunch theorems

### Agnostic Causal Inference





