# Bayesian Causal Models for Political Science {#ch:causality}

<!------- TO DO ---------
- why no build with tufte?
------------------------->
```{r stopper, eval = FALSE, cache = FALSE, include = FALSE}
knitr::knit_exit()
```


```{r knitr-03-causality, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

<!-- who knows if this works on next latex build -->
$\renewcommand{\ind}[0]{\perp \!\!\! \perp}$
$\renewcommand{\doop}[1]{\mathit{do}\left(#1\right)}$
$\renewcommand{\diff}[1]{\, \mathrm{d}#1}$
$\renewcommand{\E}[1]{\mathbb{E}\left[#1\right]}$
$\renewcommand{\p}[1]{p\left(#1\right)}$



```{r r-03-causality, cache = FALSE}
library("knitr")
library("here")
library("ggdag")
library("magrittr")
library("tidyverse")
library("ggplot2")
library("latex2exp")
library("scales")
library("patchwork")
library("broom")
library("brms")
library("tidybayes")
```


<!------- TO DO ---------
This might need to be chapter 2!!!! 

- The model has a discussion of priors that might feel out of place
  without first couching it in a viewpoint of Bayes for the project.
- Is there only one viewpoint? No? 
- The model is a "pragmatic Bayes" (MCMC for fitting, structural priors),
  whereas the causal inf stuff is mixed in its view.
  Sometimes its "normalizing Bayes" (information for sensitivity testing),
  other times is merely "structural" WIPs. 
  Maybe one thing to articulate (to myself, to others) is a position on how
  structural priors make us think about properties of Bayes estimators.
  (in the RDD case, the OLS model is consistent but we aren't in asymptopia.
   How else do we evaluate the properties of the Bayes estimator
   when we include structural information?)
------------------------->


<!------- TO DO ---------
- is the intro too specific?
- Have an abstract and then an outline of the chapter?
- missing right now: connect to political science!
------------------------->

Before I employ the estimates of party-public ideology obtained in Chapter \@ref(ch:model), this chapter discusses a Bayesian view of causal inference.
This framework addresses two major themes in the empirical problems that I confront later in the project, as well as several other minor themes.

First, this project views causal inference as a problem of posterior predictive inference.
Causal models are tools for inferences about missing data: what we would observe if a treatment variable were set to a different value.
The unobserved data are "unobserved potential outcomes" in the Rubin causal framework or "counterfactual outcomes" in the Pearl framework [@rubin:2005:potential-outcomes; @pearl:2009:causality].
Causal inference can be Bayesian if the target of interest is the probability distribution of unobserved potential outcomes (or the probability distribution of any causal estimand), conditioning on the observed data.
In this chapter, I will argue that this is what researchers are implicitly trying to obtain, even if implicitly, nearly all of the time.

Second, the Bayesian approach incorporates uncertainties about the key independent variable in this project.
Causal estimands (to use the Rubin terminology) are comparisons of potential outcomes are two hypothetical values of a treatment, usually a unit's outcome under an _observed_ treatment versus an _unobserved_ treatment.
The data in this project frustrate the typical structure of a causal estimand because the treatment value of interest---policy ideology in a district-party public---is not observed.
Instead, it is estimated up to a probability distribution specified by the measurement model in Chapter \@ref(ch:model).
Uncertainty about the effect of setting district-party ideology to some new value $\theta'$ therefore contains multiple sources of uncertainty: statistical uncertainty about the estimated causal effect, and measurement uncertainty about the original value of $\theta$ before a causal intervention.
Bayesian analysis provides the statistical machinery to quantify uncertainty in these causal effects as if they were any other posterior quantity, by marginalizing the posterior distribution over any nuisance parameters.

This chapter unpacks these issues according to the following outline.
First, I review the notation and terminology for causal modeling in empirical research, where data and causal estimands are posed in terms of "potential outcomes" or "counterfactual" observations. 
I then describe a Bayesian reinterpretation of these models, which uses probability distributions to quantify uncertainty about causal effects and counterfactual data, conditional on observed data.
Because Bayesian modeling remains largely foreign to political science, I spend much of the chapter explaining what a Bayesian approach to causal inference means with theoretical and practical justifications: how priors are inescapable for many causal claims, how priors provide valuable structure to improve the estimation of causal effects, and practical advice for constructing and evaluating Bayesian causal models.
Finally, I provide examples of Bayesian causal modeling by replicating and extending published studies in political science, showing where priors add value to causal inference.



## Overview of Key Concepts


### Causal Models {#sec:causal-inf}

As an area of scientific development, _causal inference_ refers to the formal modeling of causal effects, the assumptions required to identify causal effects, and research designs that make these assumptions plausible.
Scientific disciplines, especially social sciences, have long been interested in substantiating causal claims using data, but the rigorous definition of the full causal model and identifying assumptions distinguishes the current causal inference movement from other informal approaches.

Contemporary approaches to causal inference span several fields, most notably in economics, epidemiology, and computer science.
The dominant modeling approach to causal inference in political science is rooted in a notation of _potential outcomes_ [@rubin:1974:potential-outcomes; @rubin:2005:potential-outcomes].
This "Rubin model" formalizes the concept of a causal effect by first defining a space of hypothetical outcomes.
The outcome variable $Y$ for unit $i$ is a function of a treatment variable $A$.
"Treatment" refers only to a causal factor of interest, regardless of whether the treatment is randomly assigned.^[
  Some causal inference literatures refer to treatments as "exposures," which may feel more broadly applicable to settings beyond experiments. For this project, I make no distinction between treatments and exposures.
]
Considering a binary treatment assignment where $A = 1$ represents treatment and $A = 0$ represents control, unit $i$'s outcome under treatment is represented as $Y_{i}(A = 1)$ or $Y_{i}(1)$, and the outcome under control would be $Y_{i}(A = 0)$ or $Y_{i}(0)$.
The benefit of expressing $Y$ in terms of hypothetical values of $A$ allows the causal model to describe, with formal exactitude, the entire space of possible outcomes that result from treatment assignment.
The model allows us to define a treatment effect for an individual unit, denoted $\tau_{i}$, as the difference in potential outcomes when changing the treatment $A_{i}$.
\begin{align}
  \tau_{i} &= Y_{i}(A_{i} = 1) - Y_{i}(A_{i} = 0)
  (\#eq:tau-i)
\end{align}
If $\tau_{i}$ is not $0$, then $A_{i}$ has a causal effect on $Y_{i}$.
Defining the causal model in terms of unit-level effects provides an exact, minimal definition of a causal effect: $A$ affects $Y$ if the treatment has a nonzero effect for any unit.
A causal model may describe more complex features of a causal system, such as whether a unit complies with their treatment assignment, whether the treatment effect are indirectly mediated by other variables, and so on.

The entire space of potential outcomes is a hypothetical device. 
Although a causal model defines potential outcomes for every unit under every treatment assignment, it is not possible to observe all of these potential outcomes, since a unit can receive only one treatment, and thus can only take a single outcome value. 
This implies that the individual causal effect ($\tau_{i}$), while a valid feature of the hypothetical causal model, is never actually observed for any unit.
This "fundamental problem of causal inference" is the core philosophical problem in causal inference; the researcher never observes a unit under more than one treatment status, so they can never make causal claims with observed data alone [@holland:1986:causal-inf].
Instead, causal claims are possible only by imposing assumptions on the data.
These "identification assumptions" specify the conditions under which observed can be used to describe what the data would look like if units received counterfactual treatments [@keele:2015:causal-inf].
For observational causal inference, it is common to invoke the following four assumptions. 
The _consistency_ assumption states that when a unit receives a treatment, the outcome that we observed is the potential outcome for that unit under that treatment.
\begin{align}
  \text{Consistency: } Y_{i} = Y_{i}(A_{i} = a) \mid A_{i} = 1
  (\#eq:consistency-assumption)
\end{align}
In other words, observing the outcome does not affect the outcome, and there are no hidden versions of treatment that are not defined in the causal model. 
The _no interference_ assumption states that unit $i$'s potential outcome depends only on $i$'s treatment status, not on any other units' treatment status.
\begin{align}
  \text{No Interference: } Y_{i}\left(A_{i}\right) = Y_{i}\left(A_{1}, A_{2}, \ldots, A_{n}\right)
  (\#eq:interference-assumption)
\end{align}
The consistency and no-interference assumptions are sometimes grouped into a single assumption, called the "stable unit treatment value assumption" [or SUTVA, @rubin:1980:comment-randomization].
The _conditional independence_ assumption (CIA), also known as conditional "ignorability" or conditional "unconfoundedness," states that potential outcomes are independent of treatment status conditional on a unit's membership in a specific stratum of covariates $X_{i} = x$.
\begin{align}
    \text{Ignorability: } 
    \p{A_{i} = a \mid Y_{i}\left(A_{i} = 1\right), Y_{i}\left(A_{i} = 0\right), X_{i} = x} &=
      \p{A_{i} = a \mid X_{i} = x}
   (\#eq:ignorability-assumption)
\end{align}
The CIA is essentially an assumption about unit-level expectations under counterfactuals.
Within stratum $X_{i} = x$, any observed $Y_{i}$ in the treatment group are independent from the $Y_{i}(1)$ potential outcome values for units in the control group.
This assumption is violated by self-selection processes or other confounding affects that sort units into treatment statuses in a way that is correlated with their potential outcomes.
The CIA is usually invoked alongside a _positivity_ assumption,
\begin{align}
   \text{Positivity: } 0 < p\left(A_{i} = a \mid X_{i} = x\right) < 1, \; \forall a \in \mathcal{A}, \; \forall x \in \mathcal{X}
   (\#eq:positivity-assumption)
\end{align}
where $\mathcal{A}$ and $\mathcal{X}$ are respectively the spaces of possible treatments and covariate strata.
Positivity stipulates nonzero probability of treatment statuses within each strata, ensuring that causal inferences are conducted only on units that could conceivably have received other treatment statuses.^[
  See @liao-et-al:2019:post-predict-time-varying for Bayesian causal estimation for "overlapping populations" under violations of strict positivity.
]
Conditional independence and positivity assumptions are essential for conducting causal inference using methods of covariate adjustment.

All together, these assumptions enable inferences about potential outcomes, which aren't always observed, using only observed data.
For instance, from these assumptions it follows that the average $Y$ values observed at $A = a$ are equal to the average potential outcomes that we would obtain by setting $A = a$ ourselves, within strata $X = x$:
\begin{align}
  \begin{split}
    \E{Y_{i} \mid A_{i} = a, X_{i} = x}
      &= \E{Y_{i}(A = a) \mid A_{i} = a, X_{i} = x} \\
      &= \E{Y_{i}(A = a) \mid X_{i} = x}
  \end{split}
  (\#eq:assumption-implications)
\end{align}
The first line of \@ref(eq:assumption-implications) equates observed outcomes given treatment to potential outcomes under an imposed treatment, which is enabled by SUTVA (consistency and no interference).
Conditional independence allows us to suppresses the explicit conditioning on $A_{i} = a$ in the second line, and positivity is implied by referring generically to treatment level $a$ and covariate stratum $x$.

Implications derived from identification assumptions are typically posed in terms of expectations about potential outcomes, $\E{Y_{i}\left(A_{i}\right)}$, instead of unit-level potential outcomes, $Y_{i}\left(A_{i}\right)$, because unit-level causal heterogeneity is unidentified without additional assumptions [@holland:1986:causal-inf]. 
This is why causal quantities of interest, also known as causal estimands, are usually posed in terms of expectations as well.
This project will generally discuss _conditional average treatment effects_ (CATEs), which are expectations of treatment effects averaged over the population of units within a covariate stratum.
Let $\bar{\tau}(a, a', x)$ be the average effect of setting $A = a$, as opposed to some other value $a'$, within stratum $X = x$.
\begin{align}
  \bar{\tau}(a, a', x) = \E{Y_{i}(a) - Y_{i}(a') \mid X_{i} = x}
  (\#eq:CATE)
\end{align}

When deriving estimands such as the CATE with identification assumptions, it is important to note that assumptions describe minimally sufficient conditions for _nonparametric_ causal identification [@keele:2015:causal-inf]. 
There is no guarantee that linear regression models, or any parametric models, adequately control for confounders.
For this reason, it is important to distinguish causal estimands from estimation methods, the latter of which can introduce statistical assumptions that differ from identification assumptions.

In order to separate the major planks of causal inference methods, I center the discussion of causal inference going forward around a three-part hierarchy of causal methodology.

1. Causal model: the hypothetical model that defines all treatments and potential outcomes at the unit level.
   The causal model is an omniscient view of the causal system, defining individual-level causal effects even though they cannot be observed in real data.
2. Identification assumptions: the assumptions required to identify causal estimands using only the observed data. 
   These assumptions specify how knowledge of observed $Y$ data can be used to make inferences about counterfactual $Y$ data that is not observed.
   Inferences enabled by identification assumptions are typically posed as expectations about counterfactual data, unless further assumptions are specified that relate individual unit effects to average effects.^[
     For instance, a _constant additive effects_ assumption states that all  causal effects are constant for all units and can be combined without interactions [@Rosenbaum:2002:observational-studies].
   ]
   Because inferences are posed as expectations, they are abstractions that follow from analytical derivations and do not depend on statistical estimation approaches.
3. Estimation methods: How do we estimate the expectations derived from the causal model and identification assumptions? Do these estimation methods introduce additional statistical assumptions on top of the identification assumptions?

I lay out this hierarchy for two main reasons.
First, it clarifies why researchers use certain research designs or statistical approaches to overcome particular problems with their data.
Statistical assumptions, we will see, can undermine identification assumptions, which is why causal inference scholars tend to promote estimation strategies that rely on as few additional assumptions as possible [@keele:2015:causal-inf].
One way to avoid these assumptions is to use research designs that eliminate confounding "by design" rather than through statistical adjustment, such as randomized experiments, instrumental variables, regression discontinuity, and difference-in-differences [for instance, @angrist-pischke:2008:mostly-harmless].
If researchers cannot design away these difficult assumptions, other methods are available to adjust for confounders without as many strict assumptions about the functional form of the causal model as are commonly invoked in parametric regression models.
Causal inference is not synonymous with the new "agnostic statistics" movement [e.g. @aronow-miller:2019:agnostic-statistics], but it is animated by a similar motivation to identify statistical methods that rely on as few fragile assumptions as possible.
For causal inference problems, these methods include matching, doubly robust models, and machine learning methods for estimating flexible conditional expectation functions that are to varying degrees robust to misspecification, nonlinearities, or non-additivities in the data generating process [@sekhon:2009:opiates; @aronow-samii:2016:regression; @hill:2011:bart; @green-kern:2012:bart; @samii-et-al:2016:retrospective-causal-inference-ML; @aronow-miller:2019:agnostic-statistics].
This dissertation will employ machine learning methods, in particular Bayesian neural networks (BNNs), to estimate regression functions that rely less on exact, reduced-form model specification choices.

Second, the three-part hierarchy of causal inference clarifies where my contributions around Bayesian causal estimation will be focused.
As I discuss below, the "easiest way in" for Bayesian methods is through statistical estimation (level 3), since some flexible estimation methods are convenient to implement using Bayesian technologies [@imbens-rubin:1997:bayes-compliance; @ornstein-duck-mayr:2020:GP-RDD].
I push this further by arguing that Bayesian estimation changes the interpretation of the causal model (level 1) by implying a probability distribution over the space of potential outcomes.
This probability distribution allows the researcher to say which causal effects and counterfactual data are more plausible than others, which is a desirable property of causal inference that is not available through conventional inference methods.
The Bayesian approach also has the power to extend the meaning of identification assumptions (level 2) by construing them also as probabilistic rather than fixed features of a causal analysis [@oganisian-roy:2020:bayes-estimation].





### Bayesian Inference {#sec:bayes-inf}

Bayesian reasoning is a contentious and misunderstood topic in empirical political science, so it is important to establish some essential tenets to the approach before melding it with causal modeling.
Bayesian analysis is the application of conditional probability for statistical inference.
Its mechanical underpinnings are uncontroversial, essential building blocks of probability theory: how the probability of an event changes by conditioning on other known information.
Any controversy surrounding Bayesian methods in political science is better understood as a disagreement over which modeling constructs we choose to describe using probabilities.

Whereas many statistical methods begin with a model of data given fixed parameters, Bayesian inference consists of a joint model for all components in a system.
The "joint model" is simply a probability model for more than one event.
For example, suppose that we are interested in the joint probability distribution of age and vote choice in a population.
The joint distribution of these two variables as $p(\mathit{Age}, \mathit{Vote})$, which can be equivalently expressed by factoring it in two ways:
\begin{align}
  p(\mathit{Vote} \mid \mathit{Age})p(\mathit{Age}) 
    &= p(\mathit{Age} \mid \mathit{Vote})p(\mathit{Vote})
  (\#eq:condition-prob-vote-age)
\end{align}
If we observe an individual's vote choice, how does this affect the probability distribution of age? 
Probability theory says that we divide the joint probability by the probability of the conditioning event.
\begin{align}
\begin{split}
  \frac{p(\mathit{Vote} \mid \mathit{Age})p(\mathit{Age})}{p(\mathit{Vote})}
   &= \frac{p(\mathit{Age} \mid \mathit{Vote})p(\mathit{Vote})}{p(\mathit{Vote})} \\
  \frac{p(\mathit{Vote} \mid \mathit{Age})}{p(\mathit{Age})}
  &= p(\mathit{Age} \mid \mathit{Vote})
\end{split}
(\#eq:updated-vote-age)
\end{align}
This maneuver reveals Bayes' theorem: the probability of $A$ given $B$, expressed in terms of $B$ given $A$.
Bayes' theorem provides a formal method for rationally updating a joint probability distribution by conditioning on known information.

The Bayesian paradigm of applied statistical modeling applies Bayes' theorem to data $\mathbf{y}$ and parameters $\boldsymbol{\pi}$.
The joint model for the data and the parameters takes the form
\begin{align}
  p(\mathbf{y}, \boldsymbol{\pi}) = p(\mathbf{y} \cap \boldsymbol{\pi}) = p(\mathbf{y} \mid \boldsymbol{\pi})p(\boldsymbol{\pi}),
  (\#eq:joint-model)
\end{align}
where $p(\mathbf{y} \mid \boldsymbol{\pi})$ represents the probability distribution of the data, conditioning on parameters, and $p(\boldsymbol{\pi})$ represents the probability distribution of parameters, marginalizing over the data.
The marginal parameter distribution is usually referred to as a "prior distribution," since it describes the researcher's prior information (or, controversially, prior "beliefs") about the parameter.
The joint model provides machinery for learning about parameters by conditioning the parameters on the data,
\begin{align}
  p(\boldsymbol{\pi} \mid \mathbf{y}) &= \frac{p(\mathbf{y} \mid \boldsymbol{\pi})p(\boldsymbol{\pi})}{p(\mathbf{y})}
  (\#eq:joint-model-post)
\end{align}
also known as obtaining the _posterior distribution_ of the parameters.

These basic concepts of conditional probability and Bayesian updating are not foreign to political science, but it will be important to establish an interpretation of Bayesian thinking and Bayesian modeling that is productive for causal inference. 
This project invokes what @mcelreath:2017:decolonized-bayes calls an "inside view" of Bayesian statistics---Bayesian statistics on its own terms.
The inside view is a response to an "outside view" of Bayesian statistics as penalized maximum likelihood.
Under the outside view, data follow probability distributions, while parameters are fixed.
Bayesian estimation, in turn, is a likelihood model with an additional penalty on the parameters, and because the penalty represents the researcher's subjective beliefs, the penalty feels _ad hoc_ and anti-scientific.
This view of Bayesian statistics is admittedly confusing, and if we take it at face value, it is no wonder that causal inference in political science has largely avoided Bayesian tools other than for computational convenience.

The inside view, as mentioned above, construes a Bayesian model as a joint model for all variables in a system.
The use of the word "variables" to encompass both data and parameters is crucial.
The technology of a Bayesian model does not regard data and parameters as distinct from one another.
They follow the same rules, just as age and vote choice followed the same rules in the above example.
Data and parameters are both instantiations of uncertain processes, with the only semantic difference between the two being that observed variables are called "data" while unobserved variables are called "parameters" [@mcelreath:2017:decolonized-bayes].^[
  The semantic conventions are often sloppier in practice than many researchers would like to think.
  Many analyses use data that summarize lower-level processes, such as per-capita income in U.S or the percentage of women who vote for the Democratic presidential candidate, which behave like random variables in that their values could differ under repeated sampling.
  The semantic distinction between data and parameters has a similar spirit to the @blackwell-et-al:2017:measurement-error view of measurement uncertainty, where "measurement error" falls on a spectrum between fully observed data and missing data.
]
Prior distributions and likelihood functions are the same thing: probability distributions that quantify uncertainty about a variable.
If I were to observe a new data point from a model, I would be unable to predict its value exactly, but some values are more probable than others, given the parameters that condition the data.
The same premise holds for parameters: if I could observe a parameter, I would have been unable to anticipate it exactly, but I could bet that some parameters are more likely than others, given the data that I have already seen.
The joint model for all variables encapsulates the probabilistic relationships between data and parameters.
Starting with the prior model, $p(\mathbf{y}, \boldsymbol{\pi})$, we can condition the model on chosen parameters to obtain a rationalizable distribution of data, $p(\mathbf{y} \mid \boldsymbol{\pi})$.
Or we can condition the model on data to obtain a rationalizable distribution of parameters, $p(\boldsymbol{\pi} \mid \mathbf{y})$. 
@mcelreath:2020:bayes-counting calls these maneuvers "running the model forward" (updating data given parameters) or "running the model backward" (updating parameters given data). 
<!-- It can be helpful to think of the model as a machine with many interlocking parts, where if you crank the machine, the pieces that you choose to constrain will affect which pieces do and do not move. -->


<!------- TO DO ---------
- plot a prior, predictive, and inferential distribution
------------------------->

From the inside view, Bayesian updating proceeds by considering a variety of possible scenarios that create data and evaluating which scenarios are consistent with the data.
The joint prior model, $p(\mathbf{y} \mid \boldsymbol{\pi})p(\boldsymbol{\pi})$ describes an overly broad set of possible configurations of the world. These configurations contain a distribution of possible parameters, $p(\boldsymbol{\pi})$, and possible data given parameters, $p(\mathbf{y} \mid \boldsymbol{\pi})$.
Bayesian updating decides which configurations of the world are consistent with the data and are therefore more plausible.
The plausibility, or posterior probability, of a parameter value is greater if the observed data are more likely to occur under that parameter value versus another value.
In turn, the posterior distribution downweights model configurations that are implausible, or inconsistent with the data [@mcelreath:2020:rethinking-2, chapter 2].
This is an important distinction from non-Bayesian statistical inference, since there can is no formal notion of "plausible parameters given the data" without a posterior distribution, which necessitates a prior distribution.
<!------- TO DO ---------
- is this true?
------------------------->
For causal inference, this means there can be no formal notion of "plausible causal effects" without a probability distribution over causal effects.
The mission in the remainder of this chapter is to establish a framework for causal inference in terms of plausible effects and plausible counterfactuals.

The inside view of Bayesian modeling, and the philosophical unity that it brings to statistical machinery and inference, is possible even if using uninformative prior distributions that are indifferent to possible parameters  _a priori_.
This is how Bayesian methods tend to appear in political science to date, with noninformative priors that exist primarily to facilitate Bayesian computation for difficult estimation problems.
The infamy of Bayesian methods, however, is owed to the ability of the researcher to specify "informative" priors that concentrate probability density on model configurations that are thought to be more plausible even before data are analyzed.
There are many modeling scenarios where this concentration of probability delivers results that are almost unthinkable without prior structure: multilevel models that allocate variance to different layers of hierarchy, highly parameterized models with correlated parameters such as spline regression, and sparse regressions where regularizing priors are used to shrink coefficients and preserve degrees of freedom to overcome the "curse of dimensionality" [@bishop:2006:pattern-rec; @gelman-et-al:2013:BDA].
At the same time, many researchers are skeptical of Bayesian methods because supplying a model with non-data information can be spun as data falsification [@garcia-perez:2019:bayes-data-falsification].
As I elaborate in Section \@ref(sec:causal-priors), it is a mistake to equate flat prior "flatness" with prior "uninformativeness," and there are many legitimate sources of prior information that have nothing to do with subjective beliefs.


<!------- TO DO ---------
- Furthermore, the mere non-flatness of a prior is often less consequential than the parameter's structural relationship to other parameters.
For instance, whether a prior has a scale of $10$ versus $30$ almost always matters less than whether the prior contains hierarchical structure for borrowing information from other groups of data.
It is no coincidence the Bayesian contributions in this dissertation and in other areas of applied statistics are concerned with different prior structures more than the precision of otherwise equal prior structures. 
------------------------->

<!------- TO DO ---------
- "random variable"
- no "histomancy" but rather a prior about the stochastic process that generates each data point
------------------------->






## Bayesian Causal Modeling {#sec:intro-bcm}

Having reviewed the basics of causal models and Bayesian inference, we now turn to a framework for Bayesian causal modeling.
The distinguishing feature of a Bayesian causal model is that the elemental units of the model, the potential outcomes, are given probability distributions.
This probability distribution reflects available causal information that exists outside the current dataset.
Bayesian inference proceeds updating our information about causal effects and counterfactual potential outcomes in light of the observed data.
The headings under \@ref(sec:intro-bcm) introduce this modeling framework at a high level.
I provide a probabilistic interpretation and notation for potential outcomes models, describe how a Bayesian modeling framework affects the "hierarchy of causal inference," and provide some broad justification for Bayesian causal modeling.



### Probabilistic Model for Potential Outcomes {#sec:po-distributions}

As with other causal models, we begin at the unit level.
Unit $i$ receives a treatment $A_{i} = a$, with potential outcomes $Y_{i}\left(A_{i} = a\right)$.
Suppose a binary treatment case where $A_{i}$ can take values $0$ or $1$, so the unit-level causal effect is $\tau_{i} - Y_{i}\left(1\right) - Y_{i}\left(0\right)$.
Although $\tau_{i}$ is unidentified, it is possible to estimate population-level causal quantities by invoking identification assumptions.
For instance, the conditional average treatment effect at $X_{i} = x$, $\bar{\tau}(X = x) = \E{Y_{i}(1) - Y_{i}(0) \mid X_{i} = x}$, can be estimated from observed data assuming consistency, non-interference, conditional ignorability, and positivity. 
Suppressing the unit index $i$,
\begin{align}
\begin{split}
  \bar{\tau}(X = x) 
      &= \E{Y(A = 1) - Y(A = 0)\mid X = x} \\
      &= \E{Y(A = 1) \mid X = x} - \E{Y(A = 0) \mid X = x} \\
      &= \E{Y \mid A = 1, X = x} - \E{Y \mid A = 0, X = x}
\end{split}
(\#eq:cate-proof)
\end{align}
where the third line is obtained by the identification assumptions.
The identification assumptions connect _causal estimands_ and what I will call _observable estimands_.
Causal estimands are the true causal quantities, but they are unobservable because they are stated as contrasts of potential outcomes.
Observable estimands are the observable analogs of causal estimands and are equivalent to causal estimands if identification assumptions hold.
Other literature refers to observable estimands as "nonparametric estimators" [@keele:2015:causal-inf], but I steer clear of this language because the gap between observable estimands and estimators is important for understanding the contributions of the Bayesian causal approach.

The transition to a Bayesian probabilistic model begins with an acknowledgment that no estimate of the observable estimand, $\E{Y \mid A = a, X = x}$, will be exact. 
The assumptions identify causal effects only in an infinite data regime, where the observable estimand is known exactly.
Inference about causal effects from finite samples, however, requires further statistical assumptions that link the observable estimand to an estimator or model.
<!------- TO DO ---------
- redo with an expectation-level model
------------------------->
Let $f(A_{i}, X_{i}, \boldsymbol{\pi}) + \epsilon_{i}$ be a model for $Y_{i}$ consisting of a function $f(\cdot)$ treatment $A_{i}$, covariates $X_{i}$, and parameters $\boldsymbol{\pi}$, and an error term $\epsilon_{i}$.
We let the systematic component $f(\cdot)$ be a plug-in estimator for $\E{Y \mid A = a, X = x}$.
This setup is similar to any modeling assumption that appears in observational causal inference to link an estimator to the observable estimand, including parametric models for covariate adjustment, propensity models, matching, and more [@acharya-blackwell-sen:2016:direct-effects; @sekhon:2009:opiates].^[
  Although researchers are focusing more attention on estimation methods that focus on these statistical assumptions themselves, either by model ensembles/averages or "robust" models for propensity and response.
]
<!------- TO DO ---------
- cites for parametric adjustment, propensity, matching
- robust
- ensembles in causal inf
------------------------->
We use the statistical model to generate a CATE estimate, $\hat{\bar{\tau}}(X = x)$, by differencing these model predictions over the treatment.
\begin{align}
\begin{split}
  \bar{\tau}(X = x) 
    &= \E{Y \mid A = 1, X = x} - \E{Y \mid A = 0, X = x} \\
  \hat{\bar{\tau}}(X = x)
    &= \E{f(A_{i} = 1, X_{i}1 = x, \boldsymbol{\pi})
       - f(A_{i} = 0, X_{i} = x, \boldsymbol{\pi})}
\end{split}
(\#eq:cate-f)
\end{align}
Where the second line includes $\hat{\bar{\tau}}$ to indicate that $f(\cdot)$ is an estimator of $\bar{\tau}$.

The Bayesian approach, inspired largely by @rubin:1978:bayesian, confronts the problem with a joint model for data and parameters: $\p{Y, \boldsymbol{\pi}} = \p{Y \mid f(A, U, \boldsymbol{\pi})}\p{\boldsymbol{\pi}}$.
The data are distributed conditional on the statistical model prediction $f(\cdot)$, which conditions on the model parameters $\boldsymbol{\pi}$.
The parameters also have a prior distribution $\p{\boldsymbol{\pi}}$, or a distribution marginal of the data.
These models for data and parameters are added statistical assumptions on top of causal identification assumptions.
The data model is similar to any estimation approach that uses a probability model for errors (e.g. any MLE method or OLS with Normal errors). 
The prior model has no analog in OLS or unpenalized MLE, but this added statistical assumption will be leveraged as a major benefit as we explore Bayesian causal estimation below.

The joint generative model is sufficient to characterize the probability distribution for the conditional average treatment effect as defined in Equation \@ref(eq:cate-f),
\begin{align}
  p(\bar{\tau}(X = x))
    &= \int p\left[
         f(A = 1, X = x, \boldsymbol{\pi}) 
         - f(A = 0, X = x, \boldsymbol{\pi})
         \mid \boldsymbol{\pi}
       \right] 
       \p{\boldsymbol{\pi}}
       \diff{\boldsymbol{\pi}}
  (\#eq:prior-cate)
\end{align}
<!------- TO DO ---------
- should the p(pi) be there?
------------------------->
which is the probability distribution of model contrasts for $A = 1$ versus $A = 0$.
This distribution of model contrasts contains two sources of uncertainty: uncertainty about the data given parameters, and uncertainty over the parameters themselves.
Integrating over $\boldsymbol{\pi}$ in Equation \@ref(eq:prior-cate) marginalizes the distribution with respect to the uncertain parameters. 
Because the integrated parameters are the prior distribution $\p{\boldsymbol{\pi}}$, the expression in \@ref(eq:prior-cate) represents a prior distribution for the CATE.
This is an inherent feature of the Bayesian approach: probability distributions of causal quantities even before data are observed.

Conditioning on the observed data returns the posterior distribution for the CATE...
\begin{align}
  p(\bar{\tau}(X = x) \mid Y)
    &= \int p\left[
         f(A = 1, X = x, \boldsymbol{\pi}) 
         - f(A = 0, X = x, \boldsymbol{\pi})
         \mid \boldsymbol{\pi}, Y
       \right] 
       \p{\boldsymbol{\pi} \mid Y}
       \diff{\boldsymbol{\pi}}
  (\#eq:post-cate)
\end{align}
where we would integrating over the posterior distribution of the parameters, instead of the prior distribution, returns a probability distribution for the CATE $\bar{\tau}$ that reflects Bayesian updating from data $Y$.




### Why Bayesian Causal Modeling {#sec:why-bcm}

The Bayesian causal approach is sensible for causal inference because it facilitates _direct inference_ about treatment effects given the data: which effect sizes are more likely or less likely than others.
While confidence intervals are often misused to make probabilistic statements about parameters, the posterior distribution and posterior intervals actually enable the researcher to state the probability of positive treatment effects, negligible treatment effects, and more.
Making positivistic statements about model conclusions is a natural way to think about the scientific aims of any discipline engaged causal inference: the world probably works in this or that way, given this evidence.
This is the view espoused by Don Rubin himself, the namesake of the Rubin causal model commonly employed in causal political science, who writes in the context of causal inference that "a posterior distribution with clearly stated prior distributions is the most natural way to summarize evidence for a scientific question" [@rubin:2005:potential-outcomes, p. 327].
It isn't statistically coherent to invoke similar language under a non-Bayesian inference paradigm, since we can't directly characterize the plausibility of a causal effect without a posterior belief, which entails a prior belief.
Rather than conducting inferences about the plausibility of unknown parameters, non-Bayesian methods typically conduct inference about the plausibility of data given fixed parameters. 
Namely, a $p$-value is a measure of the plausibility of data under a null hypothesis that the researcher usually does not find credible to begin with, and usually with no corresponding measure of the relative plausibility of an alternative hypothesis [@gill:1999:NHST].
Restated formally, conventional inference methods estimate $\p{\mathbf{y} \mid H_{0}}$, when scientists are actually interested in $\p{\boldsymbol{\pi} \mid \mathbf{y}}$.

The notion of direct inference is especially valuable when the observed data represent the whole population, which is common for causal inference in political science.
Under a "frequentist" inference approach, estimators inherit their statistical properties from the sampling distribution of the estimator: the theoretical probability distribution of estimates obtained by computing the estimator in each of an infinite number of independent samples from the population.
Many causal questions in political science conflict with this inferential framework, however, because the data come from events that have no possibility of future sampling.
Instead, the data come from one-off events in history and often contain the entire population of relevant units, so uncertainty about statistical inferences bears no resemblance to the randomness of a sampling mechanism [@western-jackman:1994:comparative-bayes].
There is non-Bayesian work on methods for incorporating uncertainty without sampling, such as "randomization inference" where the source of uncertainty is treatment assignment itself [@keele-et-al:2012:RI; @abadie-et-al:2020:design-uncertainty]
The foundations of uncertainty in Bayesian inference are parametric families that represent imperfect information about the generative processes underlying model variables.
Whether this imperfect information reflects sampling randomness or other epistemic uncertainty, such as randomization, can be subsumed in the Bayesian framework [@rubin:1978:bayesian].

Another reason why Bayesian methods make sense for causal modeling is because causal models, at their core, are models for counterfactual data. 
Because the Bayesian model is a _generative_ model for parameters and data, the model contains all machinery required to directly quantify counterfactual potential outcomes using probability distributions.
To see this in action, we can "run the model forward" to create a predictive distribution for $Y$ given the model parameters.
Denote these simulated observations as $\tilde{Y}$ to distinguish them from the data observed $Y$.
If we marginalize this predictive distribution with respect to the prior parameters, we obtain a "prior predictive distribution"---the distribution of data we would expect under the prior [@gelman-et-al:2013:BDA].
\begin{align}
  \p{\tilde{Y} \mid A = a, X = x} &= \int \p{\tilde{Y} \mid A = a, X = x, \boldsymbol{\pi}}\p{\boldsymbol{\pi}} \diff{\boldsymbol{\pi}}
  (\#eq:prior-predictive)
\end{align}
We update this distribution by conditioning on observed data, delivering a "posterior predictive distribution"—the distribution of data that we expect from the posterior parameters.
\begin{align}
  \p{\tilde{Y} \mid Y, A = a, X = x} &= \int \p{\tilde{Y} \mid A = a, X = x, \boldsymbol{\pi}}\p{\boldsymbol{\pi} \mid Y} \diff{\boldsymbol{\pi}}
  (\#eq:post-predictive)
\end{align}
These predictive distributions are the basis for out-of-sample inference in Bayesian generative models,^[
  Simulations of this sort are possible under any likelihood-based model that posits a generative probability distribution for the data, but Bayesian predictive distributions marginalize over the parameter distribution instead of conditioning on fixed parameters.
  This makes Bayesian predictive distributions a more complete accounting of  statistical and epistemic sources of uncertainty.
]
and they are the basis for counterfactual inference as well.
Invoking the causal identification assumptions, we generate counterfactual data as predictive distributions as well, setting the treatment $A = a$ to some other value $A = a'$. 
Denote these counterfactual predictions $\tilde{Y}'$, which I will subscript $i$ to show that this model implies a probability distribution for individual data points as well as aggregate treatment effects.
\begin{align}
  \p{\tilde{Y}_{i}' \mid Y, A_{i} = a', X_{i} = x} &= \int \p{\tilde{Y}_{i}' \mid A_{i} = a', X_{i} = x, \boldsymbol{\pi}}\p{\boldsymbol{\pi} \mid Y} \diff{\boldsymbol{\pi}}
  (\#eq:counterfactual-predictive)
\end{align}
Stated more simply: if causal models define a space of potential outcomes, then Bayesian causal models are probabilistic representations of the potential outcome space.
Probability densities over potential outcomes are defined in the prior and in the posterior, and they can be defined all the way to the unit level if the generative model contains a probability statement for unit data.^[
  Some modeling approaches can estimate average causal effects with group-level statistics only, eliding the unit-level model altogether.
  This can weaken the model's dependence on parametric assumptions for units, falling back onto more dependable parametric assumptions for the statistics, e.g. the Central Limit Theorem for group means.
  A model of this type will naturally stop short of defining probability distributions for counterfactual units, but it does define probability distributions for counterfactual means.
  In some cases, such as binary outcome data, means in each group are sufficient statistics for the raw data, so the unit level model is implied by the group-level model.
  See Section \@ref(sec:models-for-means) for explanation and examples.
]
The Bayesian view of causal inference, where the statistical model is nothing more remarkable than a missing data model for unobserved counterfactuals, is at least as old as @rubin:1978:bayesian.^[
  In more general modeling contexts beyond causal inference,@jackman:2000:bayes-missing-data makes a similar argument that all estimates, inferences, and goodness-of-fit statistics can be unified as functions of missing data, with Bayesian posterior sampling as a natural way to describe our information about these functions.
  This has a similar spirit as @mcelreath:2017:decolonized-bayes's "inside view" of Bayesian stats, where all data and parameters are unified as functions of probabilistic variables in a joint generative model.
]
Bayesian methods for causal inference have appeared in political science only sporadically in the decades since [@horiuchi-et-al:2007:experimental-design; @green-et-al:2016:lawn-signs; @ornstein-duck-mayr:2020:GP-RDD].
Unlike these rare examples of Bayesian causal inference, however, this chapter will contain more practical guidance for thinking Bayesianly about causal modeling, more synthesis of Bayesian causal innovations from other fields, and more examples of Bayesian causal modeling in practice.

It is common for advocates of Bayesian inference to celebrate the fact that the posterior distribution quantifies uncertainty in all parameters simultaneously, but this is especially useful for causal methods that entail multiple estimation steps.
These multi-stage procedures include instrumental variables, propensity score weighting, synthetic control, causal mediation approaches, and structural nested mean models [@angrist-pischke:2008:mostly-harmless; @imai-et-al:2011:black-mox-mediation; @acharya-blackwell-sen:2016:direct-effects; @xu:2017:synthetic-control; @blackwell-glynn:2018:causal-TSCS].
Bayesian approach to these methods combines all estimation stages into one model, estimating the ultimate treatment effect by marginalizing the posterior distribution with respect to the "design stage" parameters [@mccandless-et-al:2009:bayesian-pscore; @liao:2019:bayesian-causal-inference; @zigler-dominici:2014:propensity-uncertainty].
The combined modeling approach diverges from methods that use only point estimators, which ignore design stage uncertainty by conditioning the final-stage effects on design stage estimates or must derive post-hoc variance estimators to account for design stage uncertainty.
It is reminiscent of "uncertainty propagation" methods insofar as uncertainty from design estimates are pushed forward to later estimates—for example, the way @kastellec-et-al:2015:electoral-scotus-connection simulate measurement uncertainty in MRP estimates of public opinion in later analyses.
Unlike uncertainty propagation, which is mechanically similar to a "posterior cut" [@plummer:2015:bayesian-cuts], the complete Bayesian model effectively treats the design stage estimates as priors and updates all model parameters using information from all stages of the model [@liao-zigler:2020:two-stage-bayes; @zigler:2016:bayes-propensity; @zigler-et-al:2013:feedback-propensity].

The combined modeling approach is important for this project because the key independent variable, district-party public ideology, is estimated from a Bayesian measurement model. 
In order to understand district-party ideologies effects in primary elections, it is essential to pass posterior uncertainty from this measurement model into the causal analyses.
However, building a combined model for district-party ideology and all downstream effects would be logistically overwhelming, so I approximate the full model by drawing ideal points in causal analyses from a multivariate prior distribution that reflects the measurement model's posterior samples. 
For instance, to understand the causal effect of district-party ideal point $\bar{\theta}_{g}$ on some outcome measure $y_{g}$, our causal model contains a model for the outcome,
\begin{align}
  y_{g} &\sim \mathcal{D}\left(\theta_{g}, \ldots \right),
  (\#eq:unit-outcome)
\end{align}
where $\mathcal{D}\left(\cdot\right)$ is some distribution, and we place a multivariate Normal prior on the vector of all district-party ideal points $\bar{\boldsymbol{\theta}}$ as
\begin{align}
  \boldsymbol{\bar{\theta}} &\sim \mathrm{MultiNormal}\left(\boldsymbol{\bar{\bar{\theta}}}, \bar{\Sigma}\right)
  (\#eq:theta-prior)
\end{align}
where $\bar{\bar{\boldsymbol{\theta}}}$ and $\bar{\Sigma}$ are the estimated mean vector and variance–covariance matrix of the ideal point samples from the measurement model.
The multivariate Normal prior is justified on the ground that the original ideal point model smoothed ideal point estimates using a hierarchical Normal prior, and estimating the variance–covariance matrix from the samples is a simple method to summarize systematic relationships between ideal points as a function of their geographic attributes.
This approximation avoids the difficulties of writing an overly complicated model while still giving ideal points a prior that resembles their posterior distribution from the IRT model.

One final justification for Bayesian causal modeling is that prior information is everywhere.
This is a longer discussion that I untangle in Section \@ref(sec:causal-priors), but to preview, priors matters for the way researchers think about their modeling decisions, and they affect the inferences that researchers draw from data, even if they wish to avoid explicit Bayesian thinking about their analyses.




### Bayesian Inference in the Hierarchy of Causal Inference {#sec:bayes-hierarchy}


This section interprets the Bayesian causal inference framework in light of the "hierarchy of causal inference" described in Section \@ref(sec:causal-inf).
The hierarchy helps us account for the ways that Bayesian methods have already been invoked for causal inference in political science and in other fields, and it helps us understand how the Bayesian statistical paradigm reinterprets causal inference more broadly.
To review, the hierarchy consisted of three parts:

1. The causal model: definition of potential outcome space, causal estimands expressed in terms of potential outcomes.
2. Identification assumptions: linkage from causal estimands expressed as potential outcomes to observable estimands expressed using observed data.
3. Estimation: Methods for estimating observable estimands with finite data.

We began our discussion of the Bayesian causal model above by considering a plug-in estimator for an observable estimand that came from a Bayesian statistical model.
Bayes was invoked as "mere estimation," so we began our understanding of Bayesian causal modeling at level 3 of the hierarchy. 
As only an estimation method, a Bayesian estimator (such as a posterior expectation value) doesn't obviously change the meaning of the observable estimand or the causal estimand. 
After all, the estimator exists in the space of real data, unlike a causal estimand that belongs to the hypothetical space of potential outcomes.
Being merely an estimator, we could evaluate the Bayesian model for its bias and variance like any other estimator.

The realm of "mere estimation" is where many Bayesian causal approaches appear in political science and other fields.
The estimation benefits of Bayes tend to fall into three categories: priors provide practical stabilization or regularization, posterior distributions are convenient quantifications of uncertainty, or MCMC provides a tractable way to fit a complex model.
We could characterize the use of Bayesian methods for these purposes as practically valuable but theoretically dispensable, in the sense that researchers might prefer non-Bayesian means to the same ends. 
For instance, @green-kern:2012:bart adapt Bayesian Additive Regression Trees [or BART, @chipman-et-al:2010:BART] to measure treatment effect heterogeneity in randomized experiments. [See @hill:2011:bart for a non-political science introduction to causal inference with BART.]
The advantages of a regression tree model for treatment heterogeneity is that it can explore arbitrary interactions among covariates while controlling overfitting, but the fact that BART is Bayesian is an afterthought.
We observe a similar pattern in the use of Gaussian process models for fitting the running variable in regression discontinuity designs by @ornstein-duck-mayr:2020:GP-RDD and in the development of augmented LASSO estimators for sparse regression models by Ratkovic and Tingley [-@ratkovic-tingley:2017-direct-estimation; -@ratkovic-tingley:2017:sparse-lasso-plus].^[
  See @tibshirani:1996:lasso for a general introduction to the LASSO shrinkage estimator using the L1 penalized optimization.
  @park-casella:2008:bayesian-lasso implement a Bayesian LASSO using Laplace priors for regression coefficients.
]
These authors use priors to regularize richly parameterized functions, posterior distributions to characterize uncertainty, and MCMC to estimate models, but the theoretical implications of Bayesian causal estimation are not a major focus.

What does it mean for Bayesian estimation to have theoretical implications for causal inference? 
This brings our focus to level one of the causal inference hierarchy: the model of potential outcomes
Any estimation method that invokes Bayesian tools requires priors for model parameters. 
Because causal estimates are functions of model parameters, prior densities on model parameters imply that some causal effects can be more or less likely even before considering any data.
If the statistical model contains a unit-level data model, which is the case for most regression approaches, this implies that unit-level potential outcomes also have prior probability densities: some potential outcomes at the unit level are more or less likely, marginal of any observed data.
This is a decisive philosophical departure from a non-Bayesian approach to causal modeling, where potential outcomes and causal effects are simply defined in a space of outcomes.
<!------- TO DO ---------
- what's the correct term for this space?
------------------------->
As with causal effects, the benefit of prior distributions for unit outcomes is that that we can obtain, describe, and conduct direct inference using the posterior distribution for unit counterfactuals.
The drawback is that researchers must specify priors for model parameters and understand how they impact the implied priors for unit counterfactuals.
A few recent methodology papers in political science have invoked this idea of unit counterfactual estimation in a Bayesian framework, which is especially intuitive for synthetic control estimation [@carlson:2020:GP-synth; see @ratkovic-tingley:2017-direct-estimation for a regression application].
But these papers do not highlight the notion of direct priors for counterfactuals, so skeptical applied researchers have little guidance for understanding what it means to have priors on counterfactual data, either theoretically or practically.

The theoretical notion of prior densities in the potential outcome space is only interesting if it makes sense to see priors as important for causal estimation.
I have already argued that priors enable direct probabilistic inference on causal effects and unit counterfactuals, which is the essential modeling goal for causal inference in the first place.
It is natural to ask if it is sensible to use flat or uninformative priors achieve to enable direct probabilistic inference while minimizing the uncomfortable feeling of specifying priors for unobservable counterfactuals, avoiding the need to think hard about priors at all.
As I discuss in future sections, however, flat priors obscure rather than avoid these complexities.
This is because even simple modeling scenarios that begin with flat priors usually contain quantities of interest whose priors cannot also be flat.
As a result, it is the researcher's job to decide how to parameterize a problem and how the chosen priors affect the important quantities in the analysis.

Priors on important modeling constructs do not have to be an inconvenience.
There are many scenarios where priors can actually relax assumptions, which building robustness checks directly into a statistical model.
This is how Bayesian inference affects layer two of the hierarchy: identification assumptions.
By their nature, identification assumptions can never be validated by consulting the data, so most causal inference research projects simply condition the analysis on the identification assumption holding.
If we invoke a measurement model of treatment effects akin to @gerber-et-al:2004:obs-learning,
\begin{align}
\text{Estimated Effect} &= \text{True Effect} + \text{Bias} + \text{Error},
(\#eq:ggk-model)
\end{align}
such that the estimated effect equals the true effect only if it is estimated with zero random error and zero systematic bias.
Identification assumptions imply a prior that the bias is precisely zero, but in many applied research contexts, this assumption may be unreasonable to sustain with 100% certainty.
A Bayesian approach to identification assumptions allows the researcher to relax their model of treatment effects by specifying a different prior on the bias term, or the sensitivity parameters that compose the bias term, that is consistent with the researcher's reasonable expectations for the remaining bias in the research design [@oganisian-roy:2020:bayes-estimation].
This bears resemblance to sensitivity testing approaches in which the researcher evaluates the treatment effect estimates by stipulating a range of fixed values for a key sensitivity parameter, and then evaluating the treatment effect estimate conditional on each fixed value [@imai-et-al:2011:black-mox-mediation; @acharya-blackwell-sen:2016:direct-effects].
Constructing these identification assumptions as priors lets the researcher conduct inference about treatment effects by _marginalizing over_ their priors for design parameters, rather than conditioning on fixed design parameter values with little thought to which values are plausible or implausible.
One recent political science example of this approach is @leavitt:2020:bayes-did, who frames the parallel trends assumption in a difference-in-differences (DiD) analysis as a prior over unobserved trends.
This introduces an additional layer of "epistemic uncertainty" into the DiD analysis that would ordinarily be assumed to be zero by design.
I elaborate on the value of priors for unidentifiable quantities in Section \@ref(sec:identifiability-priors).

A few papers in political science and related fields invoke Bayesian causal models that occupy more than one level of this causal inference hierarchy at once.
One important figure to note is Rubin himself, who has advocated for a "phenomenological Bayes" approach to causal inference since his pioneering papers on causal inference [@rubin:1978:bayesian; @rubin:1978:phenomenological-bayes; @imbens-rubin:1997:bayes-compliance; @rubin:2005:potential-outcomes].
The fundamental tenet of the so-called "phenomenological" approach is the use of parametric models to generate posterior distributions for unobserved potential outcomes, which are used to construct causal estimates as functions of these unit-level posterior predictions.
Posterior distributions for units are essential to the approach because statistical models may differ in their parameterization, making model parameters difficult to compare, but counterfactual data from the posterior distribution can always be compared to observed data regardless of the model parameterization.
This is especially useful for studies with more complex missing data structures, such as studies with noncompliance, because Bayesian methods can always return a posterior distribution even for quantities that are not point-identified from data [@imbens-rubin:1997:bayes-compliance].
As such, the Rubin example invokes a Bayesian framework for estimation (level three) as well as a philosophical posture that the posterior distribution directly characterizes the plausibility of counterfactual data (level one).
A recent political science example invoking a Bayesian approach for missing data under treatment noncompliance is @horiuchi-et-al:2007:experimental-design.

Another important frontier for Bayesian methods in causal inference is meta-analysis.
A fundamental modeling dilemma for meta-analysis is the choice between "fixed effects" and "random effects" approaches to meta-analysis [@borenstein-et-al:2011:meta-analysis].
Fixed effects meta-analysis assumes that all studies contain imperfect estimates of the same underlying treatment effect, while random effects admit the possibility of between-study heterogeneity in the true underlying effect.
Bayesian approaches to meta-analysis can compromise between the two extremes, relaxing the fixed effects assumption by allowing between-study variation while incorporating prior information that rules out extreme heterogeneity more aggressively than the conventional random effects approach.
A common example from Bayesian pedagogy is a meta-analysis of parallel experiments across schools by @rubin:1981:eight-schools. 
The approach simultaneously estimates study-specific treatment effects, cross-study variance in treatment effects, and a population average treatment effect.
@meager:2019:microcredit creates a similar model to summarize the experimental evidence on micro-credit expansion in developmental economics.
Like the noncompliance approaches above, these meta-analyses engage in levels one and three by constructing the problem explicitly as Bayesian learning of experimental effects and engaging assertively with Bayesian modeling assumptions using hierarchical priors in the meta-analytical model.
In political science, @green-et-al:2016:lawn-signs perform a fixed effects analysis to aggregate noisy treatment effects, invoking a notion of Bayesian learning from parallel experiments but stopping short of a richer statistical model for cross-study variation.
As I show in Section \@ref(sec:models-for-means), the researcher's assumptions about cross-study variation is highly consequential for meta-analytic inference, and a Bayesian framework provides inimitable benefits for theorizing about those assumptions and understanding how they affect inferences.

For the remainder of this chapter, I explore several ways Bayesian modeling changes our view of causal inference at all three levels of hierarchy: causal modeling, identification assumptions, and statistical estimation.
Many points discussed below touch on more than one level of the hierarchy, so the chapter cannot be neatly divided according to the hierarchy.






<!-- bayesian viewpoints -->
<!-- @rubin:1978:bayesian -->
<!-- @rubin:2005:potential-outcomes -->
<!-- @baldi-shahbaba:2019:bayesian-causality -->

<!-- meta-analysis -->
<!-- @rubin:1981:eight-schools -->
<!-- @meager:2019:microcredit -->
<!-- @green-et-al:2016:lawn-signs -->

<!-- compliance -->
<!-- @imbens-rubin:1997:bayes-compliance -->
<!-- @horiuchi-et-al:2007:experimental-design -->

<!-- BART for heterogeneity -->
<!-- @green-kern:2012:bart -->
<!-- @guess-coppock:2018:backlash -->

<!-- direct counterfactuals w/ incidental Bayes -->
<!-- @ratkovic-tingley:2017-direct-estimation -->
<!-- @carlson:2020:GP-synth -->

<!-- rdd -->
<!-- @ornstein-duck-mayr:2020:GP-RDD -->
<!-- @branson-at-al:2019:bayes-rdd -->
<!-- @chib-jacobi:2016:bayes-rdd -->

<!-- ? -->
<!-- @lattimore-rohde:2019:do-calc-bayes-rule -->








## Understanding Priors in Causal Inference {#sec:causal-priors}


This is because a prior's flatness is only relative to the parameterization of a model, and functions of parameters are not likely to have flat densities even if the underlying parameters have flat densities


### Priors as Data Falsification

<!------- TO DO ---------
- figure this out
DATA FALSIFICATION digression
------------------------->

Data falsification versus unavoidable choice:
imagine a study with a posterior distribution $p(\mu \mid \mathbf{y})$ that is proportional to the likelihood times the prior. 
  \begin{align}
    (\#eq:propto)
    p(\mu \mid \mathbf{y}) &\propto p(\mathbf{y} \mid \mu)p(\mu)
  \end{align}
Rewrite the right side in product notation for $n$ observations of $y_{i}$ for units indexed by $i$, letting $l(y_{i}) = p(y_{i} \mid \mu)$
  \begin{align}
    p(\mu \mid \mathbf{y}) &\propto \prod\limits_{i = 1}^{n}l(y_{i})p(\mu)
    (\#eq:propto-index)
  \end{align}
Suppose that we express this proportionality on the log scale, where the log posterior is proportional to the log likelihood plus the log posterior.
  \begin{align}
    \begin{split}
      \log p(\mu \mid \mathbf{y}) 
        &\propto \sum\limits_{i = 1}^{n} \log l(y_{i}) 
          + \sum\limits_{i = 1}^{n} \log p(\mu) \\[12pt]
        &\propto \log l(y_{1}) + \log p(\mu)
                 + \log l(y_{2}) + \log p(\mu)
                 + \ldots
                 + \log l(y_{n}) + \log p(\mu)
    \end{split}
  (\#eq:log-propto-index)
  \end{align}


The setup in \@ref(eq:log-propto-index) highlights a few appealing intuitions.
First, it shows how each observation "adds information" to the log posterior distribution.
Data that are more likely to be observed given the parameter (larger $l(y_{i})$ values) increase the posterior probability that parameter. 
We also see that the prior probability "adds information" to the posterior in the same way data add information, captured by the addition of each $p(\mu)$ term. 
Parameters that are more probable in the prior are more probable in the posterior.

The proportionality \@ref(eq:log-propto-index) also reveals how the posterior "learns" from flat priors.
A flat priors implies that prior probability $p(\mu)$ is constant for all potential values of $\mu$.
Because \@ref(eq:log-propto-index) is a proportionality, this lets us disregard $p(\mu)$ entirely by factoring it out of the proportionality, leaving us with an expression that the log posterior is proportional to the likelihood of the data only if the prior is flat.
\begin{align}
  \log p(\mu \mid \mathbf{y}) 
        &\propto
        \log l(y_{1}) + \log l(y_{2}) + \ldots + \log l(y_{n})
  (\#eq:log-propto-flat)
\end{align}
If $p(\mu)$ is not flat, however, and $p(\mu)$ varies across values of $\mu$, we can no longer ignore $p(\mu)$ in \@ref(eq:log-propto-index) shows that $p(\mu)$ varies across values $\mu$.
Not only does this prevent us from dropping $p(\mu)$ from the proportionality, but it also reveals how the prior "adds information" to the posterior by the same mechanism that observations do: adding to the log posterior distribution. 
This general expression where both data and priors contribute to the posterior distribution has led some researchers to argue the Bayesian inference with non-flat priors is analytically indistinguishable from data falsification [@garcia-perez:2019:bayes-data-falsification].
We can highlight this behavior by obscuring each $p(\mu)$ term with a square $\square$.
\begin{align}
  \log p(\mu \mid \mathbf{y}) 
        &\propto
        \log l(y_{1}) + \square + \log l(y_{2}) + \square \ldots + \log l(y_{n}) + \square
  (\#eq:log-propto-falsification)
\end{align}
Behind each square is _some contribution_ to the log posterior.
The fact that it adds information to the log posterior is unaffected by whether the hidden term is the probability of an additional observation $l(y_{i})$ or the prior probability of a parameter value $p(\mu)$.
<!------- TO DO ---------
- END FALSIFICATION
------------------------->


<!------- TO DO ---------
- but where was I going with this?
------------------------->





### Flatness is a Relative, Not an Absolute, Property of Priors {#sec:prior-flatness}

Formally, a flat prior is a probability distribution that assigns equal probability density to all possible values of a parameter.
It is common for researchers to prefer flatter priors in situations where they lack specific prior information about model parameters.
In cases where a prior distribution is perfectly flat, the posterior mode will be equivalent to the maximum likelihood estimator regardless of the sample size.
For this reason, many researchers who find themselves exploring Bayesian methods use flat or nearly-flat priors as a "default choice".
It is likely that a researcher exploring Bayesian modeling for causal inference might make a similar choice, in the interest of "letting the data speak."
A common criticism of this practice is that flat priors understate the researcher's actual prior information.
Although this sounds obviously true, it is easy to see why an applied researcher would be unbothered by it—what's the harm with a vague prior?
Instead, I argue that the default use of flat priors can often _misspecify_ prior information in many common modeling scenarios.
Researchers must set prior distributions on parameters, but they usually have prior information about outcome data, which are functions of parameters.
If researchers are not mindful of the functional relationship between raw parameters and other quantities of interest in the analysis, they will not the pernicious effects that "default priors" can have on model behavior. 


```{r normal-factoring}
normal_mu <- 3
normal_sd <- 2


implied_normal <- 
  tibble(
    x = seq(-10, 10, .01),
    pi = dnorm(x),
    h = dnorm(x, mean = normal_mu, sd = normal_sd)
  ) %>%
  pivot_longer(cols = -x, values_to = "density", names_to = "param") %>%
  print()
```




```{r plot-normal-factoring}
ggplot(implied_normal) +
  aes(x = x, color = param, fill = param) +
  geom_ribbon(
    aes(ymin = 0, ymax = density),
    alpha = 0.7,
    outline.type = "upper"
  ) +
  labs(
    x = NULL, 
    y = NULL,
    title = "Priors and Implied Priors",
    subtitle = "Functions of parameters have implied prior density"
  ) +
  annotate(
    geom = "text",
    x = 0 + normal_mu + normal_sd,
    y = 0.9 * (filter(implied_normal, param == "h") %$% max(density)),
    label = TeX("Transformed parameter: $h(\\pi)$"),
    hjust = 0
  ) +
  annotate(
    geom = "text",
    x = 0 + 1,
    y = 0.9 * (filter(implied_normal, param == "pi") %$% max(density)),
    label = TeX("Original parameter: $\\pi$"),
    hjust = 0
  ) +
  coord_cartesian(xlim = c(-3, 10)) +
  scale_fill_manual(values = c(primary, tertiary)) +
  scale_color_manual(values = c(primary, tertiary)) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = seq(-10, 10, 2)) +
  theme(legend.position = "none")
```

To understand the consequences of prior choices, it is essential to understand the _implied prior_. 
Suppose we have a parameter $\pi$ and a function of that parameter, $h(\pi)$. 
If $\pi$ is a prior density, then $h(\pi)$ has an implied prior density that is affected by the density of $\pi$ and the composition of the function $h(\cdot)$.
Consider a simple example where $\pi$ is distributed $\mathrm{Normal}\left( 0, 1 \right)$, and $h(\pi) = `r normal_mu` + `r normal_sd`\pi$. 
The implied prior for $h(\pi)$ is $\mathrm{Normal}\left( `r normal_mu`, `r normal_sd` \right)$, because a linear transformation of a Normal random variable results in another Normal random variable.
Figure \@ref(fig:plot-normal-factoring) shows the probability density of $\pi$ and the implied density of $h(\pi)$.
Any function of a parameter will an implied probability density, but there is no general guarantee that the implied density will be easy to understand like the example in Figure \@ref(fig:plot-normal-factoring).

```{r plot-normal-factoring, include = TRUE, fig.cap = "Demonstration of centered and non-centered parameterizations for a Normal distribution. The non-centered parameterization is statistically equivalent, but the location and scale are factored out of the distribution.", fig.height = 5}
```

In order to understand a model, the researcher should understand the consequences of priors on the model's predictive distribution for new data, because prior distributions in parameter space yield implied priors in outcome space.
This highlights an ever-present, pragmatic problem in building any model: a researcher has prior information about the _world_, but they must set priors on model parameters.
This requires the researcher to solve backward for priors in parameter space that resemble reasonable expectations about the behavior of data.
In order to understand and anticipate the consequences of priors, it is important to understand the data's functional dependence on model parameters and how those functions transform probability densities.
It is thus a general principal of model-building that "the prior can often only be understood in the context of the likelihood" [@gelman-et-al:2017:prior-likelihood].
The close relationship between priors and the data model expose where flat priors create problematic model behaviors.



```{r beta-binom}
n_bb <- 1e4

set.seed(9348)

tibble(
  alpha = rbeta(n_bb, 1, 1),
  alpha_normal = plogis(rnorm(n_bb, 0, 10)),
  alpha_logistic = plogis(rlogis(n_bb, 0, 1))
) %>%
  mutate(
    x_beta = rbinom(n = n(), size = n_bb, prob = alpha),
    x_normal = rbinom(n = n(), size = n_bb, prob = alpha_normal),
    x_logistic = rbinom(n = n(), size = n_bb, prob = alpha_logistic)
  ) %>%
  pivot_longer(
    cols = starts_with("x_"), 
    names_to = "param",
    values_to = "value"
  ) %>%
  mutate(
    param = fct_relevel(param, "x_beta", "x_normal", "x_logistic")
  ) %>%
  ggplot() +
  aes(x = value) +
  geom_histogram(
    fill = primary,
    boundary = 0, 
    position = "identity", 
    alpha = 0.7
  ) +
  facet_wrap(
    ~ param,
    labeller = as_labeller(
      c(
        "x_beta" = "Probability scale: Beta(1, 1)",
        "x_normal" = "Logit scale: Normal(0, 10)",
        "x_logistic" = "Logit scale: Logistic(0, 1)"
      )
    )
  ) +
  scale_fill_viridis_d(option = "plasma", end = 0.88) +
  scale_x_continuous(labels = comma) +
  ggeasy::easy_remove_legend() +
  ggeasy::easy_remove_y_axis() +
  labs(
    title = "Prior Flatness ≠ Prior Vagueness",
    subtitle = "How transformations of parameter space affect implied priors",
    x = str_glue("Binomial variable x ({comma(n_bb)} draws)"),
    y = NULL
  )
```

It is helpful to highlight an example where flat priors, believed "by default" to be reasonable and conservative, create problematic or nonsensical results [@seaman-et-al:2012:noninformative-priors].
Consider a binomial outcome variable that counts $x$ successes out of $n$ independent trials, each with success probability $\alpha$.
We want to estimate $\alpha$, and we don't have specific prior information about it.
We represent ignorance about $\alpha$ using a flat $\mathrm{Beta}\left(1, 1\right)$ density. 
Not consider the identical data but estimating $\alpha$ with a logit model likelihood instead, where $\alpha = \mathrm{logit}^{-1}\left(\eta\right)$.
In the logit model, $\alpha$ is a deterministic function of the parameter $\eta$, so we place the prior on $\eta$.
We are ignorant about its value as well, so we follow a default instinct and give it a prior with a wide variance, $\mathrm{Normal}(0, 10)$ on the logit scale, and we might have considered scale values larger than 10 as well to represent more ignorance.

If we take both of these models and generate prior simulations for $x \sim \mathrm{Binom}(n, \alpha)$, depicted as histograms in Figure \@ref(fig:beta-binom), the implied prior distributions for $x$ do not resemble one another at all.
The first panel shows the implied prior for $x$ when $\alpha$ has a flat Beta prior, resulting in a distribution for $x$ that is also flat.
The middle panel shows the implied prior for $x$ from the logit model, where $\mathrm{logit}\left(\alpha\right) = \eta \sim \mathrm{Normal}\left(0, 10\right)$, resulting in a very different prior for $x$ where most prior probability is concentrated on very small or very large values of $x$.
Why does this prior for $x$ appear so informative when we used such a vague prior for $\eta$?
Because $\eta$ had a wide prior on the logit scale, and the probability scale is a nonlinear transformation of the logit scale, the resulting prior for $x$ is also affected by the nonlinear mapping between parameter spaces.
Only a thin range of logit-scale values map to probabilities that we routinely encounter in political science:
logit sclae values between $-3$ and $3$ correspond to probabilities between approximately $`r plogis(-3) %>% round(3)`$ and $`r plogis(3) %>% round(3)`$.
Because the $\mathrm{Normal}\left(0, 10\right)$ prior places most probability density outside of reasonably values on the logit scale, most of our implied probabilities are unreasonably small or large as well.
In order to obtain a flat prior for $x$ using a logit model, we would actually use the prior $\eta \sim \mathrm{Logistic}\left(0, 1\right)$, shown in the third panel of Figure \@ref(fig:beta-binomial).
The standard Logistic prior creates a flat density on the probability scale because the inverse link function for a logit model is the cumulative distribution function for a standard Logistic distribution.^[
  This same intuition holds for a standard Normal prior in a probit model, demonstrated in Section \@ref(sec:priors).
]



```{r beta-binom, include = TRUE, fig.width = 9, fig.height = 4, out.width = "100%"}
```




- [x] Introduce implied prior
- [x]  outcome vs. parameters
- principal: info vs. shape
- example, horiuchi
- lesson for causal inference: we should expect to get different results because thinking explicitly about priors shows us where implied priors aren't flat (and that's fine) or where implied priors ARE flat (and that's batshit).
<!-- information vs. shape, warping, covariant transformations -->

Flat priors are problematic for many analyses because they lead to implied priors that have strange behaviors that researchers would be uncomfortable confronting.


More complex functions of parameters, especially nonlinear functions, can produce in implied priors with strange shapes that are difficult to anticipate using intuition alone. [horiuchi]


Outcome space != parameter space. Flat priors about outcomes is not the same thing as flat priors about parameters.
Information ≠ shape.
The mapping from geometric space to statistical information is entirely dependent on the scale of a parameter space, and different model parameterizations invoke different scales.

<!------- TO DO ---------
- start with a flat priors experiment and then go to the probit space?
- make the connection to _likelihoods_ explicit.
------------------------->




<!------- TO DO ---------
- "flat" isn't non-informative. They are just non-informative _relative to_ the likelihood. A likelihood that identifies a narrower region of parameter space will more quickly overcome a flatter prior, but it isn't the same as "non-informative." The speed with which a likelihood targets parameter space is a feature of the likelihood.
------------------------->

<!------- TO DO ---------
- flat priors over effects imply non-flat priors for potential outcomes
- flat priors for potential outcomes imply mode-zero priors for causal effects
------------------------->

<!------- TO DO ---------
- does this go in previous section?
------------------------->

The primary resistance to Bayesian inference in applied research is the need to set a prior at all. To many researchers, the prior distribution is an additional assumptions that is never feels justified because it is external to the data. Often researchers wish to sidestep this choice altogether, preferring a "flat" prior that prefers all parameters equally.

We have seen so far that the parameterization of a model has consequences for prior specification. Reparameterization may result in an algebraically equivalent likelihood
<!------- TO DO ---------
- define "equivalent reparameterization"
------------------------->


The incoherence of flatness: 

- no universally valid strategy for specifying flat priors because it is always possible to rearrange the data model either by transforming a parameter or otherwise rearranging the likelihood. 

Consider an experiment with a binary treatment $Z$ and a binary outcome variable $Y$. We want to determine the effect of $Z$ by comparing the success probability in the treatment group, $\pi_{1}$, to the success probability in the control group, $\pi_{0}$.



- "no way to conceptualize an uninformative prior because you can always rearrange the problem through a reparameterization or transformation of a parameter"
- examples of transformations having crazy implications/MLE being wild (logit).
- Jeffreys prior: actually a very limited range of priors that satisfy an "invariance" property. My words: such that the "amount of information obtained from data about is invariant to parameterization of the likelihood, for all possible values of the parameter," or, "the only way for the posterior distribution to be exactly the same, given the same data, for all true parameter values (?), is the Jeffreys prior," or, regardless of the data, I will learn the same thing about the generative model regardless of which equivalent parameterization of the generative model is used.  
    - is it worth it to think about the theoretical meaning of information
    - how does flatness reflect information in nonlinear scales?


Suppose we have some posterior distribution which relies on some parameter vector $\overrightarrow{\alpha}$. 
\begin{align}
  p\left(\overrightarrow{\alpha} \mid y\right) &\propto 
    p(y \mid \overrightarrow{\alpha} )
    p( \overrightarrow{\alpha} )
\end{align}
Consider some alternate parameterization of the likelihood parameterized by $\overrightarrow{\beta}$. 

Nonlinear transformation of $\pi$ does not preserve a uniform density over parameters.





Alex meeting takeaways:

- every prior has a "covariant" prior in a different parameterization
- the posteriors will be covariant as well.
- The way you get between them is by transforming the parameter and doing the appropriate Jacobian transformation to the density.
- Jeffrey's priors are a special case of this where the prior is proportional to the determinant of the information matrix. This has the beneficial property of "optimal learning" from the data. For example, flat Beta prior doesn't "hedge toward 50" in quite the same way.





### Priors and Model Parameterization {sec:prior-parameterization}

<!-- ### Principled Approaches to Model Parameterization -->

<!------- TO DO ---------
- this needs reverse outline
- triangular prior over treatment effect
------------------------->

Priors are defined with respect to a model of the data (the likelihood). 
<!------- TO DO ---------
- discuss Betancourt, Simpson, Gelman
------------------------->
We may have priors about the way the world works, but we rarely have priors about model parameters. This is because parameters are an invention in the model. They are mathematical abstractions similar to points and lines, so they only exist when we translate the world into a mathematical language. This means that the mathematical representation of the world is in direct dialog with the choices available to a researcher about how to encode prior information. In the real world, the prior information that I have about the world isn't affected by a mathematical representation of the world. As a researcher, the way I encode prior information depends on the choices I make about that mathematical representation.

One essential feature for understanding prior choices in practice is the _parameterization_ of the data model, $p(y \mid \phi)$, for some generic parameter $\phi$.^[
  Bayesian practitioners sometimes refer to the data model as the "likelihood." 
  This can be confusing because the "likelihood function" more traditionally refers to the _product_ of the data probabilities under the data model.
  References to the "parameterization of the likelihood" should be understood as interchangeable with "parameterization of the data model," since the former is determined entirely by the latter. 
]
We say that a data model has an "equivalent reparameterization" if for some transformed parameter $\psi = f(\phi)$, the function that defines the data model can be rewritten in terms of $\psi$ and return an equivalent likelihood of the data. More formally, the parameterization is equivalent if $p(y \mid \phi) = p(y \mid \psi)$ for all possible $y$.

In a maximum likelihood framework, equivalent parameterizations are a more benign feature of the modeling framework. Reparameterization may result in likelihood surfaces that have easier geometries for optimization algorithms to explore, but the _value_ of the likelihood function is unaffected by the algebraic definition or parameterization of the likelihood function. For instance, a Normally distributed variable $x$ with mean $\mu$ could be parameterized in terms of standard deviation $\sigma$ or in terms of precision $\tau = \frac{1}{\sigma^{2}}$, but the resulting density is unaffected.
\begin{align}
  \frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{x - \mu}{2\sigma}\right)^{2}}
  &=
  \sqrt{\frac{\tau}{2\pi}}e^{-\frac{\tau\left(x - \mu\right)^{2}}{2}}
  (\#eq:normal-sigma-equal-tau)
\end{align}
The consequence for Bayesian analysis, however, is that the parameterization of the data model determines the set of parameters and their functional relationship to the data.


<!------- TO DO ---------
- linear reparameterization: easy to understand
- decide if non-centering is worth it to discuss here
- nonlinear reparameterization: a much bigger problem
------------------------->



One example of equivalent reparameterization arises with the different possible ways to write a linear regression model. 
The first form specifies $y_{i}$ for unit $i$ as a linear function of $x_{i}$ with a random error that is mean $0$ and standard deviation $\sigma$.
\begin{align}
  (\#eq:ols-noncenter)
  y_{i} &= \alpha + \beta x_{i} + \varepsilon_{i}, & \text{where } \varepsilon_{i} &\sim \mathrm{Normal}\left(0, \sigma \right)
\end{align}
The second form, more common when viewing linear regression in the framework of generalized linear models, is to express $y_{i}$ directly as the random variable, with a conditional mean defined by the regression function and standard deviation $\sigma$.
\begin{align}
  (\#eq:ols-center)
  y_{i} &\sim \mathrm{Normal}\left( \alpha + \beta x_{i},\sigma\right)
\end{align}
Algebraically, these two models are identical.
The difference is only a matter of which component has the distributional assumption.
In Equation \@ref(eq:ols-noncenter), the distribution is assigned to $\varepsilon_{i}$, so $y_{i}$ is a random variable only by way of $\varepsilon_{i}$.
In Equation \@ref(eq:ols-center), we assign the distributional assumption directly to $y_{i}$, bringing the regression function into the mean rather than "factoring it out" of the distribution.


The linear regression context is one context where the choice of parameterization appears. These two parameterizations are typically called the "centered" and "non-centered" parameterization for a Normal distribution. In the centered parameterization, the random variable is drawn from a distribution "centered" on a systematic component, whereas the non-centered distribution factors out any location and scale information from the distribution, such that the only remaining random variable is a standardized variate. The equations below describe a Normal variable $\nu$ with mean $`r normal_mu`$ and standard deviation $`r normal_sd`$.
\begin{align}
  \text{Centered Parameterization:} && \nu &\sim \mathrm{Normal}(`r normal_mu`, `r normal_sd`) \\
  \text{Non-Centered Parameterization:} && \nu &= `r normal_mu` + `r normal_sd`z, & \text{where } z &\sim \mathrm{Normal}(0, 1)
\end{align}
<!-- To demonstrate that these parameterizations are equivalent, I simulate $$ simulations from each parameterization and plot their empirical cumulative distribution functions alongside each other in Figure \@ref(fig:plot-normal-factoring). Because the distributions are the same, the empirical CDFs are identical except for random sampling error.  -->

```{r, include = TRUE, fig.width = 10, fig.height = 2, out.width = "100%", fig.cap = "A spectrum of attitudes toward priors. "}
tibble(
  x = c(0.5, 2, 3.5),
  y = 1,
  label_head = c(
    "Minimalist Prior",
    "Pragmatic Prior",
    "Principled Prior"
  ),
  label_body = c(
    "Priors are nuisances,\nmerely facilitate inference/MCMC",
    "Priors are helpful:\nstructural info, regularization",
    "Priors ensure coherence,\nencode beliefs & hypotheses"
  )
) %>%
  ggplot() +
  aes(x = x, y= y) +
  geom_text(
    aes(y = 0.75, label = label_head),
    vjust = 0,
    fontface = "bold"
  ) +
  geom_text(
    aes(y = 0.75, label = label_body),
    vjust = 1.5
  ) +
  annotate(
    geom = "segment",
    arrow = arrow(
      angle = 30, length = unit(0.25, "inches"), ends = "both", type = "open"
    ),
    x = 0, xend = 4, y = 1, yend = 1
  ) +
  annotate(
    geom = "text",
    x = 0, y = 1, hjust = 1.1,
    label = "Less Informative"
  ) +
  annotate(
    geom = "text",
    x = 4, y = 1, hjust = -0.1,
    label = "More Informative"
  ) +
  coord_cartesian(ylim = c(0.25, 1.25), xlim = c(-1, 5)) +
  theme_void()
```





Problems of beliefs:

- No degree of belief.
- Parameterization makes this too challenging.
- Prior might change depending on what I ate for lunch. 
- "Elicitation" of priors satisfying the wrong audience, or at the very least can be easily misused. We do not want to elicit priors about arcane model parameters. We want to elicit priors about the _world_ (Gill Walker)d


Problems of nuisance prior

- parameterization gets you again
- the MLEs are unstable, overfit
- make the regularization argument in-sample




Pragmatic view of priors

- we're between full information and nuisance prior
- Weak information: structure, regularization, identification
- Structural information about parameters
- regularization toward zero (L1, L2), learning by pooling
- stabilizing weakly identified parameters, separation, etc.


Parameters are a _choice_. 

- They are part of the _rhetoric_ of a model. Sometimes we make pragmatic choices (something is easy to give an independent prior to, but independence isn't always valuable per se). Sometimes we make principled choices (normality, laplace, etc). 
- They deserve scrutiny (else just "excrete your posterior") and are a part of the model that you should check and diagnose. 
- They aren't merely a nuisance because we can use them to our benefit, 
- sometimes when we parameterize a problem to reveal easier things to place convenient priors on
<!------- TO DO ---------
- Gelman "Parameterization" paper: parameterization for convenience often reveals a new family of models. For Bayesian inference, it reveals a new family of priors. For _causal_ inference, we should pay careful attention to the way parameterization bears on the priors that we do and do not feel comfortable using, and the way that parameterization presents opportunities to improve on other work.
------------------------->



Models are a tool, set it up so that it works.

Constrained parameters in causal mediation?






```{r diff-means-example}
means_data <- 
  tibble(
    x = seq(-1, 1, .05),
    unif = ifelse(x >= 0, dunif(x, 0, 1), NA),
    triangle = 
      ifelse(
        x < 0, 
        (2 * (x - -1)) / ((1 - -1) * (0 - -1)), 
        (2 * (1 - x)) / ((1 - -1) * (1 - 0))
      )
  ) 

plot_prior <- function(data, x, y) {
  ggplot(data) +
  aes(x = {{x}}, y = {{y}}) +
  geom_ribbon(
    aes(ymin = 0, ymax = {{y}}),
    fill = primary
  ) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1.5))
}
```


```{r plot-diff-means-example}
p_unif <- 
  ggplot(means_data) +
  aes(x = x, y = unif) +
  geom_ribbon(
    aes(ymin = 0, ymax = unif),
    fill = primary,
    alpha = 0.7
  ) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1.5)) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(
    x = TeX("Group Mean: $\\mu_{z}$"),
    y = "Prior Density"
  )

p_triangle <- 
  ggplot(means_data) +
  aes(x = x, y = triangle) +
  geom_ribbon(
    aes(ymin = 0, ymax = triangle),
    fill = primary,
    alpha = 0.7
  ) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1.5)) +
  labs(x = TeX("Treatment Effect: $\\mu_{z = 1} - \\mu_{z = 0}$"), y = NULL) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) 

p_unif + p_triangle +
  plot_annotation(
    title = "Prior Densities for Difference in Means",
    subtitle = "If means have Uniform(0, 1) priors",
    caption = TeX("$x$-axes not fixed across panels")
  )
```



For instance, consider a simple experiment with a binary outcome variable $y_{i}$ and binary treatment assignment $z_{i} \in \{0, 1\}$. Suppose that the treatment effect of interest is a difference-in-means, $\bar{y}_{z = 1} - \bar{y}_{z = 0}$, estimated from a linear probability model. This linear probability model might be parameterized in two ways. First is a conventional regression setup,
\begin{align}
  y_{i} &= \alpha + \beta z_{i} + \epsilon_{i}
  (\#eq:diff-means-example)
\end{align}
where $\alpha$ is the control group mean, $\alpha + \beta$ is the treatment group mean, $\beta$ represents the difference in means, and $\epsilon_{i}$ is a symmetric error term for unit $i$. With the model parameterized in this way, the researcher must specify priors for $\alpha$ and $\beta$. Suppose that the researcher gives $\beta$ a flat prior to represent ignorance about the treatment effect. An equivalent _likelihood model_ for the data would be to treat each observation as a function of its group mean $\mu_{z}$.
\begin{align}
  y_{i} &= z_{i}\mu_{1} + \left(1 - z_{i}\right)\mu_{0}  + \epsilon_{i}
  (\#eq:separate-means-example)
\end{align}
Although the treatment effect $\beta$ from Equation \@ref(eq:diff-means-example) is equivalent to the difference in means $\mu_{1} - \mu_{0}$ from Equation \@ref(eq:separate-means-example), the parameterization of the model affects the implied prior for the difference in means. If the researcher gives a flat prior to both $\mu_{\mathbf{z}}$ terms, the implied prior for the difference in means will not be flat. Instead, it will be triangular, as shown in Figure \@ref(fig:plot-diff-means-example). The underlying mechanics of this problem are well-known in applied statistics---if we continue adding parameters, the Central Limit Theorem describes how the resulting distribution will converge to Normality---but it takes the explicit specification of priors to shine a light on the consequences of default prior choices in a particular case. In particular it shows how even flat priors, which are popularly regarded as "agnostic" priors because of their implicit connection to maximum likelihood estimators, do not necessarily imply flat priors about the researcher's key quantities of interest. Rather, flat priors can create a variety of unintended prior distributions that do not match the researcher's expectations. I return to this important idea in the discussion about setting priors for a probit model in Section \@ref(sec:probit-prior).

<!------- TO DO ---------
- probit in the model chapter?
- logit in this chapter?
------------------------->

```{r plot-diff-means-example, include = TRUE, fig.height = 5, fig.width = 8, out.width = "100%", fig.cap = "Model parameterization affects prior distributions for quantities of interest that are functions of multiple parameters or transformed parameters. The left panel shows that the difference between two means does not have a flat prior if the two means are given flat priors. Note that the $x$-axes are not fixed across panels."}
```

- equivalent parameterizations



```{r}
tibble(
  r = 1:50000
) %>%
  mutate(
    prior_control = runif(n = n(), 0, 1),
    treat_lower = 0 - prior_control,
    treat_upper = 1 - prior_control,
    prior_effect = rnorm(n = n(), sd = 10),
    prior_treat = prior_control + prior_effect
  ) %>%
  pivot_longer(cols = starts_with("prior_")) %>%
  ggplot(aes(x = value)) +
  facet_wrap(~ name) +
  geom_histogram(boundary = 0)
```






### Balancing Pragmatic and Principled Approaches to Priors {#sec:bayes-how-to}



Various roles that priors take on

- merely facilitate posterior inference
- structural information
- weak information
- regularization / stabilization
- prior knowledge

A general orientation toward priors in this dissertation:

- Not about "stacking the deck" or hazy notions of "prior beliefs"
- information, not belief
- Bayesian view of probability is _more general_, contains information and beliefs. Information is priors, but it's also data. Information is the fundamental unit of uncertainty-quantification
- inference about the thing we care about (counterfactuals)
- structural information when we have it
- Causal inference: "agnosticism" is something valuable generally

Priors are not de-confounders

- downweighting, not upweighting


### Statistical Bias

1. posterior isn't about frequency properties (especially in one-off data)
2. What is "bias?"
    - look up in BDA
    - "Requiring unbiased estimates will often lead to relevant information being ignored (as we discuss with hierarchical models in Chapter 5)" (94) 
- Why would we want this? Inference makes more sense. 
    - What's the probability of a model/hypothesis, given the data 
    - vs. What's the probability of "more extreme data" (?) given a model that I do not believe.
- posterior probabilities mean what they say they mean
    - conditioning on data (and implicitly the model), this is the distribution of parameters
    - p-values are the "probability of more extreme results." They condition on the model, but they're only useful if they do not.
- Proper frequentist analysis is violated as soon as you look at the data. 
- Frequency properties are still possible for Bayesian estimators, but we view frequency properties as a byproduct of something more essential (MSE).




### Structural Priors and Weak Information

Structure (bounds), regularization (L1, L2), hierarchy

$p$ doesn't care about your $n$.




### Understanding Log Prior _Shape_

This is low-key pretty big


### Models for Means {#sec:models-for-means}

In many situations, we can get around a data-level model if we want to.



### Generalization, Big and Small

<!------- TO DO ---------
- should this go at the end?
------------------------->


Models (likelihoods) are priors

- we restrict the space of all other models
- could think about "flexible" models, but these are just priors over more spaces

Identification assumptions are priors

Generalization to any population is a prior

- priors are not MY data, but ANY data
- parameters describe ANY data
- Lampinen Vehtari 2001: likelihood as "prior for the data" is the basis for all generalization from any finite model
<!-- Describing the prior information explicitly distinguishes the Bayesian approach from the maximum likelihood (ML) methods. It is important to notice, however, that the role of prior knowledge is equally important in any other approach, including the ML. Basically, all generalization is based on the prior knowledge, as discussed in Lemm (1996, 1999); the training samples provide information only at those points, and the prior knowledge provides the necessary link between the training samples and the not yet measured future samples.  -->

We are always doing violence, but the framework lets us build out more and more general models to structure our uncertainties

Email to David:

> My first reaction to this is like this: it's probably correct to say that Bayesians may have some shared ideas about how to think about generalizing that might differ systematically from non-Bayesians, but I am not sure how much of that is because of Bayes per se so much as just...the types of modeling someone is willing to do. By "modeling" I mean, functional form assumptions are you willing to make about data, which is different from "Bayes" in that the former is something required of all modeling and the latter is only what you can say about parameters. For example, a functional modeling thing you might do is specify (using some weights or something) how an estimate in one sample might map to another sample, but whether you do that with Bayesian estimation is a separate choice aside from the functional model itself. That being said, even though functional modeling things can be done with Bayes/non-Bayes point of view, it does seem right to say that certain modeling approaches may feel more natural in a Bayesian framework, or that Bayesians might construe a problem slightly differently because they are more used to hierarchical modeling. 
> 
> That's a pragmatic view of things, but I can give you a more theoretically abstract view, and think of situations where Bayesian lets you do things that non-Bayes can't. It all comes down to how "seriously" you want to take the tenets of Bayesian work and what kind of generalization-based claims you want to make. So I will lay out a series of vignettes that start from a world where Bayes is "not unique" and then gets into worlds where Bayes gets more and more necessary to say what you want to say about the out-of-sample world. I will use the example of estimating some parameter, but you can translate this into learning about a "mechanism" however Erica and Nick are defining that, and you can think about it non-statistically as well even if I'll use statistical modeling language.
> 
> I estimate some parameter $\mu$ in a study, but it's one instance of a more general phenomenon. If the study were representative of the world, you can imagine that the effect is one instance of the "population effect" and give it a hierarchical prior as such: $\mu \sim D(\theta, \sigma)$ where D() is some distribution, $\theta$ is the "general" effect. If I learn about $\mu$ (the estimate and standard error $\sigma$), I learn about $\theta$! Choice of distribution depends on your assumptions about the stochastic process at work. naturally, but that logic works basically just like a likelihood function choice. (In fact, Bayes sees priors for parameters as mechanistically no different from likelihoods for data. Which is to say, MLE models like logit are simply hierarchical priors on the data, and regression is estimating the hierarchical parameters of the prior.) This is basically a meta-analysis setup using Bayesian language: if you want tangible examples you can look at the Don Rubin "eight schools experiment" which is about generalizing from parallel studies in schools, or Rachael Meager (sp) has a paper applying this setup to micro-credit experiments in the development econ context: what do we learn about the "overall effect of microcredit expansion" by assimilating information from different studies. In this sense the priors are just ways to structure the meta-analysis.
> 
> If the study is unrepresentative or "not externally valid" then it's up to the researcher to specify some approach to modeling the invalidity: $\theta \sim D(f(\theta), \sigma)$, where f() is some function that distorts the representativeness of the study. Which is to say, $f(\theta)$ is the expectation for a study with these distortions. Researcher's task is then to learn the form of f(). These distortions might be like sample bias, the country where the study was performed, or whatever, and all you're really doing is reweighting or adjusting the estimate to make more sense for the target population. If you can estimate parameters that determine f(), viola you can infer the posterior distribution for the true $\theta$. But this is what I mean when I say that none of this is really EXCLUSIVELY Bayesian. Reweighting/adjusting happens in non-Bayes world all the time; the main thing that is different is how you write the model and your ability to say that the population estimate is a "posterior of the true parameter, given the information learned from the data." One example of this kind of thing is maybe the multilevel regression and poststratification: we use national surveys to model the attitudes of different demographic groups, and then we use those model predictions plus census information to project estimates for smaller units, for example states or counties, based on the demographic composition of those units! This example goes the other direction from where Erica and Nick want to go (from representative to unrepresentative) but the technology sounds similar: estimate something in the data that you have, and map it into a space where you do not have data. MRP in political science comes from Bayes world and feels natural there, but nothing saying that it HAS to be Bayesian in its overall approach.
> 
> Now we get to a world where Bayes is more necessary. If there's something TRULY Bayesian that really makes no sense withoutBayes, it is the fact that I actually do not need data about f() in order to estimate $\theta$. This is because I have priors even in the absence of data. If all I have is priors about f(), then even if I collect data about ONLY $\mu$ and NOTHING about f(), I nonetheless update my information about $\theta$. This is because the parameters are functionally related through f(), so if I learn about the subsample then I learn about the population. Stated differently, learning about $\mu$ restricts the space of $\theta$ because I can basically "solve backward" using my priors about f. This is the stuff that is very natural in Bayes, and I can think of basically no analog in non-Bayes that lets you do something similar (other than picking point estimates for unknowns and simulating, which doesn't have the same theoretical coherence as a prior/posterior distribution). Of course, this means that inference on f() is subject to the priors that go into f(), which is exactly the kind of thing that non-Bayesians are super afraid of despite majorly misconstruing how this works (IMO). For one, the functional form of f() is the kind of thing non-Bayesians would make assumptions about anyway, so that's not unique to Bayes at all. And secondly, the priors that would go into f() would usually be generic enough that researchers aren't "picking their hypothesis" (a common and frankly stupid stereotype) so much as restricting the space of f() to rule out stuff that's frankly impossible. Happy to give you more concrete examples of the kind of "weakly informative priors" that someone would use in a situation like that if it's a route you want to dig into more. It's this kind of stuff that I think non-Bayesians are under-utilizing: how much extrapolation power you get by being willing to place even weak priors on stuff you can't exactly identify with data. And if you REALLY want to give a nod to Bayesian views of extrapolation, this is the area you'd want to dig into, because it's the stuff that doesn't really make sense without Bayes. You can sort of see Ken and I do this in our voter ID paper (which you can find on my website) though we kind of wimp out of fully placing priors on f().
> 
> Here's where things get really abstract because, gun to my head, we can be really scorched-earth and say that all extrapolation falls apart without a Bayesian notion of priors. Think about any model for data: $y \sim D(\theta)$, I think my data come from this distribution, and if I were to go out into the world and collect new data, my estimate for a new data point is characterized by this distribution assumption i.e. this prior for the data. If you try to lay out a formal definition of what "generalization" is, I would say that there is no such thing as generalization without an implicit prior that links your observed data to unobserved things that you want to project into. There are stats theorems out there called "no free lunch" theorems that basically say "all statistical inference is limiting the space of models that link parameters to data, and there is no way to improve your guess for a new data point using a model except to impose prior information on the system by way of the model." So this would be a hard line view of what priors mean in a philosophy of science (not necessarily quantitative or statistical, mind you), but that if you accept that view it trickles down into the more minor examples in a very natural way: the only way to generalize is by using priors to structure the connection between what I do observe and what I do not observe.








### Regularization and Prediction Problems



One-Off Data Collection:

1. posterior isn't about frequency properties
2. What is "bias?"
    - look up in BDA
    - "Requiring unbiased estimates will often lead to relevant information being ignored (as we discuss with hierarchical models in Chapter 5)" (94) 
- Why would we want this? Inference makes more sense. 
    - What's the probability of a model/hypothesis, given the data 
    - vs. What's the probability of "more extreme data" (?) given a model that I do not believe.
- posterior probabilities mean what they say they mean
    - conditioning on data (and implicitly the model), this is the distribution of parameters
    - p-values are the "probability of more extreme results." They condition on the model, but they're only useful if they do not.
- Proper frequentist analysis is violated as soon as you look at the data. 
- Frequency properties are still possible for Bayesian estimators, but we view frequency properties as a byproduct of something more essential (MSE).


### Regularization-Induced Confounding

This is a huge, underappreciated problem in the broad ML-for-causal-inference world


## Bayesian Opportunities





### Priors for Imperfect Identifiability {#sec:identifiability-priors}

Relatedly: priors over models


"The Bayesian approach also clarifies what can be learned in the noncompliance problem when causal estimands are intrinsically not fully “identified.” In par- ticular, issues of identification are quite different from those in the frequen- tist perspective because with proper prior distributions, posterior distribu- tions are always proper." (Imbens and Rubin 1997)

This is where Imbens and Rubin push (also Horiuchi et al. example)

Randomization limits the impact of the Bayesian assumptions 

- "Classical random- ized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis." (Rubin 1978)

<!------- TO DO ---------
- from above:
------------------------->

Casting these identification assumptions as priors has at least two distinct benefits over an approach with fixed sensitivity parameter values.
Firstly, it allows the researcher to conduct inference about the treatment effect by _marginalizing over_ their priors for design parameters, rather than conditioning on fixed design parameters.
By marginalizing over the design parameters, the researcher obtains one posterior distribution that averages over their prior design uncertainty, rather than an arbitrary range of design parameter values that contains no information about which design parameter values are realistic or unrealistic.
Secondly, and relatedly, marginalizing over design parameters speaks to the inferential question that is more consistent with the researcher's guiding question: which treatment effects are consistent with the data and our prior information.
Sensitivity analysis using fixed parameters, meanwhile, answers a different question that is less useful for the goal of posterior inference about treatment effects: how big of an assumption violation is required in order for treatment effect estimates to be statistically insignificant, with no mention of whether violations of that size are likely or unlikely.

returns a quantity of interest that is more consistent with the researcher's ultimate inferential goal—what treatment effects are consistent with prior information and the data—rather than a quantity 



Many identification assumptions can be encoded as functional form assumptions in an estimated model.
Consider a regression scenario with outcome $Y_{i}$, treatment $A_{i}$, and some other covariate $U_{i}$.
\begin{align}
  \begin{split}
    \text{True model: } Y_{i} &= \alpha + \tau A_{i} + \beta U_{i} + \epsilon_{i} \\
    \text{Estimated model: } Y_{i} &= \hat{\alpha} + \hat{\tau} A_{i} + e_{i}
  \end{split}
(\#eq:omitted-confounder)
\end{align}
If we invoke an identifying assumption that the assignment of $A_{i}$ is ignorable, then our estimate for the average effect of $A_{i}$ using the second line of \@ref(eq:omitted-confounder) is unbiased.
This is equivalent to an assumption that the correlation between the treatment $A_{i}$ and the true error term $\epsilon_{i}$ is exactly zero.
For causal inference purposes, researchers have developed sensitivity tests that recover the treatment effect under ignorability violations, which work by picking a non-zero error correlation value and deriving the treatment under that model [@imai-et-al:2011:black-mox-mediation; @acharya-blackwell-sen:2016:direct-effects].

, and building a richer understanding of how inferences depend on modeling assumptions.

Researchers have derived estimators for $\tau$ in the presence of a, which can be used in a causal inference context to test the "robustness" or "sensitivity" of inferences to the ignorability assumption [@imai-et-al:2011:black-mox-mediation; @acharya-blackwell-sen:2016:direct-effects].
If this correlation is zero, the treatment effect estimate is unbiased
These are 

In a causal inference context, researchers sometimes develop sensitivity tests to test the robustness of their inference to violations of ignorability assumptions [@imai-et-al:2011:black-mox-mediation; @acharya-blackwell-sen:2016:direct-effects], exclusion restrictions [@nyhan-skovron-titiunik:2017:reg-bias], and so on.
Because identification assumptions are special cases of these more general models with additional nuisance parameters, it would be straightforward in a Bayesian framework to specify prior distributions on nuisance parameters that represent priors for violated assumptions. 
Unpublished work by @leavitt:2020:bayes-did explores this approach in a difference-in-difference contexts, introducing an "epistemic" source of variance in his treatment effect estimate by specifying a prior on the parallel trends assumption.




- Oganisian and Roy (2020) "identification" assumptions vs. "statistical" assumptions. Identification assumptions get you to a place where you can express causal effects in terms of expectations about $Y$ given treatment, within confounding strata (perhaps then averaging over confounders). "Statistical" assumptions define how we think we can build $E[Y]$



### Application: Weakly Informed Regression Discontinuity

This section presents a reanalysis of Hall's -@hall:2015:extremists regression discontinuity study of congressional elections.
Portions of the original analysis contain a pathological result where confidence intervals for key parameters of interest contain values that could not possibly occur with nonzero probability.
We overcome the pathology using weakly informative priors that contain structural information about the dependent variable only, excluding impossible parameters from the prior but uninformative over possible parameter values.
This minor prior intervention successfully guides the posterior distribution away from impossible regions of parameter space, resulting in a posterior distribution that is consistent with the data as well as external structural information about the problem.
This intervention does not undermine the main takeaway from the original study, but the Bayesian estimates for the effect of interest are notably smaller and more precisely estimated.

With similar aims to this project, @hall:2015:extremists examines primary elections and their impact on ideological representation in Congress.
The study asks if extremist candidates for Congress are more likely or less likely to win the general election contest than candidates who are relatively moderate by comparison.
The treatment variable of interest, the ideological extremity of a party's general election nominee, is confounded by several factors.
Competitive districts may lead more moderate candidates to run in the first place, creating selection biases for which candidates represent which district.
Conversely, voters in electorally "safe" districts may feel freer to nominate more extreme candidates because the likelihood of their party losing the seat in the general election is sufficiently low. 
Furthermore, the incumbency advantage in general elections confounds this picture if incumbents tend to be more moderate than challengers who are angling to raise their name recognition [@gelman-king:1990:incumbency]. 

To identify the effect of candidate ideology, @hall:2015:extremists leverages the vote margin in the primary election as a forcing variable in a regression discontinuity design (RDD).
In a primary contest between a relative extremist and a relative moderate, the extremist advances to the general election if their vote share in the primary is greater than the moderate's, i.e.\ the extremist's _margin_ (difference) over the moderate is any greater than $0$.
If the extremist's primary margin is any less than $0$, the moderate advances to the general election instead.
This primary margin deterministically assigns congressional candidacies to treatment or control if the extremist wins or loses the primary, respectively.
While candidate ideology's effect on general election outcomes may be confounded in the aggregate, the effect can be identified at the threshold (extremist margin of $0$).
The key identification assumption for a "sharp" regression discontinuity design is that the forcing variable, $X_{i}$, and the expected outcome given the forcing variable, $\E{Y_{i}(x) \mid X_{i}}$, are both continuous at the threshold $x_{0}$.
This assumption identifies _local_ treatment as the difference in the limits of the conditional expectations for treatment ($X = 1$) and control ($X = 0$) at the threshold [@skovron-titiunik:2015:practical-rdd; @calonico-et-al:2014:rdd-CIs].
\begin{align}
  \lim_{x \downarrow x_{0}} \E{Y_{i} \mid X_{i} = x} -
    \lim_{x \uparrow x_{0}} \E{Y_{i} \mid X_{i} = x}
  &= 
  \E{Y_{i}(1) - Y_{i}(0) \mid X_{i} = x_{0}}
  (\#eq:rdd-continuity)
\end{align}
Equation \@ref(eq:rdd-continuity) implies that the difference in potential outcomes can be identified from observed data only by observing that everything else about units is continuous at the threshold except for the realized treatment value.

```{r rd-data}
load(here("data", "_model-output", "03-causality", "RDD-workspace.Rdata"))
```

```{r plot-rd}
tibble(
  rv = seq(0 - bandwidth_margin, 0 + bandwidth_margin, .0001),
  treat = as.numeric(rv > 0)
) %>%
  prediction::prediction(ols_lmfit, data = ., interval = "confidence") %>%
  as_tibble() %>%
  ggplot() +
  aes(x = rv, y = fitted.fit, group = treat) +
  coord_cartesian(
    ylim = c(-0.05, 1.25), 
    xlim = c(-bandwidth_margin, bandwidth_margin)
  ) +
  # annotate(
  #   geom = "ribbon",
  #   x = c(-0.02, 0),
  #   ymin = c(0.25, 0), ymax = c(.25, 1),
  #   color = secondary,
  #   fill = secondary_light,
  #   alpha = 0.2
  # ) +
  # annotate(
  #   geom = "text",
  #   x = -0.02, y = 0.25,
  #   label = "Range of possible\nintercept values",
  #   hjust = 1.1
  # ) +
  annotate(
    geom = "text",
    x = 0.006, y = 1.15,
    label = "Confidence set includes \nintercepts with zero\nprior probability",
    hjust = 0
  ) +
  annotate(
    geom = "ribbon",
    x = c(0, 0.005),
    ymax = c(
      max(confint(ols_lmfit)["(Intercept)", ]), 
      mean(c(max(confint(ols_lmfit)["(Intercept)", ]), 1))
    ),
    ymin = c(1, mean(c(max(confint(ols_lmfit)["(Intercept)", ]), 1))),
    color = med2,
    fill = med2,
    alpha = 0.5
  ) +
  geom_ribbon(
    aes(ymin = fitted.lwr, ymax = fitted.upr),
    # fill = primary_light,
    fill = "gray",
    alpha = 0.5
  ) +
  geom_line() +
  geom_vline(xintercept = 0) +
  geom_line(aes(y = 0), linetype = 2) +
  geom_line(aes(y = 1), linetype = 2) +
  # annotate(
  #   geom = "segment", linetype = 2, 
  #   x = -0.05, xend = -0.05, y = 0, yend = 1
  # ) +
  # annotate(
  #   geom = "segment", linetype = 2,
  #   x = 0.05, xend = 0.05, y = 0, yend = 1
  # ) +
  scale_y_continuous(breaks = seq(0, 1, .25)) +
  scale_x_continuous(
    breaks = seq(-.04, .04, .02), 
    labels = scales::number_format(scale = 100, suffix = " pts")) +
  labs(
    title = "RDD Predictions from OLS Model",
    # subtitle = "Local treatment effect at threshold",
    y = "General election win probability",
    x = "Extremist vote margin in primary election"
  )
```

```{r flat-samples}
const_draws <- 
  gather_draws(brm_flat, b_control, b_treat) %>%
  mutate(
    invalid = (.value < 0) | (.value > 1),
    .variable = case_when(
      .variable == "b_control" ~ "Non-Extremist Intercept",
      .variable == "b_treat" ~ "Extremist Intercept"
    )
  ) %>%
  print()

effect_draws <- 
  spread_draws(brm_flat, b_control, b_treat, trt_effect) %>%
  mutate(
    invalid = 
      (b_control < 0 | b_control > 1) |
      (b_treat < 0 | b_treat > 1) |
      (trt_effect < -1 | trt_effect > 1)
  ) %>%
  select(-c(b_control, b_treat)) %>%
  mutate(
    .value = trt_effect,
    .variable = "Treatment Effect"
  ) %>%
  print()

```

```{r plot-flat-intercepts}
ggplot(const_draws) +
  aes(x = .value) +
  facet_wrap(~ .variable) +
  geom_histogram(
    boundary = 1, binwidth = .02,
    fill = primary,
    aes(alpha = invalid)
  ) +
  geom_vline(xintercept = c(0, 1), linetype = 2) +
  theme(legend.position = "none") +
  scale_alpha_manual(values = c("FALSE" = 0.8, "TRUE" = 0.3)) +
  labs(
    x = NULL, y = NULL,
    title = 'Posterior Samples from Bayesian Model',
    subtitle = 'All parameters given improper flat priors'
  )
```

```{r plot-flat-effect}
ggplot(effect_draws) +
  aes(x = .value, alpha = fct_rev(as.factor(invalid))) +
  # facet_wrap(~ .variable) +
  geom_histogram(
    boundary = 1, 
    binwidth = .02,
    fill = tertiary,
    position = "stack"
  ) +
  geom_vline(xintercept = 0, color = "gray80") +
  theme(legend.position = "none") +
  scale_alpha_manual(values = c("FALSE" = 1, "TRUE" = 0.5)) +
  labs(
    x = "LATE: Extremism Effect on Win Probability", 
    y = NULL,
    title = "Posterior Samples of Treatment Effect",
    subtitle = 'Bayesian linear model with improper flat priors'
  ) +
  annotate(
    geom = "text", x = -0.95, y = 210, 
    label = "Samples from invalid\nparameter space",
    vjust = -0.2
  ) +
  annotate(
    geom = "segment", 
    x = -0.95, y = 210, xend = -0.7, yend = 150
  ) +
  NULL
```

@hall:2015:extremists applies the RD design by assuming that the effect of candidate ideology on vote share and win probability in the general election are identified locally where the extremist's margin in the primary election crosses $0$.
For this example, we concentrate on models that predict win probability, since these are the estimates that contain pathological results that we can avoid with Bayesian methods.
Hall estimates RD models using a few different specifications, but I replicate his simplest design, which is a linear probability model (LPM) of the following form.
The local linear regression is justified by the limit intuition of the key assumption in \@ref(eq:rdd-continuity); any nonlinear regression function, as long as it is continuous at the cutoff, converges to linearity at the cutoff in the limit.
Data were obtained from Hall's replication materials, available on his website.^[
  <http://www.andrewbenjaminhall.com/>, last accessed July 02, 2020.
]
The outcome $y_{dpt}$ is a binary indicator that takes the value $1$ if the general candidate running in district $d$ for party $p$ in election year $t$ wins the general election, and it takes $0$ if the candidate loses the general election,
\begin{align}
\begin{split}
  y_{dpt} 
  &= 
  \beta_{0} + \beta_{1}(\text{Extremist Wins Primary})_{dpt} + 
    \beta_{2}(\text{Extremist Primary Margin})_{dpt} \\
    &\quad + \beta_{3}(\text{Extremist Wins Primary} \times \text{Extremist Primary Margin})_{dpt} + \epsilon_{dpt}
\end{split}
(\#eq:hall-rd)
\end{align}
where _Extremist Primary Margin_ is the extremist candidate's margin over the moderate candidate with the highest vote in the primary, and _Extremist Wins Primary_ is a binary indicator equaling $1$ if that margin exceeds $0$, and $\epsilon_{dpt}$ is an error term. 
When the extremist margin exceeds $0$, the candidate representing case $dpt$ is the extremist, otherwise the candidate representing $dpt$ is the moderate.
The coefficient $\beta_{1}$ represents the intercept shift associated with the extremist primary win, estimating the treatment effect of candidate extremism  at the discontinuity.
I replicate this LPM using ordinary least squares, and I also create a Bayesian equivalent using an algebraically reparameterization.
The Bayesian parameterization has the same linear form, but instead of specifying two lines an interaction term, I subscript the coefficients by $w$, which indexes the treatment status (_Extremist Wins Primary_),
\begin{align}
\begin{split}
  y_{dpt} &= \alpha_{w[dpt]} + \beta_{w[dpt]}(\text{Extremist Primary Margin})_{dpt} + \epsilon_{dpt} \\
  \epsilon_{dpt} &\sim \mathrm{Normal}\left( 0, \sigma \right)
\end{split}
(\#eq:bayes-rd)
\end{align}
where $\alpha_{w}$ is an intercept for treatment status $w$, and $\beta_{w}$ is the slope for treatment $w$.
This parameterization implies two lines, one line for $w = 0$ and another line for $w = 1$.
The treatment effect at the discontinuity is the difference between the intercepts, $\alpha_{1} - \alpha_{0}$.
This parameterization will be helpful for extending the model below.

I plot the OLS win probability estimates around the discontinuity in Figure \@ref(fig:plot-rd).
At the discontinuity, we estimate that extremism decreases a candidate's win probability by `r coef(ols_lmfit)["treat"] %>% abs() %>% round(2)` percentage points, which is the same effect found by @hall:2015:extremists.
The original publication lacks a graphical depiction of these results.
Our visualization of the RD predictions reveal that the confidence set for the parameter estimates that compose the treatment effect contain many values that would be impossible to observe.
The point estimate for average moderate candidate win probability at the discontinuity is `r coef(ols_lmfit)["(Intercept)"] %>% round(2)`, which is a possible number to obtain, but the 95 percent confidence intervals includes values as high as `r confint(ols_lmfit)["(Intercept)", "97.5 %"] %>% round(2)`, which far exceeds the maximum possible value of $1.0$.

```{r plot-rd, include = TRUE, fig.width = 7, fig.height = 6, fig.cap = "OLS estimates of the predicted probability that a candidate wins the general election. Local average treatment effect is estimated at the threshold. Treatment effect is defined by parameter estimates whose confidence sets contain values with no possibility of being true."}
```

This pathology is possible in any LPM with finite data, but there are a few pragmatic reasons why we might not worry about it.
First, for fully saturated model specifications, predicated probabilities from a model LPM are unbiased estimates of the true probabilities, and thus are an unbiased estimate of the treatment effect of interest.
<!------- TO DO ---------
- cite
------------------------->
For frequentist inference, constructing a 95 percent confidence interval on this unbiased estimate might be enough to suit the researcher's needs.
In this particular case, however, these reasons may not satisfy our goals.
First, the model isn't fully saturated.
Because the design employs a local linear regression, the extrapolation of the regression function to the threshold is model-dependent [@calonico-et-al:2014:rdd-CIs].
It makes sense, then, to build a model that constrains those extrapolations only to regions of parameter space that are mathematically possible for the problem at hand. 
Furthermore, the repeated sampling intuition of the frequentist approach does not guide our inferences because the data in the analysis are the population of interest.
We have no ability to repeatedly sample this data generating process, so our uncertainty about our inferences must come from some other mechanism.
Most importantly, because the intercept estimates are essential for defining the treatment effect of interest, the degree to which this one estimate is corrupted presents a significant problem for the inferences we can draw from the analysis.

To visualize just how much posterior probability this model places in impossible regions of parameter space, Figure \@ref(fig:plot-flat-effect) shows a histogram of posterior samples for the treatment effect from the Bayesian version of this model using (improper) flat priors on all parameters.
Because flat priors do nothing to concentrate prior probability density away from pathological regions of parameter space, a large proportion of posterior samples contain intercept estimates that do not and cannot represent win probabilities.
Of the `r (parallel::detectCores() * (rd_iter - rd_warmup)) %>% scales::comma()` posterior samples considered by this model fit, `r 
  const_draws %>%
  filter(.variable == "Non-Extremist Intercept") %$% 
  mean(invalid, na.rm = TRUE) %>% 
  scales::percent(accuracy = 1)
`
of the non-extremist intercepts are "impossible" to obtain because they are greater than $1$ or less than $0$.
A small number of MCMC samples for the extremist intercepts take impossible values as well. 
As a result, just `r effect_draws %$% mean(invalid == FALSE) %>% scales::percent(accuracy = 1)` of MCMC samples for the treatment effect is composed of parameters that are mathematically possible.
Even invoking the practical benefits of the LPM, such a high level of corruption in the most important quantity of this analysis is cause to rethink the approach.

```{r plot-flat-effect, include = TRUE, fig.width = 7, fig.height = 5, fig.cap = 'Histogram of posterior samples for the treatment effect. Using flat priors for each win probability intercept, a substantial share of the treatment distribution consists of parameters that cannot possibly occur.'}
```

The Bayesian approach begins with structural prior information about the intercepts estimated at the discontinuity.
In particular, we specify a prior that these constants can only take values in the interval $[0, 1]$. 
We remain agnostic as to which values within that interval are more plausible in the prior.
The result is a uniform prior over possible win probabilities, which we apply to both intercept parameters.
\begin{align}
  \alpha_{w = 0}, \alpha_{w = 1} &\sim \mathrm{Uniform}\left(0, 1\right)
  (\#eq:const-uniform-prior)
\end{align}
The structural information in this prior is indisputable.
We know with certainty that no probability can be less than $0$ or greater than $1$.
Accordingly, this prior concentrates probability density away from treatment effects that cannot be true, while maintaining the local linear specification that is justified by the limiting intuition of the key identification assumption.
Because we give flat priors to the individual intercepts rather than the treatment effect itself, the implied prior for the treatment effect inherits the triangular shape introduced above in Figure \@ref(fig:plot-diff-means-example), which is vague despite not being flat. 

We complete the model by specifying distributions for the outcome data and the remaining parameters. 
\begin{align}
\begin{split}
  y_{dpt} &\sim \mathrm{Normal}\left(\alpha_{w[dpt]} + \beta_{w[dpt]}(\text{Extremist Primary Margin})_{dpt}, \sigma \right) \\
  \mathrm{\beta_{w}} &\sim \mathrm{Normal}\left(0, 10\right) \\ 
  \sigma &\sim \mathrm{Uniform}(0, 10)
\end{split}
(\#eq:constrained-aux-priors)
\end{align}
The Normal model for the outcome data in the first line is equivalent to the Normal error term defined in \@ref(eq:bayes-rd).
The priors for the $\beta_{w}$ slopes and residual standard deviation $\sigma$ are very diffuse given the scale of the outcome data, $\{0, 1\}$, and the running variable that only takes values in the interval $[`r -100 * bandwidth_margin`, `r 100 * bandwidth_margin`]$, a bandwidth of $\pm 5$ percentage points around the threshold.
<!------- TO DO ---------
- PPC
------------------------->

A possible retort to this model setup is a Bayesian approach would be entirely unnecessary if instead we employed a binary outcome model like logit or probit regression.
These models are typically used to estimate probabilities underlying binary data in other contexts, so we entertain it here as well.
Although this model contradicts the limiting intuition that the regression function is instantaneously linear at the discontinuity (as any function is instantaneously linear for an infinitesimal change in its input), I indulge this possible retort by building a Bayesian logit specification as well. 
This setup considers the binary election result as a Bernoulli variable with a probability parameter specified by a logit model,
\begin{align}
  y_{dpt} &\sim \mathrm{Bernoulli}\left(\pi_{dpt}\right) \\
  \mathrm{logit}\left(\pi_{dpt}\right) 
  &= \alpha^{*}_{w[dpt]} + \beta^{*}_{w[dpt]}(\text{Extremist Primary Margin})_{dpt}
  (\#eq:rd-logit-likelihood)
\end{align}
with parameters denoted $\alpha_{w}^{*}$ and $\beta_{w}^{*}$ to distinguish them from the $\alpha_{w}$ and $\beta_{w}$ parameters in the linear setup.

```{r sim-logit-prior}
logit_prior_samples <- 
  tibble(
    `Logistic(0, 1)` = rlogis(n = 200000),
    `Normal(0, 10)` = rnorm(n = 200000, sd = 10)
  ) %>%
  print()


p_logit_scale <- 
  logit_prior_samples %>%
  pivot_longer(cols = everything()) %>%
  ggplot() +
  aes(x = value, fill = name) +
  geom_histogram(
    boundary = 0,
    binwidth = .05,
    alpha = 0.5,
    position = "identity"
  ) +
  labs(
    x = TeX("Constant on logit scale: $\\alpha^{*}_{w}$"),
    y = NULL
  ) +
  scale_fill_manual(values = c("Normal(0, 10)" = tertiary, "Logistic(0, 1)" = secondary)) +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    panel.grid.major = element_blank()
  ) +
  coord_cartesian(xlim = c(-6, 6)) +
  annotate(
    geom = "text", label = "Logistic(0, 1)",
    x = -4, y = 1700
  ) +
  annotate(
    geom = "text", label = "N(0, 10)",
    x = 5, y = 500
  )

p_prob_scale <- 
  logit_prior_samples %>%
  mutate_all(plogis) %>%
  pivot_longer(cols = everything()) %>%
  ggplot() +
  aes(x = value, fill = name) +
  geom_histogram(
    boundary = 0,
    binwidth = .01,
    alpha = 0.5,
    position = "identity"
  ) +
  labs(
    x = TeX("Implied win probability: $invlogit(\\alpha^{*}_{w})$"),
    y = NULL
  ) +
  coord_cartesian(ylim = c(0, 10000)) +
  scale_fill_manual(values = c(secondary, tertiary)) +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    panel.grid.major = element_blank()
  ) +
  annotate(
    geom = "text", label = "Logistic(0, 1)",
    x = 0.4, y = 3000
  ) +
  annotate(
    geom = "text", label = "N(0, 10)",
    x = .85, y = 8500
  )
```

```{r plot-logit-priors}
p_logit_scale + p_prob_scale +
  plot_annotation(
    title = "Logit Priors and Implied Probabilities",
    subtitle = TeX("Prior samples for logit scale RDD constant $\\alpha^{*}_{w}$")
  )
```

Although this logit specification constrains all win probability estimates to fall in the appropriate region, specifying priors for logit models is more challenging because regression parameters are defined on the log-odds scale instead of the probability scale.
Fortunately for the case of regression discontinuity, the treatment effect is defined at the threshold where the running variable is $0$, so our prior for the treatment effect can be constructed in a region of parameter space where the running variable and its coefficients have dropped from the equation.
\begin{align}
\begin{split}
  \mathrm{logit}\left(\pi_{dpt}\right) 
  &= \alpha^{*}_{w[dpt]} \text{, at Extremist Primary Margin}_{dpt} = 0 \\
  \text{which implies } \pi_{dpt} &= \mathrm{logit}^{-1}\left(\alpha_{w[dpt]}\right) 
\end{split}
(\#eq:rd-logit-x0)
\end{align}
If we want to construct a prior for the treatment effect that is similar to the structural information we encoded in the linear specification, we must specify priors for the extremist and non-extremist win probabilities that are flat over valid probability values at the discontinuity.
This requires a prior for $\alpha^{*}_{w}$ on the log-odds scale that implies a flat prior for $\mathrm{logit}^{-1}\left(\alpha_{w}^{*}\right)$ on the probability scale.
To solve this problem, we leverage the logit model's connection to the standard Logistic distribution.
The logit function maps values in the $(0, 1)$ interval to any real number, and the inverse logit function maps any real number to $(0, 1)$.
We accomplish a flat prior for win probabilities at the threshold using a a standard Logistic prior on the log-odds scale,
\begin{align}
  \alpha_{w}^{*} &\sim \mathrm{Logistic}\left(0, 1\right)
  (\#eq:logit-prior)
\end{align}
which becomes a flat density for $\mathrm{logit}^{-1}\left(\alpha_{w}^{*}\right)$. 
It is startling at first to consider a prior as narrow as $\mathrm{Logistic}\left(0, 1\right)$ as an uninformative prior for a key parameter. 
But at discussed in Section \@ref(sec:prior-flatness), the connection between prior vagueness and prior flatness is not absolute.
Flatness is only a shape. The relationship between flatness and informativeness depends on model parameterization and the scale of the data.

Figure \@ref(fig:plot-logit-priors) visualizes how the Logistic prior for the intercept on the log-odds scale becomes a flat prior on the probability scale at the threshold.
The left panel shows a histogram of $\mathrm{Logistic}\left(0, 1\right)$ simulations, and the right panel shows a histogram of the same values after they are converted to probabilities using the inverse logit function. 
For comparison, I also simulate a $\mathrm{Normal}\left(0, 10\right)$ prior, which is something a researcher might pick if they wanted to be vague on the log-odds scale. 
Converting the wide Normal prior to the probability scale, however, shows that greater prior density on logit values far from zero translates to greater prior density over probability values very close to $0$ and $1$.

The fact that the wide Normal prior has strange behavior on the probability scale does not mean that it shouldn't be used in Bayesian logistic modeling.
It could be an appropriate choice for specifying priors for constructs that should be understood directly on the logit scale.
For instance, I give this exact prior to the slope parameters in this RDD logit model, 
\begin{align}
  \beta_{w} &\sim \mathrm{Normal}\left(0, 10\right)
\end{align}
because I want the prior to consider a broader distribution slopes _on the logit-scale_.
The lesson with these priors, as with any prior, is that prior distributions should be chosen to suit the modeling context.
Elements of that context include link functions, model reparameterization, the scaling of outcome data or covariates, regularization concerns, and so on.
Choosing "default priors" that always encode flatness on one scale has no guaranteed behavior for implies priors for important functions of parameters.

```{r plot-logit-priors, include = TRUE, fig.width = 9, fig.height = 4.5, out.width = "100%", fig.cap = 'Scale invariance of logit model priors. Standard logistic prior on logit scale becomes a flat prior on the probability scale. "Diffuse" priors on logit scale imply priors on probability scale that bias toward extreme probabilities.'}
```


```{r tidy-rdd}
tidy_rdd <- 
  list(
    "Bayes LPM: flat prior" = brm_flat, 
    "Bayes LPM: structural prior" = brm_constrained, 
    "Bayes logit: structural prior" = brm_logit
  ) %>%
  lapply(tidy, conf.int = TRUE, lp = FALSE) %>%
  bind_rows(.id = "model") %>%
  bind_rows(ols_default) %>%
  filter(term %in% c("p_control", "p_treat", "trt_effect")) %>%
  mutate(
    model = fct_relevel(
      model, "Bayes LPM: flat prior", "Bayes LPM: structural prior"
    )
  ) %>%
  print()


draws_rdd <- 
  list(
      'Bayes LPM: flat prior' = brm_flat, 
      "Bayes LPM: structural prior" = brm_constrained, 
      "Bayes logit: structural prior" = brm_logit
    ) %>%
  lapply(gather_draws, p_control, p_treat, trt_effect) %>%
  bind_rows(.id = "model") %>%
  mutate(
    model = fct_relevel(
      model, "Bayes LPM: flat prior", "Bayes LPM: structural prior"
    )
  ) %>%
  print()
```

```{r plot-rdd-results}
rd_hist <- draws_rdd %>%
  filter(.variable == "p_control") %>%
  ggplot() +
  aes(x = .value, y = fct_rev(model), fill = model) +
  # facet_wrap(. ~ .variable) +
  ggridges::geom_ridgeline(
    stat = "binline", 
    draw_baseline = FALSE,
    scale = 0.2,
    binwidth = .01, boundary = 1,
    position = "identity", alpha = 0.7
  ) +
  geom_vline(xintercept = 1) +
  theme(legend.position = "none") +
  labs(x = "Non-Extremist Win Probability", y = NULL) +
  scale_fill_manual(
    values = c(
      "Bayes LPM: flat prior" = tertiary,
      "Bayes LPM: structural prior" = primary,
      "Bayes logit: structural prior" = secondary
    )
  ) +
  scale_x_continuous(
    limits = c(0, 1.25),
    breaks = seq(0, 1, .2)
  ) 

rd_pr <- tidy_rdd %>%
  filter(term == "trt_effect") %>%
  filter(model != "OLS") %>%
  ggplot() +
  aes(x = fct_rev(model), y = estimate, color = model) +
  geom_pointrange(
    aes(ymin = lower, ymax = upper),
    position = position_dodge(width = -0.25),
    size = 0.75
  ) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  theme(
    axis.text.y = element_blank(),
    legend.position = "none"
  ) +
  labs(x = NULL, y = "LATE: Extremism Effect at Threshold") +
  scale_y_continuous(
    limits = c(-1, .25),
    breaks = seq(-1, .25, .25)
  ) +
  scale_color_manual(
    values = c(
      "Bayes LPM: flat prior" = "black",
      "Bayes LPM: structural prior" = primary,
      "Bayes logit: structural prior" = secondary
    )
  ) +
  geom_text(
    aes(label = str_glue("{round(estimate, 2)} (sd = {round(std.error, 2)})")),
    hjust = 0.25,
    vjust = -1,
    color = "black"
  )

rd_hist + rd_pr +
  plot_annotation(
      title = "Results of Bayesian Regression Discontinuity",
      subtitle = "How weakly informative priors affect inferences"
    )
```


<!------- TO DO ---------
- estimation/stan details?
------------------------->

These prior interventions in both the Bayesian LPM and the Bayesian logit are minor.
They merely encode structural information about the outcome scale.
Win probabilities for extremists and non-extremists are constrained to take valid values—between $0$ and $1$—but the prior is otherwise agnostic about which win probabilities are more likely than others before seeing any data. 
What effect do these minor interventions have? 
Figure \@ref(fig:plot-rdd-results) plots the results from these three Bayesian models: the problematic original model with improper flat priors, the Bayesian LPM with structural priors to constrain the intercepts, and the logit model that creates the structural prior using the transformed Logistic distribution.
The left panel shows a histogram of posterior MCMC samples for the non-extremist win probability at the threshold.
The LPM at the top of the panel included no parameter constraints whatsoever.
As a result, we see the pathological behavior where the posterior distribution places positive density on win probabilities that we know with certainty to be impossible to obtain.
The histograms in the second and third rows show the LPM and logit with the structural prior.
Both models concentrate prior density on possible win probabilities only, resulting in posterior distributions that reflect prior information better than the unconstrained model.
The posterior distributions are asymmetric and place a lot of posterior density at high win probabilities, but this should not alarm us.
The asymmetry in the distribution reflects the signals obtained from the data, rationalized against weak information encoded in the prior. 
The asymmetry is direct indicator of the way Bayesian priors added value to the analysis.

The right panel of Figure \@ref(fig:plot-rdd-results) shows how these parameter constrains ultimately manifest in our LATE estimates by plotting posterior means and $90$ percent compatibility intervals for each model.
As with nearly all Bayesian modeling approaches, our priors have the effect of shrinking important effects toward $0$ and reducing the variance of the effect.
In this particular case, the posterior mean for the local average treatment effect shrinks from 
`r 
(flat_mean <- tidy_rdd %>% 
  filter(term == "trt_effect" & model == "Bayes LPM: flat prior") %>% 
  pull(estimate) %>% 
  round(2))
` 
with flat/unconstrained priors to 
`r 
(lpm_mean <- tidy_rdd %>% 
  filter(term == "trt_effect" & model == "Bayes LPM: structural prior") %>% 
  pull(estimate) %>% 
  round(2))
`
using the LPM with constrained intercepts: a 
`r 
  (-1*(abs(lpm_mean) - abs(flat_mean)) / abs(flat_mean)) %>%
  scales::percent(accuracy = 1)
`
reduction in the magnitude of the effect.
The LATE from the Bayesian logit is 
`r 
(logit_mean <- tidy_rdd %>% 
  filter(term == "trt_effect" & model == "Bayes Logit: structural prior") %>% 
  pull(estimate) %>% 
  round(2))
`, 
which is a 
`r 
  (-1*(abs(logit_mean) - abs(flat_mean)) / abs(flat_mean)) %>%
  scales::percent(accuracy = 1)
`
reduction in magnitude. 
This shrinkage comes from the fact that some of the largest treatment effects in our original posterior distribution were composed of impossible parameters.
This manifested earlier in Figure \@ref(fig:plot-flat-effect), which showed that larger treatment effects were more likely to contain pathological parameters than the smaller treatment effects.
The standard deviation of the posterior samples is reduced for the models with structural prior constraints, so these prior interventions are also improving the precision of our estimates.
This is because a fair amount of posterior uncertainty in the unconstrained model was owed to impossible parameter values.

```{r plot-rdd-results, include = TRUE, fig.width = 9, fig.height = 5, out.width = "100%", fig.cap = "Comparison of posterior samples from Bayesian models with improper flat priors and weakly informative priors. Weakly informative models have flat priors over valid win probabilities at the discontinuity. Priors reduce effect magnitudes and variances by ruling out impossible win probabilities."}
```

It bears emphasizing that the prior interventions in this case study were no more controversial than declaring what is already known: probabilities lie between $0$ and $1$.
Since many causal research designs estimate treatment effects on binary variables, and many causal research designs are limited to small numbers of relevant real-world observations or budget-limited experimental samples, simple interventions like this have the potential to substantially improve the precision of research findings in contexts where researchers do not realize how much information they are leaving out of their analyses.


### Models for Nonparametric Treatment Effects with Applications to Meta-Analysis {#sec:meta-analysis}





## Other Frontiers of Bayesian Causal Inference 



### Beyond Estimation: Inferences About Models and Hypotheses





We can think bigger about Bayesian _inference_ for a parameter as distinct from Bayesian _estimation_ of the in-sample quantity. 
This lets us use a nonparametric data-driven estimator for the data, but the "inference" or "generalization" still has a prior.
For instance a sample mean estimates a population mean without a likelihood model for the data, but inference about the population mean often follows a parametric assumption from the Central Limit Theorem that the sampling distribution from the mean is asymptotically normal (but doesn't have to, c.f. bootstrapping).
<!------- TO DO ---------
- Read more about sharp null and an RI framework for inference, 
  since this runs up against the idea of asymptotic normality of the means.
------------------------->
Even if the point estimator we use for a mean is unbiased, we can assimilate external information during the interpretation of the estimator (biasing the inference without biasing the point estimator).
Restated: the posterior distribution is a weighted average of the raw point estimator and external information, rather than biasing the data-driven estimate directly.


Even bigger: Bayesian inference about _models_ (Baldi and Shahbaba 2019). 
This is probably where I have to start my justification for this? 
The _entire point_ of causal inference is to make inferences about counterfactuals given data (Rubin 1978?).
Invoking Bayesian inference is really the only way to say what we _want_ to say about causal effects: what are the plausible causal effects given the model/data. We do _not_ care about the plausibility of data given the null (as a primary QOI). 
- Probably want to use Harrell-esque language? Draw on intuition from clinical research, or even industry. We want our best answer, not a philosophically indirect weird jumble.
- This probably also plays into the Cox/Jeffreys/Jaynes stuff I have open on my computer.
<!------- TO DO ---------
- Corey Yanofsky recommended reading
- Gelman/Shalizi philosophy of Bayes
- Induction/Deduction in Bayes
- etc.
- symposium about "objective" Bayes
- And then also Gelman/Simpson/Betancourt "context of the likelihood"
  => we have priors about the WORLD, not about model parameters.
------------------------->





This presumes an m-closed world(?right?), which maybe we do not like (Navarro, "Devil and Deep Blue Sea").
<!------- TO DO ---------
- Read Navarro CLOSELY!
- Read ARONOW and MILLER: how to square "intra-model agonisticism" with 
  some philosophical view of what Bayes is.
- Maybe this is terminology I should work on! Draw some helpful lines that 
  do not already exist for defining subtypes of agnosticism or other
  epistemic bundles of things.
  (there is also some paper on "types of Bayesians"?
   mentioned in a McElreath talk)
------------------------->
Me debating with myself: how to think about Bayesian model selection vs "doubly robust" estimation ideas...







Inherit material from earlier section (baldi paper)

Quasi-experiment paper (LR/BF of two models)


Conventional:
\begin{align}
  p(\theta \mid \mathbf{y}) &= \frac{p(\mathbf{y} \mid \theta)p(\theta)}{p(\mathbf{y})} \\
  p(\theta \mid \mathbf{y}) &\propto p(\mathbf{y} \mid \theta)p(\theta)
\end{align}

Implied:
\begin{align}
  p(\theta \mid \mathbf{y}, \mathcal{H}) &= \frac{p(\mathbf{y} \mid \theta, \mathcal{H})p(\theta \mid \mathcal{H})}{p(\mathbf{y} \mid \mathcal{H})}
\end{align}


### Priors are the Basis for all Generalization

No-Free-Lunch theorems



### Agnostic Causal Inference

<!-- ### Modeling Cultures in Political Science: Complexity and Agnosticism -->

Sidestepping priors

complexity of bayes vs. parsimony of causal inference NOT A RULE

Causal doesn't imply nonparametric, Bayesian doesn't imply complex

At any rate: 

- simple case: sensitivity testing for noisy circumstances
- complex case: stabilizing highly parameterized problems
    - dynamic TCSC models, lots of parameters
    - that hierarchical conjoint thing
    - priors in high dimensions are scary: consider parameterizations and do simulations




- For nonparametric estimators, structural priors can be helpful for concentrating probability mass in sensible areas of space, since nonparametric estimators may be lower-powered than estimators that get a power boost from parametric assumptions.
- Parametric models, in term, are obvious areas where Bayesian estimates can go, but they are stacking more assumptions on top of the parametric assumptions.
- Semi-parametric models are an interesting middle ground, where we want to be flexible about the exact nature of the underlying relationships, but we want to impose some stabilization to prevent the model from behaving like crazy.









