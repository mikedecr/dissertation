# Modeling the Constituency's Policy Preferences {#ch:model}

<!-- bold math symbol -->
```{block, include = knitr::is_html_output(), cache = FALSE}
$\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}$
```



```{r knitr-02-model, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

```{r r-02-model, cache = FALSE}
library("knitr")
library("here")
library("magrittr")
library("tidyverse")
library("extrafont")
library("latex2exp")
library("scales")
library("patchwork")
```

To study how partisan constituencies are represented in primary elections, we require a measure of the partisan constituency's policy preferences. This chapter presents the statistical model that I use to estimate the policy ideal points of district-party publics. 
<!------- TO DO ---------
- first references
- what is "constituency?" voters? just partisans?
- What is "district-party"
------------------------->

This chapter proceeds in three major steps. First, I review the theoretical basis for ideal point models, which can be traced to spatial models of policy choice from classic formal theory work in American political science such as @downs:1957:economic-theory. I connect these formal models to statistical models of policy ideal points [in a style that follows @clinton-jackman-rivers:2004:ideal] as well as their connection to Item Response Theory (IRT) models from psychometrics and education testing [e.g. @fox:2010:bayesian-IRT].

Second, I specify and test the group-level model that I build and employ in my analysis of district-party publics. This discussion includes details that are relevant to Bayesian estimation, including identification restrictions on the latent policy space, specification of prior distributions, and model parameterizations that expedite estimation with Markov chain Monte Carlo (MCMC). I begin with a static model for one time period, and then I describe a dynamic model that smooths estimates across time using hierarchical priors for model parameters [@caughey-warshaw:2015:DGIRT]. I test both the static and the dynamic models by fitting them to simulated data and determining how well they recover known parameter values.
<!------- TO DO ---------
- Prior checking
------------------------->

Lastly, I describe how I fit the model to real data. This section describes data collection, data processing, and model performance, and a descriptive analysis of the estimates.


<!------- TO DO ---------
- Should this include a comparison to CDW measures? Hill measures?
------------------------->



## Spatial Models and Ideal Points

<!-- goal: spatial >> IRT >> Parameterizations -->

Ideal points are constructs from *spatial* models of political choice. These models exist under formal theory---they simplify scenarios in the political world into sets of actors whose behaviors obey utility functions that conform to mathematical assumptions. Spatial models invoke a concept of "policy space," where actor preferences and potential policy outcomes are represented as locations along a number line. A canonical example is a left-right continuum, where progressive or "liberal" policies occupy locations on the left side of the continuum, while conservative policies are on the right side. Actors are at least partially motivated by their policy preferences, so they strive to achieve policy outcomes that are closest to their own locations. Depending on the structure of the game, these actors often face constrained choices; they can't achieve their most-preferred policy, so they settle on something that is as close as they can get.

```{r spatial-data}
dd <- tibble(
  Left = -1,
  Right = 1,
  Actor = 0.3
) 

long_dd <- dd %>%
  gather(key = label, value = location) %>%
  mutate(height = 0) %>%  
  print()

x_min <- -2
x_max <- 2
u_scale <- 2

utility <- tibble(
  x = seq(x_min, x_max, .01),
  utility_loss = u_scale * -(x - dd$Actor)^2,
  Right = u_scale * -(x - dd$Right)^2,
  Left = u_scale * -(x - dd$Left)^2
) %>%
  print(n = nrow(.))

```


```{r plot-space-1, out.width = "100%", fig.height = 2, fig.width = 8, include = TRUE, fig.cap = "An Actor and two policy outcomes (Left and Right) represented as locations in ideological/policy space"}
utility_plot <- ggplot(long_dd) +
  aes(x = location, y = height) +
  geom_point() +
  theme_void() +
  annotate(geom = "line", x = c(-2, 2), y = 0) +
  geom_text(aes(label = label), nudge_y = 1) +
  coord_cartesian(ylim = c(-3, 3)) 

utility_plot
```

Figure&nbsp;\@ref(fig:plot-space-1) plots a simple example of an Actor's choice over two policies in one-dimensional policy space. The "Left" outcome is a more progressive policy outcome than the "Right" outcome, indicated by their locations on the line. The Actor has a location herself, which corresponds to her most-desired policy outcome. There is no policy located exactly at the Actor's preferred location, but the Actor is closer to the Right policy than to the Left. Supposing that the Actor could make an error-free choice over which policy to implement, it appears she would prefer the Right outcome to the Left outcome.

Formal models are more careful to specify the assumptions governing these scenarios, which can be complicated in many cases. For example, suppose that locations along the left-right continuum can be assigned values on the real number line. Figure&nbsp;\@ref(fig:plot-space-1) shows a one-dimensional number line, but policies can be generally represented as locations in multidimensional $\mathbb{R}^{d}$ space. The Actor's location is synonymous with her most preferred policy: her "ideal point." This is the point where the Actor's utility, in an economic utility model, is maximized with respect to policy considerations. Utility implies that the Actor has a utility function that is defined over the policy space, which depends on the distance between her ideal point and a potential policy outcome. Outcomes nearer to the Actor's ideal point are generally more preferred than farther outcomes, but this too is subject to assumptions about the shape of the Actor's utility function. Typically utility functions are assumed to be single-peaked and symmetric around an Actor's ideal point, so a closer policy is always more preferred, all else equal. The notion of an ideal point is similar to a "bliss point" in microeconomics: the optimal quantity of a good consumed such that any more or less consumption would result in decreased utility. Whether an Actor can choose the closest policy to herself depends on the structure of the game: the presence and strategy profiles of other Actors, the sequence of play, and the presence of other non-policy features of Actors' utility functions.

<!-- to do: $d$ cite -->
<!-- canonical cite for the or the notion of policy dimensions? -->

Formal models of ideal points are distinct from statistical models of ideal points. Formal models are primarily theoretical exercises; they explore the incentives and likely actions of Actors in specific choice contexts, building theoretical intuitions that can be applied in the study of real-world politics with real data. Statistical models, on the other hand, explicitly or implicitly *assume* a formal model as given and estimate its parameters using data. Data could come from legislators casting voting on bills, judges ruling on case outcomes, survey respondents stating their policy preferences (as in this project), and other situations. Researchers are typically interested in parameter estimates for the Actors' ideal points, although sometimes the parameters about the policy alternatives are substantively interesting.
<!------- TO DO ---------
- when are item params interesting  
------------------------->

<!------- TO DO ---------
- describe where ideal point â‰  ideology?
- ideal point is an operationalization of a nebulous, potentially high-dimensional concept called ideology?
- latent construct: none of the parameters are known, we have identifiability issues, and the model is still assumed!!
- this really is model-driven, we are assuming a model
------------------------->

Having distinguished formal and statistical models, I now show a derivation of a statistical model from a formal model. This exercise model will serve as a theoretical basis for the class of statistical models explored in this dissertation. I begin with notation to describe an arbitrary number of actors indexed $i \in \{1, \ldots, n\}$ making an arbitrary number of policy choices (bills, survey items, etc.) indexed $j \in \{1, \ldots, J\}$. Every Actor has an ideal point, or a location in the policy space, represented by $\theta_{i}$. Every task is choice between a Left policy located at $L_{j}$ and a Right policy located at $R_{j}$. 

The utility that an Actor receives from a Left or Right choice is a function of the distance between her ideal point and the respective choice location. Utility is maximized if an Actor can choose a policy located exactly on her ideal point, and utility is "lost" for choices farther and farther from her ideal point. The functional form of utility loss is an assumption made by the researcher. Some scholars assume that utility loss follows a Gaussian curve, while others choose a quadratic utility loss [@clinton-jackman-rivers:2004:ideal]. For this analysis, I assume a quadratic utility loss.^[
  Researchers typically avoid linear losses for technical reasons: a linear utility loss function is non-differentiable at the ideal point because function comes to a point. This prevents the researcher from using differential calculus to find a point of maximum utility.
]
<!------- TO DO ---------
- differentiable cite?
------------------------->

The choice of quadratic loss implies a utility function over the *squared distance* between an Actor and a choice location. The utility Actor $i$ receives from choosing Left or Right are given by utility functions $U_{i}\left(L_{j}\right)$ and $U_{i}\left(R_{j}\right)$, respectively. With quadratic utility loss, these utility functions take the form
\begin{align}
  \begin{split}
    U_{i}\left(R_{j}\right) = - \left( \theta_{i} - R_{j}\right)^{2} + u^{\mathtt{R}}_{ij} \\ 
    U_{i}\left(L_{j}\right) = - \left( \theta_{i} - L_{j}\right)^{2} + u^{\mathtt{L}}_{ij},
  \end{split}
  (\#eq:utility-fns)
\end{align}
where $u^{\mathtt{R}}_{ij}$ and $u^{\mathtt{L}}_{ij}$ are the idiosyncratic error terms for the Right and Left alternatives, respectively. I sometimes refer to the quadratic utility loss as the "deterministic" component of the Actor's utility function, while the idiosyncratic error terms are "stochastic" components.

```{r plot-utility}
utility_plot +
  geom_line(
    data = utility,
    aes(x = x, y = utility_loss)
  ) +
  theme(legend.position = "none") +
  annotate(geom = "linerange",
    x = dd$Right, ymax = 0, ymin = -u_scale*(dd$Actor - dd$Right)^2,
    linetype = "dashed"
  ) +
  annotate(geom = "linerange",
    x = dd$Left, ymax = 0, ymin = -u_scale*(dd$Actor - dd$Left)^2,
    linetype = "dashed"
  ) +
  geom_point() +
  NULL
```

<!-- break -->

With these utility functions laid out, Actor $i$'s decision can be a comparison of the utilities received by choosing Right or Left. Let $y_{ij}$ indicate the Actor's choice of Right or Left, where Right is coded $1$, and Left is coded $0$. The model so far implies that $y_{ij} = 1$ (Actor chooses Right) if their utility is greater for Right than for Left.
\begin{align}
  y_{ij} = 1 &\iff U_{i}\left(R_{j}\right) > U_{i}\left(L_{j}\right)
  (\#eq:choice-iff)
\end{align}
To visualize this choice, I represent the deterministic components of Equation&nbsp;\@ref(eq:choice-iff) in Figure&nbsp;\@ref(fig:plot-utility), omitting the stochastic utility terms. The parabola represents $i$'s fixed utility loss for any choice along the ideological continuum, owed to her distance from that choice. The vertex of the parabola is at the Actor's location, indicating that she would maximize her spatial utility if she could choose a policy located exactly at her ideal point. Dashed lines below the Left and Right alternatives represent the utility loss owed to the Actor's distance from those specific choices. In the current example, the Actor is closer to Right than to Left, so she receives greater utility (or, less utility *loss*) by choosing Right instead of Left.

```{r plot-utility, include = TRUE, fig.height = 2, fig.width = 8, out.width = "100%", fig.cap = "A representation of quadratic utility loss over policy choices"}
```

It is important to remember that Figure&nbsp;\@ref(fig:plot-utility) shows only the deterministic component of choice task $j$; random error components $u^{\mathtt{R}}_{ij}$ and $u^{\mathtt{L}}_{ij}$ are omitted. With idiosyncratic utility error incorporated, Equation&nbsp;\@ref(eq:choice-iff) implies that even though the Actor's distance to Right is smaller than her distance to Left, there remains a nonzero probability that $i$ chooses Left. This probability depends on the instantiated values of the idiosyncratic error terms for each choice. These error terms represent the accumulation of several possible, non-ideological shocks to utility: systematic decision factors that are not summarized by ideology, issue-specific considerations that do not apply broadly across all issues, random misperceptions about the policy locations, and so on. Supposing that these idiosyncratic terms follow some probability distribution, Equation&nbsp;\@ref(eq:choice-iff) can be represented probabilistically:
\begin{align}
  \begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &= 
    \mathrm{Pr}\left(U_{i}\left(R_{j}\right) > U_{i}\left(L_{j}\right) \right) \\
  &=
    \mathrm{Pr}\left( 
      -\left( \theta_{i} - R_{j}\right)^{2} + u^{\mathtt{R}}_{ij} > 
      -\left( \theta_{i} - L_{j}\right)^{2} + u^{\mathtt{L}}_{ij}
    \right) \\
  &= \mathrm{Pr}\left(
      \left( \theta_{i} - L_{j}\right)^{2} -
            \left( \theta_{i} - R_{j}\right)^{2} 
        > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij}
      \right)
\end{split}
(\#eq:choice-prob)
\end{align}  
The intuition for Equation&nbsp;\@ref(eq:choice-prob) is that the Actor will choose the policy alternative that is nearest to her *unless* idiosyncratic or non-policy factors overcome her ideological considerations. Supposing that the Actor is closer to Right than to Left, $\left( \theta_{i} - L_{j}\right)^{2}$ will be greater than $\left( \theta_{i} - R_{j}\right)^{2}$, capturing the Actor's deterministic inclination to prefer Right over Left. The only way for $i$ to choose Left would be if the idiosyncratic utility of Left over Right exceeded the Actor's deterministic inclinations.
  <!-- to do: assumes? -->
<!-- Note that the model so far assumes only that utility loss is quadratic over the ideological space and that the "coefficient" on the quadratic loss is equal for the distances to both Left and Right.  -->

Equation&nbsp;\@ref(eq:choice-prob) can be rearranged to reveal an appealing functional form for $i$'s choice probability. First, expand the polynomial terms on the left side of the inequality...
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &= 
    \mathrm{Pr}\left(
           \left( \theta_{i} - L_{j}\right)^{2} -
           \left( \theta_{i} - R_{j}\right)^{2} 
           > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij} 
         \right) \\
  &= \mathrm{Pr}\left(
      \theta_{i}^{2} - 2\theta_{i}L_{j} + L_{j}^{2} 
      - \theta_{i}^{2} + 2\theta_{i}R_{j} - R_{j}^{2}
      > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij} 
    \right) \\
  &= \mathrm{Pr}\left(
      2\theta_{i}R_{j} - 2\theta_{i}L_{j} + 
      L_{j}^{2} - R_{j}^{2}
      > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij} 
    \right)
\end{split}
(\#eq:expand-utility)
\end{align}

From here, there are two factorizations that reveal convenient expressions for important constructs in the model.
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &=     
    \mathrm{Pr}\left(
      2\theta_{i}R_{j} - 2\theta_{i}L_{j} + 
      L_{j}^{2} - R_{j}^{2} 
      > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij}
    \right) \\
  &= 
    \mathrm{Pr}\left(
      2\theta_{i}R_{j} - 2\theta_{i}L_{j} + 
      \left(R_{j} - L_{j}\right)
      \left(R_{j} + L_{j}\right) 
      > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij} 
    \right) \\
  &= \mathrm{Pr}\left(
      2\left(R_{j} - L_{j}\right)
      \left(\theta_{i} - \frac{R_{j} + L_{j}}{2}\right)
      > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij} 
    \right)
\end{split}
(\#eq:factor-item-params)
\end{align}
The first manipulation is to decompose $L_{j}^{2} - R_{j}^{2}$ into the two factors $\left(R_{j} - L_{j}\right)\left(R_{j} + L_{j}\right)$. The second manipulation is to factor $2\left(R_{j} - L_{j}\right)$ out of the left-side of the inequality. We perform these manipulations because the resulting terms are appreciably more interpretable than before. First, note that $\frac{R_{j} + L_{j}}{2}$ is a formula for the midpoint between the Left and Right locations. This means that the expression $\theta_{i} - \frac{R_{j} + L_{j}}{2}$ intuitively conveys which policy alternative is closer to the Actor. If the Actor is closer to Right than to Left, $\theta_{i}$ will be greater than the midpoint, and vice versa if she were closer to Left. Second, the $2\left(R_{j} - L_{j}\right)$ term captures how far apart the policy alternatives are from one another, increasing as the distance between Right and Left increases. Together, the left side of the inequality succinctly describes the deterministic component of the Actor's ideological choice: is she closer to the Left or Right policy, and by how much?^[
  @ansolabehere-et-al:2001:candidate-positioning and @burden:2004:candidate-positioning use candidate midpoints as predictors in regression analyses to estimate the impact of candidate ideal points in House elections.
]

The final manipulation is to simplify the terms above, which results in a convenient parameterization for statistical estimation.
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) 
  &= 
    \mathrm{Pr}\left(
      2\left(R_{j} - L_{j}\right)
      \left(\theta_{i} - \frac{R_{j} + L_{j}}{2}\right)
      > u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij} 
    \right) \\
  &= \mathrm{Pr}\left(
      \iota_{j}\left(\theta_{i} - \kappa_{j}\right)
      > \epsilon_{ij}
    \right),
\end{split}
(\#eq:choice-2p)
\end{align}
This results in the "discrimination parameter" $\iota_{j} = 2\left(R_{j} - L_{j}\right)$, the "midpoint" or "cutpoint" parameter $\kappa_{j} = \dfrac{R_{j} + L_{j}}{2}$, and a joint error term $\epsilon_{ij} = u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij}$.^[
  The names for these parameters are adapted from item-response theory (IRT), an area of psychometrics that is similarly interested in inferring latent traits from observed response data. I discuss the connection between this model and the IRT model in the next section.
]
Parameterizing the model in this way expresses the utility comparison in a simpler, linear form. Similar to Equation&nbsp;\@ref(eq:factor-item-params) above, $\theta_{i} - \kappa_{j}$ shows how far the Actor is from the midpoint between Left and Right, and $\iota_{j}$ captures behaves as a "slope" on this distance: the distance from the midpoint has a _stronger influence_ when the policy alternatives are farther from one another, since more utility is lost over larger spatial distances. I explore the intuitions of this functional form more thoroughly in the following section.

A complete statistical model is obtained by making a parametric assumption for the distribution of $\epsilon_{ij}$. Assuming that $\epsilon_{ij}$ is a draw from a standard Normal distribution,^[
  This implies that $\mathrm{E}\left(u^{\mathtt{L}}_{ij}\right) = \mathrm{E}\left(u^{\mathtt{R}}_{ij}\right)$ and that $\mathrm{Var}\left(u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij}\right) = 1$. 
  For a given choice $j$, imposing a scale restriction on the error variance is not problematic because the ideological scale is latent and can be arbitrarily stretched. Any non-unit variance for item $j$ can be compensated for by scaling the discrimination parameter $\iota_{j}$ (i.e.\ multiplying both sides of the inequality established in Equation&nbsp;\@ref(eq:choice-2p) by some scale factor). The important assumption is that the error variance of a choice $j$ is equal _across individuals_: $s_{ij} = s_{j}$ for all $i$.
]
Equation&nbsp;\@ref(eq:choice-2p) implies a probit regression model for the probability that Actor $i$ chooses Right on choice $j$:
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &= 
  \mathrm{Pr}\left(
      \iota_{j}\left(\theta_{i} - \kappa_{j}\right)
      > \epsilon_{ij}
    \right) \\
  &= 
  \mathrm{Pr}\left(
    \iota_{j}\left(\theta_{i} - \kappa_{j}\right) - \epsilon_{ij} > 0 
  \right) \\
  &= 
  \Phi\left( \iota_{j}\left(\theta_{i} - \kappa_{j}\right) \right),
\end{split}
(\#eq:logit-irt)
\end{align}
where $\Phi(\cdot)$ is the cumulative Normal distribution function. Many IRT models assume that $\epsilon_{ij}$ follows a standard Logistic distribution, [for example @londregan:1999:ideal-pts], resulting in a logistic regression model rather than a probit model.^[
  A technical point of difference between the probit and logit model is the way parameters are scaled to yield the final line of Equation&nbsp;\@ref(eq:choice-2p). If it is assumed that $\epsilon_{ij}$ is a Logistic draw with scale $s_j$, this implies that 
  $\mathrm{Var}\left(u^{\mathtt{L}}_{ij} - u^{\mathtt{R}}_{ij}\right) = \dfrac{s_{j}^{2}\pi^{2}}{3}$, where it is assumed that $s_{j} = 1$ for the standard Logistic model. 
]
As I show below, the probit model facilitates the group-level model much more easily than the logit model.

<!------- TO DO ---------
- Does londregan prefer logit due to some identifiability concern???
- cite CW/Fox
------------------------->



### The "Item Response Theory" Approach to Survey Response

Scholars of ideal point models have noted their similarity to models developed under item response theory (IRT) in psychometrics [for example, @londregan:1999:ideal-pts]. IRT models have a similar mission as ideal point models: measuring latent features in the data given individuals' response patterns to various stimuli. The canonical psychometric example is in education testing, where a series of test questions is used to measure a student's latent academic "ability" level. This section connects ideal point models to IRT in order to explain their important theoretical and mathematical intuitions.

#### Latent Traits

The first important feature to note about IRT models is that they are *measurement models*. The goal of a measurement model is to use observed data $\mathbf{y}$ to estimate some construct of theoretical interest $\bm{\theta}$, supposing that there is a distinction between the two. The observed data $\mathbf{y}$ are affected by $\bm{\theta}$, but there is no guarantee of a one-to-one correspondence between the two because $\bm{\theta}$ is not directly observed. We can represent a measurement model with general notation $\mathbf{y} = f(\bm{\theta}, \bm{\sigma})$, where $\bm{\sigma}$ represents some vector of auxiliary model parameters to be estimated in addition to $\bm{\theta}$ by fitting the model to observed data.

In an educational testing context, students take standardized tests intended to measure their academic "ability" levels. Analysts who score the tests cannot observe a student's ability directly---it is unclear how that would be possible. They do, however, observe the student's answers to known test questions. IRT models provides a structure to infer abilities from the student's pattern of test answers. The context of policy choice is similar. It is impossible to observe any individual's political ideology directly, but we theorize that it affects their responses to survey items about policy choices. The IRT setup lets us summarize an individual's policy preferences by analyzing the structure of their responses to various policy choices.

It is crucial to note that the only way to estimate a latent construct from observed data is for the model to impose assumptions about the functional relationship between the latent construct and the observed data. In this sense, the estimates can be sensitive to the model's assumptions. While this is always important to acknowledge, it is also valuable to note that model-dependence is an ever-present consideration even for simpler measurement strategies, such as an additive index that sums or averages across a battery of variables. In fact, additive indices are special cases of measurement model where key parameters are assumed to be known and fixed, which is problematic if there is any reason to suspect that item responses are correlated across individuals.^[
  Midpoint and discrimination parameters would be sources of this correlation. Additive indices are similar to a model where all all midpoint and discrimination are respectively equal to $0$ and $1$ by assumption.
]
In this way, measurement models *relax* the assumptions of simpler measurement strategies, even if the underlying mathematics are more intensive. 

<!------- TO DO ---------
- Literature about "ideology" (Clinton piece?)
- "traits"
------------------------->

#### Item Characteristics and Item Parameters


```{r example-icc-data}
# create example ICCs/IRFs
# for some number of simulated items
ex_items <- 5
fix_cut <- 0
fix_disc <- 1

# item params are random but seeded
set.seed(2019)

# ability-only, 1p, 2p models
item_params_0p <- 
  tibble(
    cutpoint = fix_cut,
    discrimination = fix_disc
  ) %>%
  print()

item_params_1p <- 
  tibble(
    cutpoint = rnorm(ex_items, fix_cut, 1.5),
    discrimination = fix_disc
  ) %>%
  print()

item_params_2p <- 
  tibble(
    cutpoint = rnorm(ex_items, 0, 1.5),
    discrimination = rnorm(ex_items, fix_disc, 0.5)
  ) %>%
  print()


# table of item params
ex_item_params <- 
  bind_rows(
    .id = "model",
    "Ability Only: No Item Variation" = item_params_0p,
    "Varying Item Difficulty" = item_params_1p,
    "Varying Difficulty & Discrimination" = item_params_2p 
  ) %>%
  mutate(
    item = 1:n(),
    model = fct_relevel(
      model, "Ability Only: No Item Variation", "Varying Item Difficulty"
    )
  ) %>%
  print()

# table of IRF data
ex_iccs <- ex_item_params %>%
  crossing(theta = seq(-4, 4, .01)) %>%
  mutate(
    eta = discrimination * (theta - cutpoint), 
    prob = plogis(eta)
  ) %>%
  print()
```

Measurement models relax assumptions about the data's functional dependence on the construct of interest. Item response theory focuses this effort on the items to which subjects respond. Different items may reveal different information about the latent construct; the design of the model governs how those item differences can manifest [see @fox:2010:bayesian-IRT for a comprehensive review of IRT modeling].

Consider a simple model where a student $i$ is more likely to answer test questions $j$ correctly if she has greater academic ability $\theta_{i}$. Analogously, a citizen who is more conservative is more likely to express conservative preferences for policy question $j$. Keeping the probit functional form from above, we can represent this simple model with the equation:
\begin{align}
  \mathrm{Pr}\left( y_{ij} = 1 \right) &= 
  \Phi\left(\theta_{i}\right),
  (\#eq:ability-only)
\end{align}
where $\theta_{i}$ is scaled such that the probability of a correct/conservative response is $0.5$ at $\theta_{i} = 0$. This model makes the implicit assumption that knowing $\theta_{i}$ is sufficient to produce exchangeable response data; there are no systematic differences in the difficulty level of the test questions or the ideological nature of the policy choices that would affect the propensity of subjects to answer correctly/conservatively on average. This implicit assumption is often unrealistic. Just as some test questions are naturally more difficult than others, some policy questions present more extreme or lopsided choices than others, leading citizens with otherwise equivalent $\theta$ values to vary systematically in their response probability across items. Although the "ability-only" model seems unrealistic when posed as such, political science is replete with additive measurement scales that omit all item-level variation: indices of policy views, the racial resentment scale, survey-based scales of political participation, and more.

Rather than assume that all items behave identically for all individuals, IRT explicitly models the systematic variation at the item level using _item parameters._ IRT models have different behaviors based on the parameterization of the item effects in the model. The simplest IRT model is the "one-parameter" model,^[
  One-parameter logit models are often called "Rasch" models, whereas their corresponding probit models are often called "Normal Ogive" models [@fox:2010:bayesian-IRT].
]
which includes an item-specific intercept $\kappa_{j}$.
\begin{align}
  \mathrm{Pr}\left( y_{ij} = 1 \right) 
  &= \Phi\left(\theta_{i} - \kappa_{j}\right)
  (\#eq:1p-ogive)
\end{align}
IRT parlance refers to the $\kappa_{j}$ parameter as the item "difficulty" parameter. In the testing context, if a student has higher ability than the difficulty of the question, the probability that they answer the test item correctly is greater than $0.5$. This probability goes up for students with greater ability relative to item difficulty, and it goes down for items with greater difficulty relative to student ability. In a policy choice context, the difficulty parameter is better understood as the "cutpoint" parameter, the midpoint between two policy choices where the respondent is indifferent between the choice of Left or Right on item $j$. These cutpoints are allowed to vary from item to item; some policy choices present alternatives that are, on average, more conservative or liberal than others. For instance, the choice of *how much* to cut capital gains taxes will have a more conservative cutpoint than a question of whether to cut capital gains taxes at all. If there were no systematic differences across items, it would be the case that $\kappa_{j} = 0$ for all $j$, and the one-parameter model would reduce to the simpler model in Equation&nbsp;\@ref(eq:ability-only).

The "two-parameter" IRT model is more common, especially in the ideal point context. The two-parameter model introduces the "discrimination" parameter $\iota_{j}$, which behaves as a slope on the difference between $\theta_{i}$ and $\kappa_{j}$. 
\begin{align}
  \mathrm{Pr}\left( y_{ij} = 1 \right)
  &= 
  \Phi
  \left( \iota_{j}\left(\theta_{i} - \kappa_{j}\right) \right),
  (\#eq:2p-ogive)
\end{align}
Intuitively, the discrimination parameter captures how well a test item differentiates between the responses of high- and low-ability students, with greater values meaning more divergence in responses. In the ideal point context, it captures how strongly a policy question divides liberal and conservative respondents.^[
  Two-parameter IRT models are sometimes written with $\iota_{j}$ is distributed through the equation: $\iota_{i}\theta_{i} + \alpha_{j}$, where $\alpha_{j} = \iota_{j}\kappa_{j}$. Although this parameterization more closely follows a linear slope-intercept equation, it loses the appealing interpretation of $\kappa_{j}$ as the midpoint between policy choices.
]


```{r plot-example-iccs, include = TRUE, echo = FALSE, fig.cap = "Examples of item characteristic curves under different item parameter assumptions", fig.width = 9, fig.height = 4, out.width = "100%"}
ggplot(ex_iccs) +
  aes(x = theta, y = prob) +
  facet_wrap(~ model) +
  geom_point(data = ex_item_params, aes(x = cutpoint, y = 0.5),
             size = 1.5) +
  geom_line(aes(group = as.factor(item)), show.legend = FALSE) +
  scale_y_continuous(breaks = c(0, .5, 1)) +
  scale_x_continuous(
    breaks = c(-3, 0, 3), 
    labels = c("Low", "Med", "High")
  ) +
  # scale_color_brewer(palette = "Set2") + 
  labs(
    x = TeX("Ability/Conservatism $(\\theta_{\\mathit{i}})$"), 
    y = "Probability of\nCorrect/Conservative Response"
  ) +
  NULL
```

Figure&nbsp;\@ref(fig:plot-example-iccs) shows how response probabilities are affected by the parameterization of item effects. Each panel plots how increases in subject ability or conservatism (the horizontal axis) result in increased response probability (the vertical axis), where the shape of the curve is set by values of the item parameters. These curves are commonly referred to as *item characteristic curves* (ICCs) or *item response functions* (IRFs). The leftmost panel shows a model with no item effects whatsoever; any item is theorized to behave identically to any other item, and response probabilities are affected only by the subject's ability (ideology). The middle panel shows a one-parameter model where item difficulties (cutpoints) are allowed to vary systematically at the item level. Difficulty parameters behave as intercept shifts, so they convey which value of $\theta$ yields a correct response with probability $0.5$, but they do not affect the *elasticity* of the item response function to changes in $\theta$. The final panel shows item response functions from the two-parameter IRT model, where item difficulties (intercepts) and discriminations (slopes) are allowed to vary across items.

<!-- IRT model parameters -->
#### IRT Interpretation of the Ideal Point Model

How do we interpret our statistical model of ideal points in light of item response theory? Recall the statistical model that we derived from the utility model above. An Actor $i$ faces policy question $j$, with a Right alternative located at $R_{j}$ and a Left alternative located at $L_{j}$. The Actor chooses the alternative closest to her ideal point $\theta_{i}$, subject to idiosyncratic utility shocks summarized by $\epsilon_{ij}$. Letting $y_{ij}$ indicate the outcome that Actor $i$ chooses the Right position on policy question $j$, the probability that $y_{ij} = 1$ is given by the two-parameter model in Equation&nbsp;\@ref(eq:2p-ogive) (or \@ref(eq:choice-2p) above).

The behavior of the item parameters can be understood by remembering that they are functions of the Left and Right choice locations. For instance, the cutpoint parameter $\kappa_{j}$ represents an intercept shift for an items response function and is equal to $\dfrac{R_{j} + L_{j}}{2}$. Suppose that $\theta_{i} - \kappa_{j} = 0$, which occurs if the item cutpoint falls directly on an Actor's ideal point. In such a case, the Actor would be indifferent (in expectation) to the choice of Left or Right, and the probability of choosing Right would collapse to $0.5$.^[
  This holds in logit and probit models, since $\mathrm{logit}^{-1}(0)$ and $\Phi(0)$ are both equal to $0.5$.
]
The value of $\kappa_{j}$ increases by moving either the Right or Left alternatives to the right (increasing $R_{j}$ or $L_{j}$), subject to the constraint that $R_{j} \geq L_{j}$. Larger values of the item cutpoint imply a lower probability that the Actor chooses Right, since $\kappa_{j}$ has a non-positive effect on the conservative response probability.^[
  Formally we can show this by taking the derivative of the link function with respect to the cutpoint: $\dfrac{\partial \iota_{j}\left(\theta_{i} - \kappa_{j}\right)}{\partial \kappa_{j}} = -\iota_{j}$, where $\iota_{j}$ is constrained to be greater than or equal to zero
]
The opposite intuition holds as the Left position becomes increasingly progressive, resulting in larger values of $\kappa_{j}$ that imply a higher probability of choosing Right, all else equal.

The discrimination parameter behaves as a "coefficient" on the distance between the Actor ideal point and the cutpoint, meaning that the Actor's choice is more elastic to her policy preferences as $\iota_{j}$ increases.^[
  Again we can demonstrate this by noticing that the derivative of the link function with respect to the discrimination parameter is $\dfrac{\partial \iota_{j}\left(\theta_{i} - \kappa_{j}\right)}{\partial \iota_{j}} = \left(\theta_{i} - \kappa_{j}\right)$. The derivative's magnitude depends on the absolute value of this distance, and its sign depends on the sign of the difference.
]
Because $\iota_{j} = 2(R_{j} - L_{j})$, the discrimination parameter grows when the distance between the Right and Left alternatives grows larger, which happens when $R_{j}$ increases or $L_{j}$ decreases.

In a special case that Right and Left alternatives are located in exactly the same location, the result is $\kappa_{j} = \iota_{j} = 0$, leading all Actors to choose Right with probability $0.5$. This result represents a situation where policy preferences are not systematically related to the choice whatsoever, and only idiosyncratic error affects the choice of Right or Left. Although the model implies that this result is *mathematically* possible, it is not realistic to expect any of the policy choices in this project to induce this behavior.


<!-- parametric assumptions -->
<!-- Up to this point, we have assumed symmetric and equally-weighted quadratic utility loss over ideological distances. No parametric assumptions have yet been made about the idiosyncratic errors or the difference in errors.  -->



### Applications of the IRT Approach

A section reviewing IRT in political science:

- ideal points (Poole and Rosenthal, CJR, Londregan, Jeff Lewis, Michael Bailey, Martin and Quinn)
- citizen ideology with survey data (Seth Hill multinomial and individual, Tausanovitch and Warshaw small area,  Caughey and Warshaw groups within areas)
- other latent modeling [@levendusky-et-al:2008-latent-partisanship]

<!-- ### Statistical Estimation of Ideal Points -->

<!-- Relation to additive indices (ability-only models) -->



## Modeling Party-Public Ideology in Congressional Districts

This section outlines my group-level ideal point model for party publics. It begins by describing the connection between the individual-level IRT model and the group-level model and its implication for the parameterization of the model (Section&nbsp;\@ref(sec:how-to-group)). I then lay out the hierarchical model for party-public ideal points in its static form (Section&nbsp;\@ref(sec:geographic-model)) and its dynamic form (Section&nbsp;\@ref(sec:dynamic-model)). I discuss technical features of model implementation, including choices for model parameterization,
model identification, 
prior distributions, 
and model testing methods such as prior predictive checks and posterior predictive checks.
<!-- (Section&nbsp;\@ref(sec:priors))  -->
<!-- (Section&nbsp;\@ref(sec:stan-setup)). -->


<!-- ### Static Model -->


### Group-Level IRT Model {#sec:how-to-group}

<!------- TO DO ---------
- clean this up
------------------------->

So far we have modeled individual responses to policy items according to their own individual ideal points, but this project is concerned with the average ideal point of a _group_ of individuals. In the group model, we assume that individual ideal points are distributed within a group $g$, where groups are define as the intersection of congressional districts $d$ and political party affiliations $p$.

As before, we observe a binary response from individual $i$ to item $j$, which we regard as a probabilistic conservative response with probability $\pi_{ij}$, which is given a probit model.
\begin{align}
  y_{ij} &\sim \mathrm{Bernoulli}(\pi_{ij}) \\
  \pi_{ij} 
  &= 
  \Phi\left(\iota_{j}\left( \theta_{i} - \kappa_{j} \right)\right)
  (\#eq:y-i-probit)
\end{align}

<!------- TO DO ---------
- Is this Fox? or Mislevy?
------------------------->

Following @fox:2010:bayesian-IRT and @caughey-warshaw:2015:DGIRT, it is helpful to reparameterize the IRT model to accommodate a group-level extension. This parameterization replaces item "discrimination" with item "dispersion" using the parameter $\sigma_{j} = \iota^{-1}_{j}$ and rewriting the model as
\begin{align}
  \pi_{ij} 
  &= 
  \Phi\left(\frac{\theta_{i} - \kappa_{j}}{\sigma_{j}}\right).
  (\#eq:y-i-irt)
\end{align}
@caughey-warshaw:2015:DGIRT describe the dispersion parameter as introducing "measurement error" in $\pi_{ij}$ beyond the standard Normal utility error from $\epsilon_{ij}$ above.

The group model begins with the notion that there is a probability distribution of ideal points within a group $g$, where a "group" is a partisan constituency within a congressional district. Supposing that individual deviations from the group mean are realized by the accumulation of a large number of random forces, we can represent an individual ideal point as a Normal draw from the group,^[
  Notation for Normal distributions will always describe the scale parameter in terms of standard deviation $\sigma$ instead of variance $\sigma^2$. This keeps the notation consistent with the way Normal distributions are expressed in Stan code.
]
\begin{align}
  \theta_{i} &\sim 
    \mathrm{Normal}\left( \bar{\theta}_{g[i]}, \sigma_{g[i]} \right)
  (\#eq:theta-i-draw)
\end{align}
where $\bar{\theta}_{g[i]}$ and $\sigma_{g[i]}$ are the mean and standard deviation of ideal points within $i$'s group $g$. 

While it is possible to continue building the model hierarchically from \@ref(eq:theta-i-draw), it would be far too computationally expensive to estimate every individual's ideal point in additional to the group-level parameters---every individual ideal point is essentially a nuisance parameter. Instead, we rewrite the model by aggregating individual-level survey response data to the group level, expressing the grouped outcome data as a function of the group parameters. Let $s_{gj} = \sum\limits_{i \in g}^{n_{gj}} y_{ij}$, the number of conservative responses from group $g$ to item $j$, where $n_{gj}$ is the total number of responses (trials) to item $j$ by members of group $g$. Supposing these trials were collected independently across groups and items (an assumption that is relaxed later), we could model the grouped outcome as a binomial random variable,
\begin{align}
  \begin{split}
  s_{gj} &\sim 
      \mathrm{Binomial}\left(n_{gj}, \bar{\pi}_{gj}\right) \\
    \bar{\pi}_{gj} &=
      \Phi\left(
        \frac{\bar{\theta}_{g} - \kappa_{j}}{
              \sqrt{ \sigma_{g}^{2} + \sigma^{2}_{j}}}
      \right),
  \end{split} %_ 
  (\#eq:y-g-irt)
\end{align}
where $\bar{\pi}_{gj}$ is the "average" conservative response probability for item $j$ in group $g$, or the probability that a randomly selected individual from group $g$ gives a conservative response to item $j$. Our uncertainty about any random individual's ideal point relative to the group mean is included in the model as group-level variance term. If individual ideal points are Normal within their group, this within-group variance can simply be added to the probit model as another source of measurement error, with larger within-group variances further attenuating $\bar{\pi}_{ij}$ toward $0.5$. @caughey-warshaw:2015:DGIRT derive this result in the supplementary appendix to their article.

<!------- TO DO ---------
- should I just rewrite it in my own style?
------------------------->

The current setup assumes that every item response is independent, conditional on the group and the item. This assumption is violated if the same individuals in a group answer multiple items---one individual who answers `r smart_number(20)` items is less informative about the group average than `r smart_number(20)` individuals who answer one item apiece. While this too could be addressed by explicitly modeling each individual's ideal point (extending the model directly from Equation&nbsp;\@ref(eq:theta-i-draw)), I implement a weighting routine that downweights information from repeated-subject observations while adjusting for nonrepresentative sample design, as I will describe in Section&nbsp;\@ref(sec:model-weights).

<!------- TO DO ---------
- I fix the rounding components here with my likelihood fix.
------------------------->


### Hierarchical Model for Group Parameters {#sec:geographic-model}

The group model described so far can be estimated straightforwardly if there are enough responses from enough individuals in enough district-party groups. In practice, however, a single survey will not contain a representative sample of all congressional districts, and certainty not a representative sample of partisans-within-districts. I specify a hierarchical model for the group parameters in order to stabilize the estimates in a principled way. The hierarchical model learns how group ideal points are related to cross-sectional (and eventually, over-time) 
<!------- TO DO ---------
- dynamics  
------------------------->
variation in key covariates, borrowing strength from data-rich groups to stabilize estimates for data-sparse groups, and even imputing estimates for groups with no survey data at all. This section describes the multilevel structure using traditional notation for hierarchical models; later in Section&nbsp;\@ref(sec:stan-setup) I describe how I parameterize the model for the estimation routine.

<!------- TO DO ---------
- don't forget the fact that CW aren't non-centered?
------------------------->

I posit a hierarchical structure where groups $g$ are "cross-classified" within districts $d$ and parties $p$. This means that groups are nested within districts and within parties, but districts and parties have no nesting relationship to one another. Districts are further nested within states $s$. I represent this notationally by referring to group $g$'s district as $d[g]$, or the $g$^th^ value of the vector $\mathbf{d}$. Similarly, $g$'s party is $p[g]$. For higher levels such as $g$'s state, I write $s[g]$ as shorthand for the more-specific but more-tedious $s[d[g]]$.

I use this hierarchical structure to model the probability distribution of group ideal points $\bar{\theta}_{g}$. I consider the group ideal point as a Normal draw from the distribution of groups whose hypermean is predicted by a regression on geographic-level data with parameters that are indexed by political party. This regression takes the form
\begin{align}
  \bar{\theta}_{g} &\sim \mathrm{Normal}\left(
    \mu_{p[g]} + \mathbf{x}^{\intercal}_{d[g]}\beta_{p[g]} + \alpha^{\mathtt{state}}_{s[g]p[g]},
    \sigma^{\mathtt{group}}_{p} %_
  \right) 
  (\#eq:theta-g-hlm)
\end{align}
where $\mu_{p[g]}$ is a constant specific to party $p$,^[
  Or "grand mean," since all covariates are eventually be centered at their means.
]
$\mathbf{x}_{d}$ is a vector of congressional district-level covariates with party-specific coefficients $\beta_{p}$. State effects $\alpha^{\mathtt{state}}_{sp}$ are also specific to each party. The benefit of specifying separate parameters for each party is that geographic features (such as racial composition, income inequality, and so on) may be related to ideology in ways that are not identical across all parties. This is an important departure from the structure laid out by @caughey-warshaw:2015:DGIRT, which estimates the same set of geographic effects for all groups in the data.
<!------- TO DO ---------
- Barry says to milk this, with an example maybe
- I think perhaps preview to use the estimates from the model?
------------------------->

<!-- We use a similar hierarchical regression for group scales (suppressing the $g$ subscript which is implied by the combination of $d$ and $p$).
\begin{align}
  \log \left( \sigma_{g} \right) 
  &\sim 
  \mathrm{Normal}\left( 
    \delta_{p} + X_{d}\delta_{p} + \eta_{sp}, 
    \psi_{\sigma}
  \right),
  (\#eq:sigma-g-draw)
\end{align}
where $\delta_{p}$ represents constants for each party, $\delta_{p}$ is a party-specific vector of coefficients on district features, and $\eta_{sp}$ are party-specific state effects. -->

The state effects are regressions on state features as well,
\begin{align}
\begin{split}
  \alpha^{\mathtt{state}}_{sp} 
  &\sim 
  \mathrm{Normal}\left(\mathbf{z}^{\intercal}_{s}\gamma_{p} + \alpha^{\mathtt{region}}_{r[s]p}, \sigma^{\mathtt{state}}_{p}\right), 
%_
% \\
% \eta_{sp} &\sim 
% \mathrm{Normal}\left(\mathbf{z}^{\intercal}_{s}\zeta_{p} + L_{r[s]p}, \psi_{\eta}\right),
\end{split}
(\#eq:state-effects-draws)
\end{align}
where state-level covariates $\mathbf{z}_{s}$ have party-specific coefficients $\gamma_{p}$.
<!-- (for group means) or $\zeta_{p}$ (for group scales).  -->
Each state effect is a function of a party-specific region effect $\alpha^{\mathtt{region}}_{[s]rp}$
<!-- (for group means) and $L_{rp}$ (for group scales)  -->
for Census regions indexed $r$, which is a modeled mean-zero effect to capture correlation within regions.
\begin{align}
  \alpha^{\mathtt{region}}_{rp} 
  &\sim
  \mathrm{Normal}\left(0, \sigma^{\mathtt{region}}_{p}\right) %_
(\#eq:region-effects-draws)
\end{align}


### Identification Restrictions {#sec:identification-restrictions}

Ideal point models, as with all latent space models, are unidentified without restrictions on the policy space. The model as written can rationalize many possible estimates for the unknown parameters, with no prior basis for deciding which estimates are best.
A two-parameter model such as this requires some restriction on the polarity, location, and scale of the policy space. 

\begin{itemize}  
\item Location: the latent scale can be arbitrarily shifted right or left. We could add some constant to every ideal point, and the response probability would be unaffected if we also add the same constant to every item cutpoint. 
\item Scale: the latent scale can be arbitrarily stretched or compressed. We could multiply the latent space by some scale factor, and the response probability would be unaffected if we also multiply the discrimination parameter by the inverse scale factor.
\item Polarity: the latent scale could be reversed. We could flip the sign of every ideal point, and the response probability would be unaffected if we also flip the sign of every item parameter.
\end{itemize}

These properties are present with every statistical model, but covariate data typically provide the restrictions necessary to identify a model.^[
  We could imagine shifting, stretching, or reversing the sign of a covariate to reveal the same mathematical behaviors. 
  All of these transformations would result in the same predictions as long as the parameters are also transformed to compensate.
]
Because the response probability is a function of the interaction of multiple parameters in a latent space, however, data alone do not provide the necessary restrictions on the space to provide a unique solution. Absent any natural restriction from the data, I provide my own restrictions on the polarity, location, and scale of the policy space. 

The polarity of the space is fixed by coding all items such that conservative responses are $1$ and liberal responses are $0$. This ensures that increasing values on the link scale always lead to an increasing probability of a _conservative_ item response. Additionally I impose a restriction that all discrimination parameters are positive, which implies that shifting any ideal point farther to the _right_ of an item cutpoint increases the probability of a conservative response, all else equal.

The location of the space is set by restricting the sum of the $J$ item cutpoints to be $0$. 
If $\tilde{\kappa}_{j}$ were an unrestricted item cutpoint, the restricted cutpoint $\kappa_{j}$ used in response model would be defined as
\begin{align}
  \kappa_{j} &= \tilde{\kappa}_{j} - \frac{\sum\limits_{j = 1}^{J} \tilde{\kappa}_{j}}{J},
  (\#eq:scale-midpoint)
\end{align}
which is performed in every iteration of the sampler. 
This restriction on the sum of the cutpoint parameters also implies a restriction on the mean of the cutpoints, since $\frac{0}{J} = 0$.

Lastly, I set the scale of the latent space by restricting the product of the $J$ discrimination parameters to be equal to $1$.
I implement this by restricting the log discrimination parameter to have a sum of $0$, which achieves an equivalent transformation.^[
  A quick demonstration using three unknown values $a$, $b$, and $c$. If $a \times b \times c = 1$, then $\log(a) + \log(b) + \log(c) = \log(1)$, which is equal to $0$.
]
Letting $\tilde{\iota}_{j}$ be the unrestricted discrimination parameter, we obtain the restricted $\iota_{j}$ as follows.
\begin{align}
  \iota_{j} &= \text{exp}\left(\log\left(\iota_{j}\right) \right) \\
  \log\left(\iota_{j}\right) &= \log\left(\tilde{\iota}_{j}\right) - \frac{1}{J} \sum\limits_{j= 1}^{J} \log\left(\tilde{\iota}_{j}\right)
  (\#eq:scale-difficulty)
\end{align}
Item discrimination is then reparameterized as dispersion, $\sigma_{j} = \iota_{j}^{-1}$. These restrictions on the item parameters are sufficient to identify $\bar{\theta}_{g}$.

<!-- \iota_{j} &= 
    \tilde{\iota}_{j} \left(
      \prod\limits_{j = 1}^{J}\tilde{\iota}_{j} 
    \right)^{-1/J} -->



### Weighted Outcome Data {#sec:model-weights}

The group-level model learns about group ideal points by surveying individuals within groups, but the model currently assumes that all $y_{gj}$ are independent conditional on the item. If the same individuals answer multiple items, this assumption is violated. Additionally, we cannot assume that responses are independent in the presence of nonrepresentative survey designs. This section describes an approach for weighting group-level data that adjusts for both issues. The corrections are lifted from @caughey-warshaw:2015:DGIRT with slight modifications.
<!-- but I am able to relax the constraint that resulting data must be integer valued. -->
<!------- TO DO ---------
- introduce data typing
------------------------->

<!-- The recipe contains three adjustments: a "design effect" to account for nonequal response probabilities within a given group $g$, an adjusted sample size for every group-item combination, and an adjusted number of successes for each group-item.  -->

<!------- TO DO ---------
- sum over i within g and j? should someone who answers 10 questions be 1/10 or 1+change? I don't get it.
- Talk to Alex
------------------------->

First, the sample size in each group-item "cell" $gj$ must is adjusted for survey design and multiple responses per individual. Let $n_{g[i]j}^{*}$ be the adjusted sample size for $i$'s group-item cell, defined as
\begin{align}
  n^{*}_{g[i]j} &=
  \sum\limits_{i = 1}^{n_{g[i]j}} \frac{1}{r_{i}d_{g[i]}},
  (\#eq:wted-n)
\end{align}
where $r_{i}$ is the number of responses given by individual $i$, and $d_{g[i]}$ is a survey design correction for $i$'s group. The effective sample size decreases when respondents answer multiple questions ($r_{i} > 1$) or in the presence of a sample design correction ($d_{g} > 1$). The design correction, originally specified by @ghitza-gelman:2013:subgroup-mrp, penalizes information collected from groups that contain greater variation in their survey design weights. It is defined as
\begin{align}
  d_{g[i]} &= 
    1 + \left(\frac{\mathrm{sd}_{g[i]}\left(w_{i}\right)}{
                    \mathrm{mean}_{g[i]}\left(w_{i} \right)} 
        \right)^{2},
  (\#eq:design-effect)
\end{align}
where $\mathrm{sd}(\cdot)$ and $\mathrm{mean}(\cdot)$ are the within-group standard deviation and mean of respondent weights $w_{i}$. If all weights within a cell are identical, their standard deviation will be $0$, resulting in a design correction equal to $1$ (meaning, no correction). Larger within-cell variation in weights increases the value of $d_{g}$, thus decreasing the effective sample size within a cell. The intuition of this correction is to account for increased variance of weighted statistics compared to unweighted statistics, given a fixed number of observations [@ghitza-gelman:2013:subgroup-mrp, 765].

To obtain the weighted number of successes in cell $gj$, I multiply the cell's weighted sample size by its weighted mean. The weighted mean $\bar{y}^{*}_{gj}$ adjusts for the respondent's survey weight $w_{i}$ and number of responses $r_{i}$ and is defined as
\begin{align}
  \bar{y}^{*}_{g[i]j} &=
  \dfrac{
    \sum\limits_{i = 1}^{n_{g[i]j}}
    \frac{w_{i}y_{ij}}{r_{i}}
  }{
    \sum\limits_{i = 1}^{n_{g[i]j}}
    \frac{w_{i}}{r_{i}}
  }.
  (\#eq:wted-y)
\end{align}
The weighted number of successes in each cell, in turn, is 
\begin{align}
  s^{*}_{gj} &= \mathrm{min}\left(n^{*}_{gj}\bar{y}^{*}_{gj}, n^{*}_{gj}\right). %*
  (\#eq:wted-s)
\end{align}
where I take the minimum to ensure that the number of successes does not exceed the adjusted sample size. 

It is likely that many values of $n^{*}_{gj}$ and $s^{*}_{gj}$ will be non-integers. Ordinarily this would be a problem for modeling a Binomial random variable, since a Binomial is an integer-valued count of successes given an integer number of trials and a success probability. For this reason, many statistical programs will return an error if a floating-point argument is bassed to the Binomial probability mass function. Whereas @caughey-warshaw:2015:DGIRT calculate $\lceil n^{*}_{gj} \rceil$ and $\lfloor s^{*}_{gj} \rceil$ to obtain integer data for estimation, I instead implement a custom Binomial quasi-likelihood function that returns log probabilities for weighted data (see Section&nbsp;\@ref(sec:logp-accumulator)).



### Bayesian Estimation and Computational Implementation {#sec:stan-setup}

<!-- ### Model Implementation in Stan  -->

<!------- TO DO ---------
BAYESIAN OVERTURE

- key benefits (just another parameter)
- additional considerations
------------------------->

I implement the model using Stan, a programming language for high-performance Bayesian analysis that extends and interfaces with `C++` [@carpenter-at-al:2016:stan]. Stan implements an adaptive variant of Hamiltonian Monte Carlo (HMC), an algorithm that efficiently collects posterior samples by "surfing" a Markov proposal trajectory along the gradient of the posterior distribution. Because the algorithm uses the posterior gradient to generate proposals, the algorithm concentrates proposals in regions of high transition probability and performs better in high dimensions than conventional Gibbs sampling algorithms. Although it possible to estimate Stan models using front-end software packages such as `brms` for R [@burkner:2017:brms], complicated models must be programmed with raw Stan code, which can be intensive. This section describes instances where the model _as programmed in Stan_ departs from the model _as written_ above. Although these alterations do not change the statistical intuitions of the model, they are essential for the model's computational stability by protecting against biased MCMC estimation and floating point arithmetic errors.
<!------- TO DO optimization? ------------------------->
These contributions should be highlighted here because they are crucial to ensuring valid inferences and are substantive improvements on previous software implementations of group-level IRT models.

<!------- TO DO ---------
- intro to Bayesian computation?
    - Hamiltonian Monte Carlo
    - log probability 
-->

#### Non-Centered Parameterization {#sec:noncentering}

<!-- Non-Centered Parameterization -->
<!-- Many common MCMC algorithms have  -->

Hierarchical models have posterior distributions whose curvatures present difficulties for sampling algorithms [@betancourt:2015:hamiltonian; @papaspiliopoulos-et-al:2007:parameterizations]. To improve the estimation in Stan,
  <!-- to do: Stan -->
<!-- Flag early on that I'll be setting things up to make things easier in Stan -->
I program the hierarchical models using a "non-centered" parameterization rather than a "centered" parameterization. Whereas the centered parameterization considers $\bar{\theta}_{g}$ as a random draw from a hierarchical distribution (Equation&nbsp;\@ref(eq:theta-g-hlm) above),
the non-centered parameterization defines $\bar{\theta}_{g}$ as a deterministic function of its conditional hypermean and a random variable.
\begin{align}
  \bar{\theta}_{g} &=
    \mu_{p[g]} + \mathbf{x}^{\intercal}_{d[g]}\beta_{p[g]} + 
    \alpha^{\mathtt{state}}_{s[g]p[g]} + 
    u_{g}\tau_{p[g]}, \\
    u_{g} &\sim \mathrm{Normal}\left(0, 1\right)
  (\#eq:theta-g-noncenter)
\end{align}
where $u_{g}\tau_{p[g]}$ behaves as a group-level error term. It is composed of a standard Normal variate $u_{g}$ and a scalar parameter $\tau_{p}$ that controls the scale of the error term. The non-centered model is algebraically equivalent to centered model in the likelihood, but it factors out (or "unnests") the location and the scale from the hyperprior. The non-centered parameterization improves MCMC sampling by de-correlating the parameters that compose the hierarchical distribution. Hierarchical models using a centered parameterization, on the other hand, are vulnerable to estimation biases due to poor posterior exploration [@betancourt:2015:hamiltonian].^[
  Stan's HMC algorithm is programmed to diagnose poor posterior exploration by detecting "divergent transitions" during sampling. Because Stan's HMC algorithm uses the gradient of the posterior distribution to propose efficient transition trajectories through the parameter space, it adaptively builds expectations about the probability density of the next Markov state. Areas of high curvature in the posterior gradient can lead to "divergences" in the HMC algorithm: transitions where the log density of a state differs substantially from what Stan anticipated when it proposed the transition. Markov chains with many divergent transitions have a high risk of being severely biased, since the divergences indicate that the Markov chain is failing to efficiently navigate the parameter space [@betancourt:2015:hamiltonian]. The non-centered parameterization smooths out these problematic regions of posterior density, safeguarding against biased MCMC estimates.
]
This is a crucial extension to the estimation approach developed by @caughey-warshaw:2015:DGIRT, whose model implements all hierarchical components using the centered parameterization.

Equation&nbsp;\@ref(eq:theta-g-noncenter) is an incomplete implementation of the non-centered form; to complete the parameterization, I apply it too all hierarchical components in the regression, including the state and region effects.
\begin{align}
\begin{split}
  \bar{\theta}_{g} &=
    \mu_{p[g]} + 
    \mathbf{x}^{\intercal}_{d[g]}\beta_{p[g]} + 
    u^{\mathtt{group}}_{g}\tau^{\mathtt{group}}_{p[g]} \\
    &\quad +
    \mathbf{z}^{\intercal}_{s[g]}\gamma_{p} +
    u^{\mathtt{state}}_{s[g]p[g]}\tau^{\mathtt{state}}_{p[s]} \\
    &\quad +
    u^{\mathtt{region}}_{r[g]p[g]}\tau^{\mathtt{region}}_{p[g]}
\end{split}
(\#eq:full-noncenter)
\end{align}
The full model places the hypermean regressions and error terms for groups, states, and regions in one deterministic equation. It contains "error terms" for each level of hierarchy---groups, states, and regions---where all parameters are indexed by party. 

<!-- Heteroskedastic model -->
<!-- 
We also have a hierarchical model that predicts the ideal point standard deviation within each group, $\sigma_{g}$. This makes the model "heteroskedastic"---we are modeling the mean ideal point within each group and the variance, conditional on hierarchical covariates. The model for $\sigma_{g}$ in non-centered form is as follows:
\begin{align}
  \log(\sigma_{g}) &= X_{g}\delta_{p} + Z_{s[g]}\eta_{p} + m_{g}\nu + m_{sp}\lambda
  (\#eq:sigma-g-hetero-noncenter)
\end{align}
Where $X_{g}$ and $Z_{sp}$ are the same group- and state-level covariates as the above regression, $\delta_{p}$ and $\eta_{p}$ are party-varying coefficients. The terms $m_{g}\nu$ and $m_{sp}\lambda$ are "factored" error terms for groups and states, where $m_{g}$ and $m_{sp}$ are each distributed $\mathrm{Normal\left(0, 1\right)}$, while $\nu$ and $\lambda$ are scale factors. 
-->


<!------- TO DO ---------
- Read stan files into an appendix?
------------------------->


#### Log Likelihood for Weighted Data {#sec:logp-accumulator}

One important implementation consideration for the group IRT model is the presence of weighted, non-integer response data. As described in Section&nbsp;\@ref(sec:model-weights), grouped data require reweighting to account for nonrepresentative sample designs and repeated observations within individual members of a group. The resulting data are likely to take non-integer values, which would cause the built-in Binomial likelihood function to fail. Whereas @caughey-warshaw:2015:DGIRT round their data to conform to Stan's Binomial likelihood function, my approach rewrites the likelihood function to accept non-integer data. This allows me to maintain the precision in the underlying data while still correcting the issue at hand.

To explain how this works in Stan, some context on Bayesian computation is helpful. It is usually sufficient for Bayesian estimation with Markov chain Monte Carlo to calculate the posterior density of model parameters only up to a proportionality constant,
\begin{align}
  (\#eq:bayes-prop)
  p(\theta \mid y) &\propto p(y \mid \theta)p(\theta),
\end{align}
where $\theta$ and $y$ generically represent parameters and observed data. For computational stability, especially in high dimensions where probability densities get very small, these calculations are done on the log scale.
\begin{align}
  \log p(\theta \mid y) &\propto \log p(y \mid \theta) + \log p(\theta),
  (\#eq:bayes-prop-log)
\end{align}
MCMC algorithms calculate the right-side of this proportionality at each iteration of the sampler to decide if proposed parameters should be accepted into the sample or rejected. In Stan, this calculation is passed to the _log density accumulator_, a variable containing the sum of the log likelihood and log prior density at every sampler iteration [@carpenter-at-al:2016:stan]. 

In the current case, it is the probability of the data $\log p(y \mid \theta)$ that presents a problem. The Binomial log likelihood function as written in Stan will not accept non-integer data, so I rewrite the kernel $K(\cdot)$ of the Binomial log likelihood,
\begin{align}
  \log K\left(p\left(s^{*}_{gj} \mid \bar{\pi}_{gj} \right)\right) 
  &=
    s^{*}_{gj} \log \bar{\pi}_{gj} 
    + \left(n_{gj}^{*} - s^{*}_{gj}\right) \log \left(1 -\bar{\pi}_{gj}\right)
  (\#eq:weighted-ll-raw)
\end{align}
where the number of trials $n_{gj}^{*}$ and the number of successes $s^{*}_{gj}$ can take non-integer values. This is the same approach that @ghitza-gelman:2013:subgroup-mrp take in a frequentist maximum likelihood context.^[
  They describe this as "simply the weighted log likelihood approach to fitting generalized linear models with weighted data" [@ghitza-gelman:2013:subgroup-mrp, 765].
]
Rather than declare that $s^{*}_{gj}$ is a standard Binomial draw, I pass the results of Equation&nbsp;\@ref(eq:weighted-ll-raw) directly to the log density accumulator. This serves as the log likelihood for the response data. Other sampling statements that add prior density to the accumulator are unaffected by my approach to the log likelihood.

I implement Equation&nbsp;\@ref(eq:weighted-ll-raw) using a log sum of exponentials routine, a practice for robust computational arithmetic that prevents numerical underflow [e.g. @Carpenter:2016:logsumexp].

[ insert what that routine actually would be]

\begin{align}
  \eta_{gj} &= 
    \left(
      \frac{\bar{\theta}_{g} - \kappa_{j}}{
            \sqrt{ \sigma_{g}^{2} + \sigma^{2}_{j}}}
    \right) \\
  \bar{\pi}_{gj} &= 
    \tilde{\Phi}\left(\eta_{gj}\right) \text{ (logistic approximation to $\Phi(\cdot)$)}\\
  &= 
    \mathrm{logit}^{-1}\left(
      0.07056 \eta_{gj}^{3} + 1.5976 \eta_{gj}
    \right)
  (\#eq:robust-likelihood)
\end{align}

To do:

- does the logistic approximation fail without log-sum-exponential after the changes we've made to the model already?

<!------- TO DO ---------
- come back to this after trying the probit in the code again!
------------------------->


#### Optimized IRT Model

The matrix expansion trick would go here, if I did it.

<!------- TO DO ---------
- resolve this
------------------------->



### Prior Distributions and Prior Checks {#sec:priors}

<!------- TO DO ---------
- Does intro to Bayesian stuff more rightly go in Ch 1/Intro?
------------------------->
The Bayesian modeling paradigm requires a joint prior probability distribution over the model parameters, which can be a benefit and a drawback of the approach. 
Priors are beneficial as means to encode external information into a model. 
This allows any analysis to move past "model fitting" and toward generalizing beyond a given dataset, since the posterior distribution represents a synthesis of external information with current data. 
Furthermore, prior distributions are a framework for _formally_ including external information, rather than informally and invisibly using prior information as criteria for model selection.
<!------- TO DO ---------
- cite this burn!!!
------------------------->
Priors most useful for stabilizing parameter estimates and downweighting unreasonable estimates, not by upweighting a researcher's preferred narrative of the data-generating process.
This is especially valuable in data-sparse settings when parameters are weakly identified or not identified at all, such as hierarchical models where group parameters can be imputed by estimating the distribution from which they are drawn using hyperpriors [@gelman-hill:2006:multilevel]. The drawback of Bayesian modeling is that prior specification is additional work for the researcher, which can be complicated especially in situations where a model is sensitive to the choice of prior. 

This section provides describes and justifies priors used in the group ideal point model. The discussion here is more detailed than in a typical paper describing a Bayesian ideal point model for several reasons. 
Firstly, the norms of the typical Bayesian workflow are evolving toward more rigorous checking of prior distributions and their implications [@betancourt:2018:workflow-blog; @gabry-et-al:2019:visualization]. These prior checks allow researchers to explore and demonstrate the consequences of their prior choices in transparent ways, but most Bayesian analyses in political science lack these explicit prior checks. Authors often declare their prior choices without explicitly justifying these choices, which can make prior specification feel opaque or even arbitrary to non-Bayesian readers.
Secondly, and more specifically to this project, the nonlinearities introduced by a probit model present particular challenges for specifying priors. Some of my choices depart from those in previous Bayesian ideal point models for important theoretical and practical reasons that I explain below.
Thirdly, model parameterization is important for effective Bayesian computation (see Section&nbsp;\@ref(sec:noncentering)), and although reparameterization does not affect the likelihood of data given the parameters, parameterization naturally affects the choice of priors.
This exploration of priors is a crucial component of the model-building process for this project and is uncommon in other Bayesian works in political science, so it is important to justify these choices with sufficient detail.


#### General Approach to Prior Distributions

Before discussing priors for the group ideal point model, it is helpful to discuss some general principles for working with prior distributions. They are not _universal_ principles, but they are _theoretical_ in the sense that they provide a pre-data orientation for prior distributions. They are heuristic principals in the sense that they provide powerful shortcuts to good analysis decisions based on lightweight signals about the problem at hand.^[
  @gelman:2017:applied-statistics holds that theoretical statistics ought to be the "theory of applied statistics," in the sense that statistical theory ought to be informed by "what we actually do" and should thus work to formalize aspects of workflow that begin merely as "good practice."
]

This discussion of priors begins with the orientation laid out by @gelman-et-al:2017:prior-likelihood that "the prior can often only be understood in the context of the likelihood." Although prior information is generally regarded as information that a researcher has before encountering data---and therefore before making any modeling decisions---in practice it is often the case that priors are chosen with reference to a specific analysis model. For example, we may have prior expectations about the proportion of Republicans who express conservative preferences on a given policy question (e.g.&nbsp;that the proportion is most likely above 50 percent), but if we model the proportion with a probit regression model, we typically specify priors on regression coefficients rather than the proportion, which is a function of the regression coefficients. This means that researchers must consider their priors as embedded in the specific data model at hand.

```{r diff-means-example}
means_data <- 
  tibble(
    x = seq(-1, 1, .05),
    unif = ifelse(x >= 0, dunif(x, 0, 1), NA),
    triangle = 
      ifelse(
        x < 0, 
        (2 * (x - -1)) / ((1 - -1) * (0 - -1)), 
        (2 * (1 - x)) / ((1 - -1) * (1 - 0))
      )
  ) 

plot_prior <- function(data, x, y) {
  ggplot(data) +
  aes(x = {{x}}, y = {{y}}) +
  geom_ribbon(
    aes(ymin = 0, ymax = {{y}}),
    fill = primary_light
  ) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1.5))
}
```

```{r plot-diff-means-example}
plot_prior(data = means_data, x = x, y = unif) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(
    x = TeX("Group Mean: $\\mu_{z}$"),
    y = "Prior Density",
    title = "Prior Densities for Difference in Means",
    subtitle = "If means have Uniform(0, 1) priors"
  ) +
plot_prior(data = means_data, x = x, y = triangle) +
  labs(x = TeX("Treatment Effect: $\\mu_{1} - \\mu_{0}$")) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) 
```

In this regard, the parameterization of a model can have a unique effect on the researcher's prior for some ultimate quantity of interest, even if the parameterization does not affect the likelihood of the data given the parameters. For instance, consider a simple experiment with a binary outcome variable $y_{i}$ and binary treatment assignment $z_{i} \in \{0, 1\}$. Suppose that the treatment effect of interest is a difference-in-means, $\bar{y}_{z = 1} - \bar{y}_{z = 0}$, estimated from a linear probability model. This linear probability model might be parameterized in two ways. First is a conventional regression setup,
\begin{align}
  y_{i} &= \alpha + \beta z_{i} + \epsilon_{i}
  (\#eq:diff-means-example)
\end{align}
where $\alpha$ is the control group mean, $\alpha + \beta$ is the treatment group mean, $\beta$ represents the difference in means, and $\epsilon_{i}$ is a symmetric error term for unit $i$. With the model parameterized in this way, the researcher must specify priors for $\alpha$ and $\beta$. Suppose that the researcher gives $\beta$ a flat prior to represent ignorance about the treatment effect. An equivalent _likelihood model_ for the data would be to treat each observation as a function of its group mean $\mu_{z}$.
\begin{align}
  y_{i} &= z_{i}\mu_{1} + \left(1 - z_{i}\right)\mu_{0}  + \epsilon_{i}
  (\#eq:separate-means-example)
\end{align}
Although the treatment effect $\beta$ from Equation&nbsp;\@ref(eq:diff-means-example) is equivalent to the difference in means $\mu_{1} - \mu_{0}$ from Equation&nbsp;\@ref(eq:separate-means-example), the parameterization of the model affects the implied prior for the difference in means. If the researcher gives a flat prior to both $\mu_{\mathbf{z}}$ terms, the implied prior for the difference in means will not be flat. Instead, it will be triangular, as shown in Figure&nbsp;\@ref(fig:plot-diff-means-example). The underlying mechanics of this problem are well-known in applied statistics---if we continue adding parameters, the Central Limit Theorem describes how the resulting distribution will converge to Normality---but it takes the explicit specification of priors to shine a light on the consequences of default prior choices in a particular case. In particular it shows how even flat priors, which are popularly regarded as "agnostic" priors because of their implicit connection to maximum likelihood estimators, do not necessarily imply flat priors about the researcher's key quantities of interest. Rather, flat priors can create a variety of unintended prior distributions that do not match the researcher's expectations. I return to this important idea in the discussion about setting priors for a probit model in Section&nbsp;\@ref(sec:probit-prior).

```{r plot-diff-means-example, include = TRUE, fig.height = 4, fig.width = 8, out.width = "100%", fig.cap = "Model parameterization affects prior distributions for quantities of interest that are functions of multiple parameters or transformed parameters. The left panel shows that the difference between two means does not have a flat prior if the two means are given flat priors. Note that the $x$-axes are not fixed across panels."}
```

- equivalent parameterizations
- weak information
    - between structural and regularizing. They achieve regularization by encoding structural information, but specific to a _class_ of problems more than the specific problem
- structure and families, less reliance on the actual prior param values




<!-- theory affected by practice, formalize workflow -->

<!-- Exploring the consequences of prior distributions is one of many reasons for the growing importance of model checking in applied Bayesian analysis [@betancourt:2018:workflow-blog; @gabry-et-al:2019:visualization; @gelman-et-al:2013:BDA, ch.&nbsp;6]. It also exemplifies how prior distributions are a _practical_ concern for modeling, not a theoretically abstract  -->

<!-- This is not to say that parameterization is always daunting or problematic for the applied researcher.  -->


<!-- Betancourt, Gelman, Simpson: Consider, for example, a simple binomial likelihood with n = 75 trials and some prior on the success probability, p. If you observe y = 40, then you can readily compute the posterior and consider issues of prior sensitivity and predictive performance regardless of the choice of prior. However, what if you observe y = 75? Then, suddenly you need to be more careful with the choice of prior, if you are working in a setting in which distinctions between probabilities of 0.9, 0.99, and 0.999 are of applied importance. This doesnâ€™t imply that the prior should explicitly depend on the measured data, just that a default prior that works well in one scenario might be problematic in another. -->

The data model itself provides important structure for choosing priors.

This is a pragmatic view. It recognizes that priors are (1) philosophically essential for posterior inference, (2) coarse tools for regularizing and model stabilization, and (3) mere nuisance terms that are required for Bayesian updating.

Subjective v. objective

- the actual degree of belief is zero
- not really the issue
- priors provide practical stability



General thoughts about priors

- WIPS and the evolution of thinking about this
    - likelihood is important weak information
    - constraining values to what is reasonable
    - but not so informed that we're downweighting reasonable values unless when context demands
        - sometimes it does, like identifiability, strong regularization/separation
- parameterization
    - normal(a, b) or a + bu, u ~ normal(0, 1)


#### Understanding the Probit Model {#sec:probit-prior}


- priors and likelihoods
    - probit is a nonlinear model
    - data aren't additively related to y, but some other thing
    - the prior is specified in reference to some likelihood


How the probit model works

- there is a latent scale
- covariates typically have linear, additive effects
- error is assumed Normal
- the latent scale is identified as having location zero and scale 1
- this means the predicted probability of a 1 is the probability that the index > 0, which is equivalent to the normal CDF at the predicted index value
- coefficients are uncertain, so uncertainty in posterior data are owed to probabilistic uncertainty about y given an index value, and baseline uncertainty about location on the index given covariates

How Bayesian modeling with probit works:

- because we know the Normal distribution, we know the region of quantile space that reasonably produces our outcome data
- combination of data and parameters shouldn't realistically lead us beyond the quantiles that produce probabilities between 1% and 99% (justify)
- it isn't crazy that some of our predictions are highly determined, so we don't want to be too restrictive, but broadly speaking, a priori, you know (justify better)

Divergence from past work

- Clinton, Jackman, Rivers
- Treier and Hillygus
- Caughey and Warshaw
    - noncentering
    - lognormal



```{r probit-explore}
prior_probit <- 
  tibble(
    x = seq(-6, 6, .01),
    Probability = pnorm(x)
  ) %>%
  print()
```

```{r plot-valid-probit, include = TRUE, fig.width = 5, fig.height = 4, out.width = "70%", fig.cap = "The region of the probit model's linear index that maps to response probabilities between 1 and 99 percent."}
ggplot(prior_probit) +
  aes(x = x, y = Probability) +
  labs(x = "Linear Index Value", y = "Success Probability") +
  scale_x_continuous(
    breaks = seq(prior_probit$x %>% min(), prior_probit$x %>% max(), by = 1)
  ) +
  coord_cartesian(ylim = c(0, 1)) +
  # layered ribbons
  geom_ribbon(
    data = tibble(
      x = c(qnorm(.01), qnorm(.99)),
      ymin = -2, ymax = 2
    ),
    aes(ymin = ymin, ymax = ymax, y = NULL),
    fill = primary_light, alpha = 0.3
  ) +
  geom_ribbon(
    data = tibble(
      x = c(qnorm(.025), qnorm(.975)),
      ymin = -2, ymax = 2
    ),
    aes(ymin = ymin, ymax = ymax, y = NULL),
    fill = primary_light, alpha = 0.5
  ) +
  geom_ribbon(
    data = tibble(
      x = c(qnorm(.05), qnorm(.95)),
      ymin = -2, ymax = 2
    ),
    aes(ymin = ymin, ymax = ymax, y = NULL),
    fill = primary_light, alpha = 0.7
  ) +
  # 90%
  annotate(geom = "text", x = 3, y = 0.5, hjust = -0.1, label = "Inner 90%") +
  annotate(
    geom = "segment", 
    x = 3, xend = 0.85, y = 0.50, yend = 0.50
  ) +
  annotate(geom = "point", x = 0.85, y = 0.5, size = 1) +
  # 95%
  annotate(geom = "text", x = 3, y = 0.35, hjust = -0.1, label = "Inner 95%") +
  annotate(
    geom = "segment", 
    x = 3, xend = mean(qnorm(c(.975, .95))), y = 0.35, yend = 0.35
  ) +
  annotate(geom = "point", x = mean(qnorm(c(.975, .95))), y = 0.35, size = 1) +
  # 99%
  annotate(geom = "text", x = 3, y = .20, hjust = -0.1, label = "Inner 98%") +
  annotate(
    geom = "segment", 
    x = 3, xend = mean(qnorm(c(.99, .975))), y = .20, yend = .20
  ) +
  annotate(geom = "point", x = mean(qnorm(c(.99, .975))), y = 0.2, size = 1) +
  # cdf
  geom_line(size = 0.75) +
  annotate(
    geom = "text", label = "Normal CDF",
    x = -3, y = .85,
    hjust = 1.1
  ) +
  annotate(
    geom = "segment", x = -3, xend = qnorm(.85),
    y = .85, yend = .85
  ) +
  annotate(geom = "point", x = qnorm(.85), y = .85, size = 1)
```


```{r, eval = FALSE}
# ---- investigation of CJR and Treier/Hillygus priors ------------

treier <- tibble(
  beta = rnorm(5000, mean = 0, sd = 1)
) %>%
  mutate(group = row_number()) %>%
  crossing(theta = seq(-3, 3, .01)) %>%
  mutate(
    index = beta * (theta - 0),
    prob = pnorm(index)
  ) %>%
  print()

treier %>%
  filter(theta == 1) %>%
  ggplot() +
    aes(x = prob) +
    geom_histogram(boundary = 0, binwidth = .01) +
    labs(
      x = "Prior Probability of Conservative Response",
      y = "Prior Frequency (5k draws)",
      title = TeX("Discrimination ~ $N(0, 1)$")
    )

  ggplot(treier) +
    aes(x= theta, y = prob) +
    geom_line(
      aes(group = group),
      alpha = 0.25
    )
```

#### Item Parameters



I specify priors on the unscaled cutpoint and discrimination parameters that are Normal and LogNormal, respectively. In order to model their joint distribution, I specify a multivariate Normal distribution for the cutpoint and logged discrimination parameter,
<!------- TO DO ---------
- Come back to This
- why give discrimination a LN prior? because it's multiplied into the equation, would be added on the log scale?
------------------------->
\begin{align}
  \begin{bmatrix}
    \tilde{\kappa}_{j} \\
    \mathrm{log}\left(\tilde{\iota}_{j}\right)
  \end{bmatrix}
  &\sim 
  \mathrm{Normal}\left(\bm{\mu}, \bm{\Sigma}\right)
\end{align}
where $\bm{\mu}$ is a $2$-vector of means and $\bm{\Sigma}$ is a $2 \times 2$ variance-covariance matrix. 
Whereas @caughey-warshaw:2015:DGIRT specify independent priors for all item cutpoint and discrimination parameters separately, my hierarchical model partially pools the item parameters toward a common distribution. 
This allows estimates to borrow precision from one another rather than "forgetting" the information learned from one item when updating the prior for the next item.
The discrimination parameter, which has a product of $1$ when scaled, is logged so that it has a mean of $0$ on the log scale. 
This simplifies the prior specification of the mean vector $\bm{\mu}$, which is a standard multivariate Normal with no off-diagonal elements.^[
  Although I use a joint prior, the assumptions about the parameters' marginal distributions are similar to @caughey-warshaw:2015:DGIRT. 
  Their choice to restrict discrimination parameter to have a product of $1$ and a LogNormal distribution is identical to my choice to restrict log discrimination parameters to have a sum of $0$ and a Normal prior.
  The benefit of my parameterization is that, by specifying the Normal family directly on the logged discrimination parameter, it is much simpler to build the joint hierarchical prior for all item parameters simultaneously.
]
\begin{align}
  \bm{\mu} &\sim 
    \mathrm{Normal}\left( 
      \begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
      \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} 
    \right)
\end{align}

```{r lkj-density}
# compute LKJ density for fixed rho values
# also generate density for sd terms
lkj_scale <- 1
# sqrt(pi) / sqrt(2)

# (to plot exact densities, not samples)
lkj_density <- tibble(rho = seq(-1, 1, length.out = 1000)) %>%
  mutate(id = row_number()) %>%
  mutate(
    mat = map(rho, ~ matrix(data = c(1, .x, .x, 1), nrow = 2, ncol = 2)),
    prop_density_lkj = map_dbl(mat, ~ rethinking::dlkjcorr(.x, eta = 2, log = TRUE))
    # log SHOULD be false but bug in the program
    # , density_lkj = prop_density_lkj / sum(prop_density_lkj)
  ) %>%
  mutate(
    sd = seq(0, 3.5, length.out = n()),
    density_halfnorm = 2 * dnorm(sd, mean = 0, sd = 1)
  ) %>%
  print()

p_lkj <- ggplot(lkj_density) +
  aes(x = rho, y = prop_density_lkj, ymax = prop_density_lkj, ymin = 0) +
  geom_ribbon(, fill = primary_light, color = NA, alpha = 0.5) +
  geom_line() +
  labs(
    x = TeX("Off-Diagonal Correlation ($\\rho$)"),
    y = "Prior Density"
  ) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
  NULL

lkj_dens_plots <- 
  p_lkj +
    aes(x = sd, y = density_halfnorm, ymax = density_halfnorm) +
    labs(
      x = TeX("Item Scales ($\\sigma_{\\tilde{\\kappa}}$ and $\\sigma_{\\tilde{\\iota}}$)"),
      y = "Prior Density",
      title = "Variance-Covariance Matrix Components"
    ) +
  p_lkj 

lkj_dens_plots
```


```{r lkj-sim}

# make dlkjcorr matrices
# extract rho for plotting
# sim items using dexp and multiplication

# this is much faster, dipshit
# rethinking::rlkjcorr(1000, K = 2, eta = 4) %>%
#   as.data.frame() %>% # for breaking-change stability
#   as_tibble()

sim_lkj_params <- rethinking::rlkjcorr(3000, K = 2, eta = 4) %>%
  as.data.frame() %>% # for breaking-change stability
  as_tibble(.name_repair = "check_unique") %>%
  select(rho = V2) %>%
  mutate(
    sd1 = rnorm(n = n(), mean = 0, sd = lkj_scale) %>% abs(),
    sd2 = rnorm(n = n(), mean = 0, sd = lkj_scale) %>% abs(),
    id = 1:n()
  ) %>%
  group_by(id) %>%
  mutate(
    cor_matrix = 
      map(rho, ~ matrix(data = c(1, .x, .x, 1), nrow = 2, ncol = 2)),
    vdiag = map2(sd1, sd2, ~ c(.x, .y) %>% diag() %>% as.matrix()),
    vc = map2(vdiag, cor_matrix, ~ .x %*% .y %*% .x),
    items = map(vc, 
      ~ mvtnorm::rmvnorm(
          n = 1, 
          mean = c(
            rnorm(1, mean = 0, sd = 1), 
            rnorm(1, mean = 0, sd = 1)
          ),
          sigma = .x
        ) %>% 
        as_tibble() %>%
        rename(cutpoint = V1, log_disc = V2)
    )
  ) %>%
  ungroup() %>%
  unnest(items) %>%
  print()

sim_lkj_params %>% pull(sd1) %>% mean()
# sim_lkj_params %>% pull(lkj)
# sim_lkj_params %>% pull(vdiag)
# sim_lkj_params %>% pull(vc)
# sim_lkj_params %>% pull(items)
```



```{r plot-item-scatter}
# point density
outer_point <- sim_lkj_params %$% 
  abs(max(max(cutpoint), max(log_disc))) %>%
  print()

p_item_scatter <- ggplot(sim_lkj_params) +
  aes(x = cutpoint, y = (log_disc)) +
  ggpointdensity::geom_pointdensity(
    size = 1
  ) +
  viridis::scale_color_viridis(option = "viridis") +
  labs(
    x = TeX("Cutpoint ( $\\tilde{\\kappa}_{j}$ )"),
    y = TeX("Log Discrimination ( log $\\tilde{\\iota}_{j}$ )"),
    title = "Unscaled Item Parameters",
    subtitle = "Simulated from joint prior",
    color = "Density (num. neighbors)"
  ) +
  coord_cartesian(
    ylim = c(-outer_point, outer_point),
    xlim = c(-outer_point, outer_point)
  ) +
  NULL

lkj_item_sim <- 
  p_item_scatter + guide_area() + plot_layout(guides = 'collect')

lkj_item_sim  
```

```{r plot-lkj}
lkj_dens_plots  / lkj_item_sim
```

<!------- TO DO ---------
- two panels: discrimination and inv(discrimination)
------------------------->

I build a prior for the variance-covariance matrix $\bm{\Sigma}$ by decomposing it into a diagonal matrix of scale terms and a correlation matrix. First I factor out the scale components.
\begin{align}
  \bm{\Sigma} &= 
  \begin{bmatrix}
    \sigma^{2}_{\tilde{\kappa}} & 
      \rho \sigma_{\tilde{\kappa}} \sigma_{\tilde{\iota}} \\
    \rho \sigma_{\tilde{\kappa}} \sigma_{\tilde{\iota}} & 
      \sigma^{2}_{\tilde{\iota}}
  \end{bmatrix}
  \\
  &= 
    \begin{bmatrix} 
      \sigma_{\tilde{\kappa}} & 0 \\
      0 & \sigma_{\tilde{\iota}}
    \end{bmatrix}
    \bm{S} 
    \begin{bmatrix} 
      \sigma_{\tilde{\kappa}} & 0 \\
      0 & \sigma_{\tilde{\iota}}
    \end{bmatrix}
\end{align}
The resulting matrix $\bm{S}$ is a $2 \times 2$ correlation matrix, meaning it has a unit diagonal and off-diagonal correlation terms (denoted $\rho$).
\begin{align}
  \bm{S} &=
    \begin{bmatrix}
      1 & \rho \\
      \rho & 1
    \end{bmatrix}
\end{align}
I then specify priors for the scale terms, $\sigma_{\tilde{\kappa}}$ and $\sigma_{\tilde{\iota}}$, and the correlation matrix $\bm{S}$ separately. This approach is also known as a "separation strategy" for covariance matrix priors [@barnard-mcculloch-meng:2000:covariance-separation].^[
  Although inverse-Wishart priors are often chosen for covariance matrices because they ensure conjugacy of the multivariate Normal distribution, recent work by Bayesian statisticians suggests that the separation strategy for covariance matrices is superior. The inverse-Wishart distribution has certain restrictive properties such as prior dependency between scales and correlations (large and small scales imply large and small correlations, respectively) that many Bayesian statisticians find undesirable compared to priors specified using the more flexible separation strategy [@alvarez-niemi-simpson:2014:covariance-prior; @akinc-vandebroek:2018:covariance-prior]. Furthermore, the conjugacy of the inverse-Wishart is irrelevant for this model because conjugacy does not provide the same computational benefit for Hamiltonian Monte Carlo samplers as it does for Gibbs samplers or analytic posterior computation.
]
The scale terms $\sigma_{\tilde{\kappa}}$ and $\sigma_{\tilde{\iota}}$ are given weakly informative $\text{Half-Normal}\left(0, 1\right)$ priors, which provide weak regularization toward zero but whose scale is wide enough that the data are likely to dominate the prior. I give $\bm{S}$ a prior from the LKJ distribution, which is a generalization of the Beta distribution defined over the space of symmetric, positive-definite, unit-diagonal matrices, such as a correlation matrix [@LKJ:2009:correlation-matrices].^[
  For a matrix $\bm{S}$ that follows an LKJ distribution with shape parameter $\eta$, the density of $\bm{S}$ is a function of its determinant: $\mathrm{LKJcorr}(\bm{S} \mid \eta) = c \times \mathrm{det}\left(\bm{S}\right)^{\eta - 1}$ with proportionality constant $c$ that depends on the dimensionality of $\bm{S}$.
]
\begin{align}
  \bm{S} &\sim \mathrm{LKJcorr}(\eta = 2)
\end{align}
The LKJ distribution has one shape parameter $\eta$, which can be interpreted like a shape parameter for a symmetric Beta distribution. Setting $\eta = 1$ yields a flat prior over all correlation matrices, where increasing values of $\eta1$ concentrate prior density toward the mode, which is an identity matrix. The chosen value of $\eta = 2$ provides weak regularization against extreme correlations near $-1$ and $+1$. Although it would have been sufficient to specify a prior for $\rho$ instead of the entire matrix $\bm{S}$, this convenience only arises in small (in this case, $2 \times 2$) correlation matrices. Larger matrices (such as those that would result from a more complex IRT model specification) would require explicit priors for a larger number of off-diagonal parameters. The LKJ prior can be generally applied to larger correlation matrices, so I choose it for the sake of building a more flexible and extensible model.

Figure&nbsp;\@ref(fig:plot-lkj) plots several details of the item prior. The top row shows the prior densities for the terms in the decomposed variance-covariance matrix $\bm{\Sigma}$. The left panel shows the Half-Normal prior density for the scale terms. The right panel shows the marginal distribution of $\rho$, the off-diagonal parameter in the matrix $\bm{S}$ that controls the covariance of items in the joint prior, generated from the LKJ correlation matrix prior. The bottom panel shows the distribution of item parameter values simulated from the multivariate Normal distribution implied by the joint hierarchical prior. Each point represents a simulated item as a combination of cutpoint values (on the horizontal axis) and log-discrimination values (on the vertical axis). Points are colored according to the number of nearby points, which informally conveys the prior density of items with particular cutpoint and discrimination values.

```{r plot-lkj, include = TRUE, fig.height = 6.5, fig.width = 6, out.width = "80%", fig.cap = "Components of the joint hierarchical prior for the unscaled item parameters. Top row shows prior densities for the parameters of the decomposed variance-covariance matrix, including the standard deviation terms (left) and the item correlation from the LKJ distribution (right). The bottom row shows prior values for unscaled items simulated from the joint prior."}
```

<!-- This is rnorm for coef priors

```{r gen-normal-priors, eval = FALSE}
normal_priors <- crossing(x = 1:1000000, sd = c(0.5, 1)) %>%
  group_by(sd) %>%
  mutate(
    v = rnorm(n(), mean = 0, sd)
  ) %>%
  mutate(sd_label = as.character(str_glue("Normal(0, {sd})"))) %>%
  print()
```

```{r plot-normal-priors, eval = FALSE}
ggplot(normal_priors) +
  aes(x = pnorm(v) - pnorm(0)) +
  facet_wrap(~ sd_label) +
  geom_histogram(
    boundary = 0, 
    fill = primary_light, color = "black"
  ) +
  labs(x = "Effect on Conservative Response Probability (Baseline = 0.5)", y = NULL) +
  scale_x_continuous(limits = c(-0.5, 0.5)) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  )
```

-->


#### Ideal Point Parameters

```{r joint-prior, eval = FALSE}
n_norm_sim <- 5000

sim_responses <- sim_lkj_params %$%
  tibble(
    cutpoint = cutpoint - mean(cutpoint),
    log_disc = log_disc - mean(log_disc)
  ) %>%
  mutate(
    discrimination = exp(log_disc),
    dispersion = 1 / discrimination,
    mu = rnorm(n = n(), mean = 0, sd = 1),
    beta = rnorm(n = n(), mean = 0, sd = 0.5),
    sigma = abs(rnorm(n = n(), mean = 0, sd = 1))
  ) %>%
  crossing(x = seq(0, 2, .5)) %>%
  mutate(
    theta = mu + beta * x,
    index = (theta - cutpoint) / sqrt(dispersion^2 + sigma^2),
    prob = pnorm(index)
  ) %>%
  print()

ggplot(sim_responses) +
  aes(x = dispersion) +
  geom_histogram(binwidth = .1) +
  coord_cartesian(xlim = c(0, 10))

ggplot(sim_responses) +
  aes(x = prob, y = as.factor(x)) +
  ggridges::geom_density_ridges(
    aes(height = ..count..),
    stat = "binline",
    position = "identity",
    boundary = 0,
    binwidth = .01,
    draw_baseline = FALSE
  )

ggplot(sim_responses) +
  aes(x = theta, y = prob) +
  facet_wrap(~ x) +
  ggpointdensity::geom_pointdensity() 

ggplot(sim_responses) +
  aes(x = x, y = prob) +
  ggpointdensity::geom_pointdensity() 

sim_responses %>%
  # filter(dispersion == sample(dispersion, 1)) %>%
  ggplot() +
  aes(x = theta, y = cutpoint) +
  facet_wrap(~ x) +
  geom_point(alpha = 0.5, aes(color = prob))
```

```{r, eval = FALSE}
tibble(x = seq(0, 10, .01)) %>%
  ggplot() +
  aes(x = x, y = dlnorm(x, meanlog = 1)) +
  geom_line()
```

For the hierarchical model that smooths group estimates, the model is parameterized to ease the specification of priors. First I standardize all covariates to have a mean of zero. This ensures that the constant $\mu_{p}$ in the hierarchical model for $\bar{\theta}_{g}$ (See Equation&nbsp;\@ref(eq:theta-g-hlm)) can be interpreted as a "grand mean" for party $p$, the average group ideal point for party $p$ when all covariates are at their means. I then give this grand mean a $\text{Normal}\left(0, 1 \right)$ prior, which implies a flat prior on the probability scale. Substantively this represents an assumption where the predicted probability of a conservative response for the typical item 

```{r, eval = FALSE}
# pass the items and HLM into the IRT model
sim_lkj_params %$%
  tibble(
    cutpoint = cutpoint - mean(cutpoint),
    dispersion = 1 / exp(log_disc - mean(log_disc))
  ) %>%
  mutate(r = row_number()) %>%
  crossing(theta = seq(-3, 3, .1)) %>%
  mutate(
    pp = pnorm((theta - cutpoint) / dispersion)
  ) %>%
  arrange(theta) %>%
  group_by(theta) %>%
  mutate(
    quantile = gtools::quantcut(pp)
  ) %>%
  ggplot() +
    aes(x = theta, y = pp) +
    geom_point(
      aes(color = quantile), show.legend = FALSE,
      alpha = 0.25,
      shape = 19,
      position = position_jitter(width = 0.05)
    ) +
    theme_void()


tibble(
  alpha = rnorm(n_norm_sim, mean = 0, sd = 1),
  beta = rnorm(n_norm_sim, mean = 0, sd = 1),
)
```

<!------- TO DO ---------
- come back
------------------------->

average Democratic constituency and the average Republican constituency "could be anything." Because the latent scale is identified by restricting the item parameters, the relaxed prior for the average ideal points prevents the ideal point priors from interfering with the identification of the scale.

```{r, eval = FALSE}
norm_sim <- 
  tibble(
    alpha = rnorm(n_norm_sim, mean = 0, sd = 1),
    beta = rnorm(n_norm_sim, mean = 0, sd = 1) 
  ) %>%
  mutate(r = row_number()) %>%
  crossing(x = seq(-3, 3, .001)) %>%
  mutate(
    link_hat = (beta * x) + alpha,
    p_hat = pnorm(link_hat)
  ) %>%
  print()

norm_sim %>% 
  filter(r == sample(r, 1000)) %>%
  ggplot() +
  aes(x = x, y = p_hat) +
  geom_line(aes(group = r), alpha = 0.25) +
  geom_vline(xintercept = seq(-2, 2, .5), color = "red")

norm_sim %>%
  filter(x %in% seq(0, 2, .5)) %>%
  # filter(beta > 0) %>%
  # filter(between(alpha, -.25, .25)) %>%
  ggplot() +
    aes(y = as.factor(x), x = p_hat) +
    ggridges::geom_density_ridges(
      aes(
        height = ..count..
        # fill = between(p_hat, quantile(p_hat, .05), quantile(p_hat, .95))
      ),
      stat = "binline",
      position = "identity",
      boundary = 0,
      binwidth = .01,
      draw_baseline = FALSE
    ) +
    theme(legend.position = "none") +
    # scale_fill_manual(values = c(secondary_light, primary_light)) +
    labs(
      title = TeX("$\\hat{\\pi}_{i} = \\Phi(\\alpha + \\beta x_{i})$"),
      subtitle = TeX("$\\alpha \\sim N(0, 1)$, $\\beta \\sim N(0, 1)$"),
      x = TeX("$\\hat{\\pi}_{i}$"),
      y = "At values of X = ..."
    )

norm_sim %>%
  filter(x %in% seq(0, 2, 1)) %>%
  ggplot() +
    aes(x = beta, y = p_hat) +
    ggpointdensity::geom_pointdensity(
      size = 1,
      method = "default"
    ) +
    facet_wrap(~ str_glue("X = {x}")) +
    labs(
      x = TeX("Hierarchical Coefficient Value: $\\beta_{p}$"),
      y = TeX("Conservative Response Probability: $\\bar{\\pi}_{gj}$"),
      color = "Density (num. neighbors)"
    ) +
    viridis::scale_color_viridis()


norm_sim %>%
  filter(x %in% seq(0, 2, .5)) %>%
  ggplot() +
    aes(x = p_hat) +
    geom_histogram() +
    facet_wrap(~ x)


norm_sim %>%
  filter(x == 1) %>%
  filter(beta >= 0) %>%
  mutate(inner = between(p_hat, quantile(p_hat, .05), quantile(p_hat, .95))) %>%
  ggplot() +
    aes(x = p_hat) +
    geom_histogram(
      boundary = 0, binwidth = 0.025,
      aes(fill = inner)
    )

norm_sim %>%
  arrange(beta) %>%
  print() %>%
  filter(between(beta, quantile(beta, .05), quantile(beta, .95)))

qnorm(p = .975, mean = 0, sd = 0.5) %>%
  pnorm(q = ., mean = 0, sd = 1)

```

I set priors for the coefficients in the hierarchical model by 

I give coefficients $\mathrm{Normal}\left(0, 0.5\right)$ priors. Substantively, this represents a prior where a typical draw, expected to be one standard deviation away from the mean, would change the probability of a conservative response from `r smart_number(pnorm(0), accuracy = .01)` to `r smart_number(pnorm(1), accuracy = .01)` (if above) or to `r smart_number(pnorm(-0.5), accuracy = .01)` (if below). Constants are given less informative $\mathrm{Normal}\left(0, 1\right)$ priors, whose density is rather flat when transformed from the link scale to the probability scale, as shown in Figure&nbsp;\@ref(fig:plot-normal-priors). Given that $95$ percent of the standard Normal distribution falls between the quantiles `r number(qnorm(0.025), accuracy = 0.01)` and `r number(qnorm(0.975), accuracy = 0.01)`, our priors should not give much weight to coefficients large enough to cause the response probability to leap from one end of that scale to the other.

<!-- ```{r plot-normal-priors, include = TRUE, fig.cap = "Histograms of priors draws for coefficients and constants in the hierarchical model for group means", fig.width = 6, fig.height = 3, out.width = "80%"} -->
<!-- ``` -->

<!------- TO DO ---------
- make this adaptive?
------------------------->

Within-group standard deviations, as well as the scale parameters in the non-centered error terms ($\tau$), are given $\mathrm{LogNormal}\left(0, 1\right)$ priors.

<!------- TO DO ---------
- scales go in here too
- $sigma_theta$
- $sigma_group, state, region$
- ------------------------->


### Dynamic Model {#sec:dynamic-model}

TBD.
