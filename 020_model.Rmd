# Modeling the Constituency's Policy Preferences {#ch:model}

<!-- bold math symbol -->
```{block, include = knitr::is_html_output(), cache = FALSE}
$\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}$
```



```{r knitr-02-model, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

```{r r-02-model, cache = FALSE}
library("knitr")
library("here")
library("magrittr")
library("tidyverse")
library("extrafont")
library("latex2exp")
```

To study how partisan constituencies are represented in primary elections, we require a measure of the partisan constituency's policy preferences. This chapter presents the statistical model that I use to estimate the policy ideal points of district-party publics. 

This chapter proceeds in three major steps. First, I review the theoretical basis for ideal point models, which can be traced to spatial models of policy choice from classic formal theory work in American political science such as @downs:1957:economic-theory. I connect these formal models to statistical models of policy ideal points [in a style that follows @clinton-jackman-rivers:2004:ideal] as well as their connection to Item Response Theory (IRT) models from psychometrics and education testing [e.g. @fox:2010:bayesian-IRT].

Second, I specify and test the group-level model that I build and employ in my analysis of district-party publics. This discussion includes details that are relevant to Bayesian estimation, including identification restrictions on the latent policy space, specification of prior distributions, and model parameterizations that expedite estimation with Markov chain Monte Carlo (MCMC). I begin with a static model for one time period, and then I describe a dynamic model that smooths estimates across time using hierarchical priors for model parameters [@caughey-warshaw:2015:DGIRT]. I test both the static and the dynamic models by fitting them to simulated data and determining how well they recover known parameter values.
<!------- TO DO ---------
- Prior checking
------------------------->

Lastly, I describe how I fit the model to real data. This section describes data collection, data processing, and model performance, and it includes a preview of the estimates before a descriptive analysis in Chapter&nbsp;\@ref(ch:descriptives).


<!------- TO DO ---------
- Should this include a comparison to CDW measures? Hill measures?
------------------------->



## Spatial Models and Ideal Points

<!-- goal: spatial >> IRT >> Parameterizations -->

Ideal points are constructs from *spatial* models of political choice. These models exist under formal theory---they simplify scenarios in the political world into sets of actors whose behaviors obey utility functions that conform to mathematical assumptions. Spatial models invoke a concept of "policy space," where possible policy outcomes are represented as locations along a number line or a multidimensional plane. A canonical example is a left-right continuum, where progressive or "liberal" policies occupy locations on the left side of the continuum, while conservative policies are on the right side. Depending on the scenario, actors of various types (voters, legislators, executives...) have to choose policy outcomes that result from their interactions with other players. Actors are at least partially motivated by their policy preferences, so they strive to achieve policy outcomes that they prefer. Sometimes these actors' face constrained choices---they can't achieve their most-preferred policy, so they settle on something that is as close as they can get.

```{r spatial-data}
dd <- tibble(
  Left = -1,
  Right = 1,
  Actor = 0.3
) 

long_dd <- dd %>%
  gather(key = label, value = location) %>%
  mutate(height = 0) %>%  
  print()

x_min <- -2
x_max <- 2
u_scale <- 2

utility <- tibble(
  x = seq(x_min, x_max, .01),
  utility_loss = u_scale * -(x - dd$Actor)^2,
  Right = u_scale * -(x - dd$Right)^2,
  Left = u_scale * -(x - dd$Left)^2
) %>%
  print(n = nrow(.))

```


```{r plot-space-1, out.width = "100%", fig.height = 2, fig.width = 8, include = TRUE, fig.cap = "An Actor and two policy outcomes (Left and Right) represented as locations in ideological/policy space"}
utility_plot <- ggplot(long_dd) +
  aes(x = location, y = height) +
  geom_point() +
  theme_void() +
  annotate(geom = "line", x = c(-2, 2), y = 0) +
  geom_text(aes(label = label), nudge_y = 1) +
  coord_cartesian(ylim = c(-3, 3)) 

utility_plot
```

Figure&nbsp;\@ref(fig:plot-space-1) plots a simple example of an Actor's choice over two policies in one-dimensional policy space. The "Left" outcome is a more progressive policy outcome than the "Right" outcome, indicated by their locations on the line. The Actor has a location herself, which corresponds to her most-desired policy outcome. There is no policy located exactly at the Actor's preferred location, but the Actor is closer to the Right policy than to the Left. Supposing that the Actor could make an error-free choice over which policy to implement, it appears she would prefer the Right outcome to the Left outcome.

Formal models are more careful to specify the assumptions governing these scenarios, which can be complicated in many cases. For example, suppose that locations along the left-right continuum can be assigned values on the real number line. Figure&nbsp;\@ref(fig:plot-space-1) shows a one-dimensional number line, but policies can be generally represented as locations in multidimensional $\mathbb{R}^{d}$ space. The Actor's location is synonymous with her "ideal point," her most-preferred policy. This is the point where the Actor's utility, in an economic utility model, is maximized with respect to policy considerations. Utility implies that the Actor has a utility function that is defined over the policy space; the Actor's realized utility depends on the distance between her ideal point and a potential policy outcome. Outcomes nearer to the Actor's ideal point are generally more preferred than farther outcomes, but this too is subject to assumptions about the shape of the Actor's utility function. Typically utility functions are assumed to be symmetric around an Actor's ideal point, so the "closer" policy is always more preferred, all else equal. The notion of an ideal point is similar to a "bliss point" in microeconomics: the quantity of a good consumed such that *more* consumption would result in *less* overall utility. Whether an Actor can choose the closest policy to herself depends on the structure of the game: the presence and strategy profiles of other Actors, the sequence of play, and the presence of other non-policy features of Actors' utility functions.

<!-- to do: $d$ cite -->
<!-- canonical cite for the or the notion of policy dimensions? -->

Formal models of ideal points are distinct from statistical models of ideal points. Formal models are primarily theoretical exercises; they explore the incentives and likely actions of Actors in specific choice contexts, building theoretical intuitions that can be applied in the study of real-world politics with real data. Statistical models, on the other hand, explicitly or implicitly *assume* a formal model as given and estimate its parameters using data. Data could come from legislators casting voting on bills, judges ruling on case outcomes, survey respondents stating their policy preferences (as in this project), and other situations. Researchers are typically interested in parameter estimates for the Actors' ideal points, although sometimes the parameters about the policy alternatives are substantively interesting.
<!------- TO DO ---------
- when are item params interesting  
------------------------->

<!------- TO DO ---------
- describe where ideal point â‰  ideology?
- ideal point is an operationalization of a nebulous, potentially high-dimensional concept called ideology?
- latent construct: none of the parameters are known, we have identifiability issues, and the model is still assumed!!
- this really is model-driven, we are assuming a model
------------------------->

Having distinguished formal and statistical models, I now show a derivation of a statistical model from a formal model. This exercise model will serve as a theoretical basis for the class of statistical models explored in this dissertation. I begin with notation to describe an arbitrary number of actors indexed $i \in \{1, \ldots, n\}$ making an arbitrary number of policy choices (bills, survey items, etc.) indexed $j \in \{1, \ldots, J\}$. Every Actor has an ideal point, or a location in the policy space, represented by $\theta_{i}$. Every task is choice between a Left policy located at $L_{j}$ and a Right policy located at $R_{j}$. 

The utility that an Actor receives from a Left or Right choice is a function of the distance between her ideal point and the respective choice location---utility is maximized if an Actor can choose a policy located exactly on her ideal point, and utility is "lost" for choices farther and farther from her ideal point. The functional form of utility loss is an assumption made by the researcher---some scholars assume that utility loss follows a Gaussian curve, while others choose a quadratic utility loss [@clinton-jackman-rivers:2004:ideal]. For this analysis, I assume a quadratic utility loss.^[
  Researchers typically avoid linear losses for technical reasons: a linear utility loss function is non-differentiable at the ideal point because function comes to a point. This prevents the researcher from using differential calculus to find a point of maximum utility.
]
<!------- TO DO ---------
- differentiable cite?
------------------------->

The choice of quadratic loss implies a utility function over the *squared distance* between an Actor and a choice location. The utility Actor $i$ receives from choosing Left or Right are given by utility functions $U_{i}\left(L_{j}\right)$ and $U_{i}\left(R_{j}\right)$, respectively. With quadratic utility loss, these utility functions take the form
\begin{align}
  \begin{split}
    U_{i}\left(R_{j}\right) = - \left( \theta_{i} - R_{j}\right)^{2} + e^{\mathtt{R}}_{ij} \\ 
    U_{i}\left(L_{j}\right) = - \left( \theta_{i} - L_{j}\right)^{2} + e^{\mathtt{L}}_{ij},
  \end{split}
  (\#eq:utility-fns)
\end{align}
where $e^{\mathtt{R}}_{ij}$ and $e^{\mathtt{L}}_{ij}$ are the idiosyncratic error terms for the Right and Left alternatives, respectively. I sometimes refer to the quadratic utility loss as the "deterministic" component of the Actor's utility function, while the idiosyncratic error terms are "stochastic" components.



```{r plot-utility}
utility_plot +
  geom_line(
    data = utility,
    aes(x = x, y = utility_loss)
  ) +
  theme(legend.position = "none") +
  annotate(geom = "linerange",
    x = dd$Right, ymax = 0, ymin = -u_scale*(dd$Actor - dd$Right)^2,
    linetype = "dashed"
  ) +
  annotate(geom = "linerange",
    x = dd$Left, ymax = 0, ymin = -u_scale*(dd$Actor - dd$Left)^2,
    linetype = "dashed"
  ) +
  geom_point() +
  NULL
```

<!-- break -->

With these utility functions laid out, Actor $i$'s decision can be a comparison of the utilities received by choosing Right or Left. Let $y_{ij}$ indicate the Actor's choice of Right or Left, where Right is coded $1$, and Left is coded $0$. The model so far implies that $y_{ij} = 1$ (Actor chooses Right) if their utility is greater for Right than for Left.
\begin{align}
  y_{ij} = 1 &\iff U_{i}\left(R_{j}\right) > U_{i}\left(L_{j}\right)
  (\#eq:choice-iff)
\end{align}
To visualize this choice, I represent the deterministic components of Equation&nbsp;\@ref(eq:choice-iff) in Figure&nbsp;\@ref(fig:plot-utility), omitting the stochastic utility terms. The parabola represents $i$'s fixed utility loss for any choice along the ideological continuum, owed to her distance from that choice. The vertex of the parabola is at the Actor's location, indicating that she would maximize her spatial utility if she could choose a policy located exactly at her ideal point. Dashed lines below the Left and Right alternatives represent the utility loss owed to the Actor's distance from those specific choices. In the current example, the Actor is closer to Right than to Left, so she receives greater utility (or, less utility *loss*) by choosing Right instead of Left.

```{r plot-utility, include = TRUE, fig.height = 2, fig.width = 8, out.width = "100%", fig.cap = "A representation of quadratic utility loss over policy choices"}
```

It is important to remember that Figure&nbsp;\@ref(fig:plot-utility) shows only the deterministic component of choice task $j$; random error components $e^{\mathtt{R}}_{ij}$ and $e^{\mathtt{L}}_{ij}$ are omitted. With idiosyncratic utility error incorporated, Equation&nbsp;\@ref(eq:choice-iff) implies that even though the Actor's distance to Right is smaller than her distance to Left, there is still a probability that $i$ chooses Left. This probability depends on the instantiated values of the idiosyncratic error terms for each choice. These error terms represent the accumulation of several possible, non-ideological shocks to utility: random misperceptions about the policy locations, domain-specific considerations about a choice that aren't summarized by ideology, and so on. Supposing that these idiosyncratic terms follow some probability distribution, Equation&nbsp;\@ref(eq:choice-iff) can be represented probabilistically:
\begin{align}
  \begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &= 
    \mathrm{Pr}\left(U_{i}\left(R_{j}\right) > U_{i}\left(L_{j}\right) \right) \\
  &=
    \mathrm{Pr}\left( 
      -\left( \theta_{i} - R_{j}\right)^{2} + e^{\mathtt{R}}_{ij} > 
      -\left( \theta_{i} - L_{j}\right)^{2} + e^{\mathtt{L}}_{ij}
    \right) \\
  &= \mathrm{Pr}\left(
      \left( \theta_{i} - L_{j}\right)^{2} -
            \left( \theta_{i} - R_{j}\right)^{2} 
        > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij}
      \right)
\end{split}
(\#eq:choice-prob)
\end{align}  
The intuition for Equation&nbsp;\@ref(eq:choice-prob) is that the Actor will choose the policy alternative that is nearest to her *unless* idiosyncratic (non-policy) factors overcome her ideological considerations. Supposing that the Actor is closer to Right than to Left, $\left( \theta_{i} - L_{j}\right)^{2}$ will be greater than $\left( \theta_{i} - R_{j}\right)^{2}$, capturing the Actor's deterministic inclination to prefer Right over Left. The only way for $i$ to choose Left would be if the idiosyncratic utility of Left over Right exceeded the Actor's deterministic inclinations.
  <!-- to do: assumes? -->
<!-- Note that the model so far assumes only that utility loss is quadratic over the ideological space and that the "coefficient" on the quadratic loss is equal for the distances to both Left and Right.  -->


Equation&nbsp;\@ref(eq:choice-prob) can be rearranged to reveal an appealing appealing functional form for $i$'s choice probability. First, expand the polynomial terms on the left side of the inequality...
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &= 
    \mathrm{Pr}\left(
           \left( \theta_{i} - L_{j}\right)^{2} -
           \left( \theta_{i} - R_{j}\right)^{2} 
           > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij} 
         \right) \\
  &= \mathrm{Pr}\left(
      \theta_{i}^{2} - 2\theta_{i}L_{j} + L_{j}^{2} 
      - \theta_{i}^{2} + 2\theta_{i}R_{j} - R_{j}^{2}
      > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij} 
    \right) \\
  &= \mathrm{Pr}\left(
      2\theta_{i}R_{j} - 2\theta_{i}L_{j} + 
      L_{j}^{2} - R_{j}^{2}
      > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij} 
    \right)
\end{split}
(\#eq:expand-utility)
\end{align}

From here, there are two factorizations that reveal convenient expressions for important constructs in the model.
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &=     
    \mathrm{Pr}\left(
      2\theta_{i}R_{j} - 2\theta_{i}L_{j} + 
      L_{j}^{2} - R_{j}^{2} 
      > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij}
    \right) \\
  &= 
    \mathrm{Pr}\left(
      2\theta_{i}R_{j} - 2\theta_{i}L_{j} + 
      \left(R_{j} - L_{j}\right)
      \left(R_{j} + L_{j}\right) 
      > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij} 
    \right) \\
  &= \mathrm{Pr}\left(
      2\left(R_{j} - L_{j}\right)
      \left(\theta_{i} - \frac{R_{j} + L_{j}}{2}\right)
      > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij} 
    \right)
\end{split}
(\#eq:factor-item-params)
\end{align}
The first manipulation is to decompose $L_{j}^{2} - R_{j}^{2}$ into the two factors $\left(R_{j} - L_{j}\right)\left(R_{j} + L_{j}\right)$. The second manipulation is to factor $2\left(R_{j} - L_{j}\right)$ out of the left-side of the inequality. We perform these manipulations because the resulting terms are appreciably more interpretable than before. First, note that $\frac{R_{j} + L_{j}}{2}$ is a formula for the midpoint between the Left and Right locations. This means that the expression $\theta_{i} - \frac{R_{j} + L_{j}}{2}$ intuitively conveys which policy alternative is closer to the Actor. If the Actor is closer to Right than to Left, $\theta_{i}$ will be greater than the midpoint, and vice versa if she were closer to Left. Second, the $2\left(R_{j} - L_{j}\right)$ term captures how far apart the policy alternatives are from one another, increasing as the distance between Right and Left increases. Together, the left side of the inequality succinctly describes the deterministic component of the Actor's ideological choice: is she closer to the Left or Right policy, and by how much?

The final manipulation is to simplify the terms above, which results in a convenient parameterization for statistical estimation.
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) 
  &= 
    \mathrm{Pr}\left(
      2\left(R_{j} - L_{j}\right)
      \left(\theta_{i} - \frac{R_{j} + L_{j}}{2}\right)
      > e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij} 
    \right) \\
  &= \mathrm{Pr}\left(
      \iota_{j}\left(\theta_{i} - \kappa_{j}\right)
      > \epsilon_{ij}
    \right),
\end{split}
(\#eq:choice-2p)
\end{align}
This results in the "discrimination parameter" $\iota_{j} = 2\left(R_{j} - L_{j}\right)$, the "midpoint" or "cutpoint" parameter $\kappa_{j} = \dfrac{R_{j} + L_{j}}{2}$, and a joint error term $\epsilon_{ij} = e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij}$.^[
  The names for these parameters are adapted from item-response theory (IRT), an area of psychometrics that is similarly interested in inferring latent traits from observed response data. I discuss the connection between this model and the IRT model in the next section.
]
Parameterizing the model in this way expresses the utility comparison in a simpler, linear form. Similar to Equation&nbsp;\@ref(eq:factor-item-params) above, $\theta_{i} - \kappa_{j}$ shows how far the Actor is from the midpoint between Left and Right, and $\iota_{j}$ captures behaves as a "slope" on this distance: the distance from the midpoint has a _stronger influence_ when the policy alternatives are farther from one another, since more utility is lost over larger spatial distances. I explore the intuitions of this functional form more thoroughly in the following section.

A complete statistical model is obtained by making a parametric assumption for the distribution of $\epsilon_{ij}$. Assuming that $\epsilon_{ij}$ is a draw from a standard Normal distribution,^[
  This implies that $\mathrm{E}\left(e^{\mathtt{L}}_{ij}\right) = \mathrm{E}\left(e^{\mathtt{R}}_{ij}\right)$ and that $\mathrm{Var}\left(e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij}\right) = 1$. 
  For a given choice $j$, imposing a scale restriction on the error variance is not problematic because the ideological scale is latent and can be arbitrarily stretched. Any non-unit variance for item $j$ can be compensated for by scaling the discrimination parameter $\iota_{j}$ (i.e.\ multiplying both sides of the inequality established in Equation&nbsp;\@ref(eq:choice-2p) by some scale factor). The important assumption is that the error variance of a choice $j$ is equal _across individuals_: $s_{ij} = s_{j}$ for all $i$.
]
Equation&nbsp;\@ref(eq:choice-2p) implies a probit regression model for the probability that Actor $i$ chooses Right on choice $j$:
\begin{align}
\begin{split}
  \mathrm{Pr}\left(y_{ij} = 1\right) &= 
  \mathrm{Pr}\left(
      \iota_{j}\left(\theta_{i} - \kappa_{j}\right)
      > \epsilon_{ij}
    \right) \\
  &= 
  \mathrm{Pr}\left(
    \iota_{j}\left(\theta_{i} - \kappa_{j}\right) - \epsilon_{ij} > 0 
  \right) \\
  &= 
  \Phi\left( \iota_{j}\left(\theta_{i} - \kappa_{j}\right) \right),
\end{split}
(\#eq:logit-irt)
\end{align}
where $\Phi(\cdot)$ is the cumulative Normal distribution function. Many IRT models assume that $\epsilon_{ij}$ follows a standard Logistic distribution, [for example @londregan:1999:ideal-pts], resulting in a logistic regression model rather than a probit model.^[
  A technical point of difference between the probit and logit model is the way parameters are scaled to yield the final line of Equation&nbsp;\@ref(eq:choice-2p). If it is assumed that $\epsilon_{ij}$ is a Logistic draw with scale $s_j$, this implies that 
  $\mathrm{Var}\left(e^{\mathtt{L}}_{ij} - e^{\mathtt{R}}_{ij}\right) = \dfrac{s_{j}^{2}\pi^{2}}{3}$, where it is assumed that $s_{j} = 1$ for the standard Logistic model. 
]
As I show below, the probit model facilitates the group-level model much more easily than the logit model.

<!------- TO DO ---------
- Does londregan prefer logit due to some identifiability concern???
- cite CW/Fox
------------------------->



### The "Item Response Theory" Approach to Survey Response

Scholars of ideal point models have noted their similarity to models developed under item response theory (IRT) in psychometrics [for example, @londregan:1999:ideal-pts]. IRT models have a similar mission as ideal point models: measuring latent features in the data given individuals' response patterns to various stimuli. The canonical psychometric example is in education testing, where a series of test questions is used to measure a student's latent academic "ability" level. This section connects ideal point models to IRT in order to explain their important theoretical and mathematical intuitions.

#### Latent Traits

The first important feature to note about IRT models is that they are *measurement models*. The goal of a measurement model is to use observed data $\mathbf{y}$ to estimate some construct of theoretical interest $\bm{\theta}$, supposing that there is a distinction between the two. The observed data $\mathbf{y}$ are affected by $bm{\theta}$, but there is no guarantee of a one-to-one correspondence between the two because $\bm{\theta}$ is not directly observed. We can represent a measurement model with general notation $\mathbf{y} = f(\bm{\theta}, \bm{\sigma})$, where $\bm{\sigma}$ represents some vector of auxiliary model parameters to be estimated in addition to $\bm{\theta}$ by fitting the model to observed data.

In an educational testing context, students take standardized tests intended to measure their academic "ability" levels. Analysts who score the tests cannot observe a student's ability directly---it isn't clear how that would be possible. They do, however, observe the student's answers to known test questions. IRT models provides a structure to infer abilities from the student's pattern of test answers. The context of policy choice is similar. It is impossible to observe any individual's political ideology directly, but we theorize that it affects their responses to survey items about policy choices. The IRT setup lets us summarize an individual's policy preferences by analyzing the structure of their responses to various policy choices.

It is crucial to note that the only way to estimate a latent construct from observed data is for the model to impose assumptions about the functional relationship between the latent construct and the observed data. In this sense, the estimates can be sensitive to the model's assumptions. While this is always important to acknowledge, it is also valuable to note that model-dependence is an ever-present consideration even for simpler measurement strategies, such as an additive index that sums or averages across a battery of variables. In fact, additive indices can be written as limiting cases of measurement model where key parameters are assumed to be known and fixed. In this way, measurement models *relax* the assumptions of simpler measurement strategies, even if the underlying mathematics are more intensive. The impulse to view measurement models as stacking "additional" assumptions atop a simpler measurement strategy is generally backward.

<!------- TO DO ---------
- Literature about "ideology" (Clinton piece?)
- "traits"
------------------------->

#### Item Characteristics and Item Parameters


```{r example-icc-data}
# create example ICCs/IRFs
# for some number of simulated items
ex_items <- 5
fix_cut <- 0
fix_disc <- 1

# item params are random but seeded
set.seed(2019)

# ability-only, 1p, 2p models
item_params_0p <- 
  tibble(
    cutpoint = fix_cut,
    discrimination = fix_disc
  ) %>%
  print()

item_params_1p <- 
  tibble(
    cutpoint = rnorm(ex_items, fix_cut, 1.5),
    discrimination = fix_disc
  ) %>%
  print()

item_params_2p <- 
  tibble(
    cutpoint = rnorm(ex_items, 0, 1.5),
    discrimination = rnorm(ex_items, fix_disc, 0.5)
  ) %>%
  print()


# table of item params
ex_item_params <- 
  bind_rows(
    .id = "model",
    "Ability Only: No Item Variation" = item_params_0p,
    "Varying Item Difficulty" = item_params_1p,
    "Varying Difficulty & Discrimination" = item_params_2p 
  ) %>%
  mutate(
    item = 1:n(),
    model = fct_relevel(
      model, "Ability Only: No Item Variation", "Varying Item Difficulty"
    )
  ) %>%
  print()

# table of IRF data
ex_iccs <- ex_item_params %>%
  crossing(theta = seq(-4, 4, .01)) %>%
  mutate(
    eta = discrimination * (theta - cutpoint), 
    prob = plogis(eta)
  ) %>%
  print()
```

Measurement models are meant to relax assumptions about the data's functional dependence on the construct of interest. Item response theory focuses this effort on the items to which subjects respond. Different items may reveal different information about the latent construct; the design of the model governs how those item differences can manifest [see @fox:2010:bayesian-IRT for a comprehensive review of IRT modeling].

Consider a simple model where a student $i$ is more likely to answer test questions $j$ correctly if she has greater academic ability $\theta_{i}$. Analogously, a citizen who is more conservative is more likely to voice conservative preferences for policy question $j$. Keeping the logistic functional form from above, we can represent this simple model with the equation:
\begin{align}
  \mathrm{Pr}\left( y_{ij} = 1 \right) &= 
  \Phi\left(\theta_{i}\right),
  (\#eq:ability-only)
\end{align}
where $\theta_{i}$ is scaled such that the probability of a correct/conservative response is $0.5$ at $\theta_{i} = 0$. This model makes the implicit assumption that knowing $\theta_{i}$ is sufficient to produce exchangeable response data; there are no systematic differences in the difficulty level of the test questions or the ideological nature of the policy choices that would affect the propensity of subjects to answer correctly/conservatively on average. This implicit assumption is obviously unrealistic. Just as some test questions are naturally more difficult than others, some policy questions present more extreme or lopsided choices than others, leading citizens with otherwise equivalent $\theta$ values to vary systematically in their response probability across items. Although the "ability-only" model seems ridiculous when posed as such, political science is replete with additive measurement scales that imply the same basic structure that omits item-level variation: indices of policy views, the racial resentment scale, survey-based scales of political participation, and more.


Rather than assume that all items behave identically for all individuals, IRT explicitly models the systematic variation at the item level using _item parameters._ IRT models have different behaviors based on the parameterization of the item effects in the model. The simplest IRT model is the "one-parameter" model, which includes an item-specific intercept $\kappa_{j}$.
\begin{align}
  \mathrm{Pr}\left( y_{ij} = 1 \right) 
  &= \Phi\left(\theta_{i} - \kappa_{j}\right)
  (\#eq:1p-ogive)
\end{align}
IRT parlance refers to the $\kappa_{j}$ parameter as the item "difficulty" parameter. In the testing context, if a student has higher ability than the difficulty of the question, the probability that they answer the test item correctly is greater than $0.5$. This probability goes up for students with greater ability relative to item difficulty, and it goes down for items with greater difficulty relative to student ability. In a policy choice context, the difficulty parameter is better understood as the "cutpoint" parameter, the midpoint between two policy choices where the respondent is indifferent between the choice of Left or Right on item $j$. These cutpoints are allowed to vary from item to item; some policy choices present alternatives that are, on average, more conservative or liberal than others. For instance, the choice of *how much* to cut capital gains taxes will have a more conservative cutpoint than a question of whether to cut capital gains taxes at all. If there were no systematic differences across items, it would be the case that $\kappa_{j} = 0$ for all $j$, and the one-parameter model would reduce to the simpler model in Equation&nbsp;\@ref(eq:ability-only).

The "two-parameter" IRT model is more common, especially in the ideal point context. The two-parameter model introduces the "discrimination" parameter $\iota_{j}$, which behaves as a slope on the difference between $\theta_{i}$ and $\kappa_{j}$. 
\begin{align}
  \mathrm{Pr}\left( y_{ij} = 1 \right)
  &= 
  \Phi
  \left( \iota_{j}\left(\theta_{i} - \kappa_{j}\right) \right),
  (\#eq:2p-ogive)
\end{align}
Intuitively, the discrimination parameter captures how well a test item differentiates between the responses of high- and low-ability students, with greater values meaning more divergence in responses. In the ideal point context, it captures how strongly a policy question divides liberal and conservative respondents.^[
  Two-parameter IRT models are sometimes written with $\iota_{j}$ is distributed through the equation: $\iota_{i}\theta_{i} + \alpha_{j}$, where $\alpha_{j} = \iota_{j}\kappa_{j}$. Although this parameterization more closely follows a linear slope-intercept equation, it loses the appealing interpretation of $\kappa_{j}$ as the midpoint between policy choices.
]


```{r plot-example-iccs, include = TRUE, echo = FALSE, fig.cap = "Examples of item characteristic curves under different item parameter assumptions", fig.width = 9, fig.height = 4, out.width = "100%"}
ggplot(ex_iccs) +
  aes(x = theta, y = prob) +
  facet_wrap(~ model) +
  geom_point(data = ex_item_params, aes(x = cutpoint, y = 0.5),
             size = 1.5) +
  geom_line(aes(group = as.factor(item)), show.legend = FALSE) +
  scale_y_continuous(breaks = c(0, .5, 1)) +
  scale_x_continuous(
    breaks = c(-3, 0, 3), 
    labels = c("Low", "Med", "High")
  ) +
  # scale_color_brewer(palette = "Set2") + 
  labs(
    x = TeX("Ability/Conservatism $(\\theta_{\\mathit{i}})$"), 
    y = "Probability of\nCorrect/Conservative Response"
  ) +
  NULL
```

Figure&nbsp;\@ref(fig:plot-example-iccs) shows how response probabilities are affected by the parameterization of item effects. Each panel plots how increases in subject ability or conservatism (the horizontal axis) result in increased response probability (the vertical axis), where the shape of the curve is set by values of the item parameters. These curves are commonly referred to as *item characteristic curves* (ICCs) or *item response functions* (IRFs). The leftmost panel shows a model with no item effects whatsoever; any item is theorized to behave identically to any other item, and response probabilities are affected only by the subject's ability (ideology). The middle panel shows a one-parameter model where item difficulties (cutpoints) are allowed to vary systematically at the item level. Difficulty parameters behave as intercept shifts, so they convey which value of $\theta$ yields a correct response with probability $0.5$, but they do not affect the *elasticity* of the item response function to changes in $\theta$. The final panel shows item response functions from the two-parameter IRT model, where item difficulties (intercepts) and discriminations (slopes) are allowed to vary across items.

<!-- IRT model parameters -->
#### IRT Interpretation of the Ideal Point Model

How do we interpret our statistical model of ideal points in light of item response theory? Recall the statistical model that we derived from the utility model above. An Actor $i$ faces policy question $j$, with a Right alternative located at $R_{j}$ and a Left alternative located at $L_{j}$. The Actor chooses the alternative closest to her ideal point $\theta_{i}$, subject to idiosyncratic utility shocks summarized by $\epsilon_{ij}$. Letting $y_{ij}$ indicate the outcome that Actor $i$ chooses the Right position on policy question $j$, the probability that $y_{ij} = 1$ is given by
\begin{align}
  \mathrm{Pr}\left(y_{ij} = 1\right) &= 
    \Phi\left( 
      \iota_{j}\left(\theta_{i} - \kappa_{j}\right) 
    \right)
\end{align}
where $\kappa_{j}$ represents the item cutpoint and $\iota_{j}$ represents the item discrimination.

The behavior of the item parameters can be understood by remembering that they are functions of the Left and Right choice locations. For instance, the cutpoint parameter $\kappa_{j}$ represents an intercept shift for an items response function and is equal to $\dfrac{R_{j} + L_{j}}{2}$. Suppose that $\theta_{i} - \kappa_{j} = 0$, which occurs if the item cutpoint falls directly on an Actor's ideal point. In such a case, the Actor would be indifferent (in expectation) to the choice of Left or Right, and the probability of choosing Right would collapse to $0.5$.^[
  This holds in logit and probit models, since $\mathrm{logit}^{-1}(0) = 0.5$ and $\Phi(0) = 0.5$.
]
The value of $\kappa_{j}$ increases by moving either the Right or Left alternatives to the right (increasing $R_{j}$ or $L_{j}$), subject to the constraint that $R_{j} \geq L_{j}$. Larger values of the item cutpoint imply a lower probability that the Actor chooses Right, since $\kappa_{j}$ has a non-positive effect on the conservative response probability.^[
  Formally we can show this by taking the derivative of the link function with respect to the cutpoint: $\dfrac{\partial \iota_{j}\left(\theta_{i} - \kappa_{j}\right)}{\partial \kappa_{j}} = -\iota_{j}$, where $\iota_{j}$ is constrained to be greater than or equal to zero
]
The opposite intuition holds as the Left position becomes increasingly progressive, resulting in larger values of $\kappa_{j}$ that imply a higher probability of choosing Right, all else equal.

The discrimination parameter behaves as a "coefficient" on the distance between the Actor ideal point and the cutpoint, meaning that the Actor's choice is more elastic to her policy preferences as $\iota_{j}$ increases.^[
  Again we can demonstrate this by noticing that the derivative of the link function with respect to the discrimination parameter is $\dfrac{\partial \iota_{j}\left(\theta_{i} - \kappa_{j}\right)}{\partial \iota_{j}} = \left(\theta_{i} - \kappa_{j}\right)$. The derivative's magnitude depends on the absolute value of this distance, and its sign depends on the sign of the difference.
]
Because $\iota_{j} = 2(R_{j} - L_{j})$, the discrimination parameter grows when the distance between the Right and Left alternatives grows larger, which happens when $R_{j}$ increases or $L_{j}$ decreases.

In a special case that Right and Left alternatives are located in exactly the same location, the result is $\kappa_{j} = \iota_{j} = 0$, leading all Actors to choose Right with probability $0.5$. This result represents a situation where policy preferences are not systematically related to the choice whatsoever, and only idiosyncratic error affects the choice of Right or Left. Although the model implies that this result is *mathematically* possible, it is not realistic to expect any of the policy choices in this project to induce this behavior.

<!------- TO DO ---------
- I made this plot of the item params as f(bill locations)
- I don't want to delete the code but it currently isn't doing anything
------------------------->

```{r item-derivs, eval = FALSE}
item_grid <- 
  tibble(
    right = seq(-3, 3, .01),
    left = seq(-3, 3, .01)
  ) %>% 
  expand(right, left) %>%
  filter(right > left) %>%
  mutate(
    Midpoint = 0.5 * (right + left),
    Discrimination = 2 * (right - left)
  ) %>%
  gather(key = param, value = value, -right, -left) %>%
  print()
```

```{r plot-item-derivs, eval = FALSE, include = FALSE, fig.cap = "Item parameters as functions of choice locations", fig.height = 3, fig.width = 8, out.width = "100%"}
grad <- ggplot(item_grid) +
  aes(x = right, y = left, z = value) +
  facet_wrap(~ fct_rev(param)) +
  viridis::scale_fill_viridis(option = "inferno") +
  coord_fixed() +
  labs(
    x = TeX("Right Location: $\R_j$"),
    y = TeX("Left Location: $\L_j$"),
    fill = "Parameter\nValue"
  )

grad + 
  geom_tile(aes(fill = value)) +
  stat_contour(bins = 10, color = "white") + 
  NULL
```




<!-- parametric assumptions -->
<!-- Up to this point, we have assumed symmetric and equally-weighted quadratic utility loss over ideological distances. No parametric assumptions have yet been made about the idiosyncratic errors or the difference in errors.  -->



### Applications of the IRT Approach

A section reviewing IRT in political science:

- ideal points (Poole and Rosenthal, CJR, Londregan, Jeff Lewis, Michael Bailey, Martin and Quinn)
- citizen ideology with survey data (Seth Hill multinomial and individual, Tausanovitch and Warshaw small area,  Caughey and Warshaw groups within areas)
- other latent modeling [@levendusky-et-al:2008-latent-partisanship]

<!-- ### Statistical Estimation of Ideal Points -->

<!-- Relation to additive indices (ability-only models) -->



## Modeling Party-Public Ideology in Congressional Districts

This section outlines my group-level ideal point model for party publics. It begins by describing the connection between the individual-level IRT model and the group-level model and its implication for the parameterization of the model (Section&nbsp;\@ref(sec:how-to-group)). I then lay out the hierarchical model for party-public ideal points in its static form (Section&nbsp;\@ref(sec:geographic-model)) and its dynamic form (Section&nbsp;\@ref(sec:dynamic-model)). I discuss technical features of model implementation, including choices for model parameterization,
model identification, 
prior distributions, 
and model testing methods such as prior predictive checks and posterior predictive checks.
<!-- (Section&nbsp;\@ref(sec:priors-identification))  -->
<!-- (Section&nbsp;\@ref(sec:stan-setup)). -->


<!-- ### Static Model -->


### Group-Level IRT Model {#sec:how-to-group}

<!------- TO DO ---------
- clean this up
------------------------->

So far we have modeled individual responses to policy items according to their own individual ideal points. In the group model, we assume that individual ideal points are distributed within a group $g$, where groups are define as the intersection of Congressional districts $d$ and political party affiliations $p$.

As before, we observe a binary response from individual $i$ to item $j$, which we regard as a probabilistic conservative response with probability $\pi_{ij}$, which is given a probit model.
\begin{align}
  y_{ij} &\sim \mathrm{Bernoulli}(\pi_{ij}) \\
  \pi_{ij} 
  &= 
  \Phi\left(\iota_{j}\left( \theta_{i} - \kappa_{j} \right)\right)
  (\#eq:y-i-probit)
\end{align}

<!------- TO DO ---------
- Is this Fox? or Mislevy?
------------------------->

Following @fox:2010:bayesian-IRT and @caughey-warshaw:2015:DGIRT, it is helpful to reparameterize the IRT model to accommodate a group-level extension. Let $\sigma_{j} = \iota^{-1}_{j}$, referring to this new parameter as the item "dispersion" parameter. @caughey-warshaw:2015:DGIRT describe the dispersion parameter as introducing additional "measurement error" in $\pi_{ij}$ beyond the Normally distributed utility error from $\epsilon_{ij}$ above.
\begin{align}
  \pi_{ij} 
  &= 
  \Phi\left(\frac{\theta_{i} - \kappa_{j}}{\sigma_{j}}\right)
  (\#eq:y-i-irt)
\end{align}

The group model begins with the notion that there is a probability distribution of ideal points within a group $g$, where a "group" is a partisan constituency within a Congressional district. Supposing that deviations from the group mean are realized by the accumulation of a large number of random forces, we can represent an individual ideal point as a Normal draw from the group,^[
  Notation for Normal distributions will always describe the scale parameter in terms of standard deviation $\sigma$ instead of variance $\sigma^2$. This keeps the notation consistent with the way Normal distributions are expressed in Stan code.
]
\begin{align}
  \theta_{i} &\sim 
    \mathrm{Normal}\left( \bar{\theta}_{g[i]}, \sigma_{g[i]} \right)
  (\#eq:theta-i-draw)
\end{align}
where $\bar{\theta}_{g[i]}$ and $\sigma_{g[i]}$ are the mean and standard deviation of ideal points within $i$'s group $g$. 

While it is possible to continue building the model hierarchically from \@ref(eq:theta-i-draw), it would be far too computationally expensive to estimate every individual's ideal point in additional to the group-level parameters---every individual ideal point is essentially a nuisance parameter. Instead, we rewrite the model by aggregating individual-level survey response data to the group level, expressing the grouped outcome data as a function of the group parameters. Let $s_{gj} = \sum\limits_{i \in g}^{n_{gj}} y_{ij}$, the number of conservative responses from group $g$ to item $j$, where $n_{gj}$ is the total number of responses (trials) of $j$ within group $g$. Supposing these trials were collected independently across groups and items, we could model the grouped outcome as a binomial random variable,
\begin{align}
  \begin{split}
  s_{gj} &\sim 
      \mathrm{Binomial}\left(n_{gj}, \pi_{gj}\right) \\
    \pi_{gj} &=
      \Phi\left(
        \frac{\bar{\theta}_{g} - \kappa_{j}}{
              \sqrt{ \sigma_{g}^{2} + \sigma^{2}_{j}}}
      \right),
  \end{split} %_ 
  (\#eq:y-g-irt)
\end{align}
where $\pi_{gj}$ is the probability that a randomly selected individual from group $g$ gives a conservative response to item $j$. Our uncertainty about any random individual's ideal point relative to the group mean is included in the model as group-level variance term. If individual ideal points are Normal within their group, this within-group variance can simply be added to the probit model as another source of measurement error, with larger within-group variances further attenuating $\pi_{ij}$ toward $0.5$. @caughey-warshaw:2015:DGIRT derive this result in the supplementary appendix to their article.

<!------- TO DO ---------
- should I just rewrite it in my own style?
------------------------->

The current setup assumes that every item response is independent, conditional on the group and the item. This assumption is violated if the same individuals in a group answer multiple items---one individual who answers `r smart_number(20)` items is less informative about the group average than `r smart_number(20)` who answer one item apiece. While this too could be addressed by explicitly modeling each individual's ideal point (extending the model directly from Equation&nbsp;\@ref(eq:theta-i-draw)), I implement a weighting routine that downweights information from repeated-subject observations while adjusting for nonrepresentative sample design, which I describe in Section&nbsp;\@ref(sec:model-weights).

<!------- TO DO ---------
- I fix the rounding components here with my likelihood fix.
------------------------->


### Hierarchical Model for Group Parameters {#sec:geographic-model}

The group model described so far can be estimated straightforwardly if there are enough responses from enough individuals in enough district-party groups. In practice, however, a single survey will not contain a representative sample of all Congressional districts, and certainty not a representative sample of partisans-within-districts. I specify a hierarchical model for the group parameters in order to stabilize the estimates in a principled way. The hierarchical models learns how group ideal points are related to cross-sectional (and eventually, over-time) 
<!------- TO DO ---------
- dynamics  
------------------------->
variation in key covariates, borrowing strength from data-rich groups to stabilize estimates for data-sparse groups, and even imputing estimates for groups with no survey data at all. This section describes the multilevel structure using traditional notation for hierarchical models; later in Section&nbsp;\@ref(sec:stan-setup) I describe how I parameterize the model for improved estimation in Stan.

<!------- TO DO ---------
- don't forget the fact that CW aren't non-centered?
------------------------->

I posit a hierarchical structure where groups $g$ are "cross-classified" within districts $d$ and parties $p$. This means that groups are nested within districts and within parties, but districts and parties have no nesting relationship to one another. Districts are further nested within states $s$. I represent this notationally by referring to group $g$'s district as $d[g]$, or the $g$^th^ value of the vector $\mathbf{d}$. Similarly, $g$'s party is $p[g]$. For higher levels such as $g$'s state, I write $s[g]$ as shorthand for the more-specific but more-tedious $s[d[g]]$.

I use this hierarchical structure to model the probability distribution of $\bar{\theta}_{g}$. I consider the group ideal point as a Normal draw from the distribution of groups whose hypermean is predicted by a regression on geographic-level data with parameters that are indexed by political party. This regression takes the form
\begin{align}
  \bar{\theta}_{g} &\sim \mathrm{Normal}\left(
    \mu_{0p[g]} + \mathbf{x}^{\intercal}_{d[g]}\beta_{p[g]} + \alpha^{\mathtt{state}}_{s[g]p[g]},
    \sigma^{\mathtt{group}}_{p} %_
  \right) 
  (\#eq:theta-g-hlm)
\end{align}
where $\mu_{0p[g]}$ is a constant specific to party $p$,^[
  Or "grand mean," since all covariates are eventually be centered at their means.
]
$\mathbf{x}_{d}$ is a vector of Congressional district-level covariates with party-specific coefficients $\beta_{p}$. State effects $\alpha^{\mathtt{state}}_{sp}$ are also specific to each party. The benefit of specifying separate parameters for each party is that geographic features (such as racial composition, income inequality, and so on) may be related to ideology in ways that are not identical across all parties. This is an important departure from the structure laid out by @caughey-warshaw:2015:DGIRT, which estimates the same set of geographic effects for all groups in the data.

<!-- We use a similar hierarchical regression for group scales (suppressing the $g$ subscript which is implied by the combination of $d$ and $p$).
\begin{align}
  \log \left( \sigma_{g} \right) 
  &\sim 
  \mathrm{Normal}\left( 
    \delta_{0p} + X_{d}\delta_{p} + \eta_{sp}, 
    \psi_{\sigma}
  \right),
  (\#eq:sigma-g-draw)
\end{align}
where $\delta_{0p}$ represents constants for each party, $\delta_{p}$ is a party-specific vector of coefficients on district features, and $\eta_{sp}$ are party-specific state effects. -->

The state effects are regressions on state features as well,
\begin{align}
\begin{split}
  \alpha^{\mathtt{state}}_{sp} 
  &\sim 
  \mathrm{Normal}\left(\mathbf{z}^{\intercal}_{s}\gamma_{p} + \alpha^{\mathtt{region}}_{r[s]p}, \sigma^{\mathtt{state}}_{p}\right), 
%_
% \\
% \eta_{sp} &\sim 
% \mathrm{Normal}\left(\mathbf{z}^{\intercal}_{s}\zeta_{p} + L_{r[s]p}, \psi_{\eta}\right),
\end{split}
(\#eq:state-effects-draws)
\end{align}
where state-level covariates $\mathbf{z}_{s}$ have party-specific coefficients $\gamma_{p}$.
<!-- (for group means) or $\zeta_{p}$ (for group scales).  -->
Each state effect is a function of a party-specific region effect $\alpha^{\mathtt{region}}_{[s]rp}$
<!-- (for group means) and $L_{rp}$ (for group scales)  -->
for Census regions indexed $r$, which is a modeled mean-zero effect to capture correlation within regions.
\begin{align}
  \alpha^{\mathtt{region}}_{rp} 
  &\sim
  \mathrm{Normal}\left(0, \sigma^{\mathtt{region}}_{p}\right) %_
(\#eq:region-effects-draws)
\end{align}


### Weighted Outcome Data {#sec:model-weights}

The group-level model learns about group ideal points by surveying individuals within groups, but the model currently assumes that all $y_{gj}$ are independent conditional on the item. If the same individuals answer multiple items, this assumption is violated. Additionally, we cannot assume that responses are independent in the presence of nonrepresentative survey designs. This section describes an approach for weighting group-level data that adjusts for both issues. The corrections are lifted from @caughey-warshaw:2015:DGIRT, but I am able to relax the constraint that resulting data must be integer valued.

<!-- The recipe contains three adjustments: a "design effect" to account for nonequal response probabilities within a given group $g$, an adjusted sample size for every group-item combination, and an adjusted number of successes for each group-item.  -->

<!------- TO DO ---------
- sum over i within g and j? should someone who answers 10 questions be 1/10 or 1+change? I don't get it.
- Talk to Alex
------------------------->

First, the sample size in each group-item "cell" $gj$ must is adjusted for survey design and multiple responses per individual. Let $n_{g[i]j}^{*}$ be the adjusted sample size for $i$'s group-item cell, defined as
\begin{align}
  n^{*}_{g[i]j} &=
  \sum\limits_{i = 1}^{n_{g[i]j}} \frac{1}{r_{i}d_{g[i]}},
  (\#eq:wted-n)
\end{align}
where $r_{i}$ is the number of responses given by individual $i$, and $d_{g[i]}$ is a survey design correction for $i$'s group. The effective sample size decreases when respondents answer multiple questions ($r_{i} > 1$) or in the presence of a sample design correction ($d_{g} > 1$). The design correction, originally specified by @ghitza-gelman:2013:subgroup-mrp, penalizes information collected from groups that contain greater variation in their survey design weights. It is defined as
\begin{align}
  d_{g[i]} &= 
    1 + \left(\frac{\mathrm{sd}_{g[i]}\left(w_{i}\right)}{
                    \mathrm{mean}_{g[i]}\left(w_{i} \right)} 
        \right)^{2},
  (\#eq:design-effect)
\end{align}
where $\mathrm{sd}(\cdot)$ and $\mathrm{mean}(\cdot)$ are the within-group standard deviation and mean of respondent weights $w_{i}$. If all weights within a cell are identical, their standard deviation will be $0$, resulting in a design correction equal to $1$ (meaning, no correction). Larger within-cell variation in weights increases the value of $d_{g}$, thus decreasing the effective sample size within a cell. The intuition of this correction is to account for increased variance of weighted statistics compared to unweighted statistics, given a fixed number of observations [@ghitza-gelman:2013:subgroup-mrp, 765].

To obtain the weighted number of successes in cell $gj$, I multiply the cell's weighted sample size by its weighted mean. The weighted mean $\bar{y}^{*}_{gj}$ adjusts for the respondent's survey weight $w_{i}$ and number of responses $r_{i}$ and is defined as
\begin{align}
  \bar{y}^{*}_{g[i]j} &=
  \dfrac{
    \sum\limits_{i = 1}^{n_{g[i]j}}
    \frac{w_{i}y_{ij}}{r_{i}}
  }{
    \sum\limits_{i = 1}^{n_{g[i]j}}
    \frac{w_{i}}{r_{i}}
  }.
  (\#eq:wted-y)
\end{align}
The weighted number of successes in each cell, in turn, is 
\begin{align}
  s^{*}_{gj} &= \mathrm{min}\left(n^{*}_{gj}\bar{y}^{*}_{gj}, n^{*}_{gj}\right). %*
  (\#eq:wted-s)
\end{align}
where I take the minimum to ensure that the number of successes does not exceed the adjusted sample size. 

It is likely that many values of $n^{*}_{gj}$ and $s^{*}_{gj}$ will be non-integers. Ordinarily this would be a problem for estimation in Stan, since Stan's Binomial likelihood function will not accept floating-point arguments. @caughey-warshaw:2015:DGIRT ensure integer data with additional rounding steps during reweighting. Instead of rounding, I write a custom Binomial likelihood function in Stan that returns log-probabilities for weighted data, which I describe below.


### Model Implementation in Stan {#sec:stan-setup}

The model is estimated with Stan, a programming language for high-performance Bayesian analysis that extends and interfaces with `C++` [@carpenter-at-al:2016:stan]. Although it possible to estimate Stan models using front-end software packages such as `brms` [@burkner:2017:brms], complicated models must be programmed with raw Stan code, which can be intensive. This section describes instances where the model _as programmed in Stan_ departs from the model _as written_ above. Although these alterations do not change the theoretical statistical properties of the model, they improve the practical performance of the model by protecting against biased MCMC convergence and floating point arithmetic errors.
<!------- TO DO optimization? ------------------------->
These contributions should be highlighted here because they are crucial to ensuring valid inferences and are substantive improvements on the work by @caughey-warshaw:2015:DGIRT. 

<!------- TO DO ---------
- intro to Bayesian computation?
    - Hamiltonian Monte Carlo
    - log probability 
-->



#### Log-Likelihood for Weighted Data {#sec:logp-accumulator}

One important implementation consideration for the group IRT model is the presence of weighted, non-integer response data. As described in Section&nbsp;\@ref(sec:model-weights), grouped data require reweighting to account for nonrepresentative sample designs and repeated observations within individual members of a group. The resulting data are likely to take non-integer values, which would cause the built-in Binomial likelihood function to fail. Whereas @caughey-warshaw:2015:DGIRT round their data to conform to Stan's Binomial likelihood function, my approach rewrites the likelihood function to accept non-integer data. This allows me to maintain the precision in the underlying data while still correcting the issue at hand.

To explain how this works in Stan, some context on Bayesian computation is helpful. It is usually sufficient for Bayesian estimation with Markov chain Monte Carlo to calculate the posterior density of model parameters only up to a proportionality constant,
\begin{align}
  (\#eq:bayes-prop)
  p(\theta \mid y) &\propto p(y \mid \theta)p(\theta),
\end{align}
where $\theta$ is some generic parameter vector and $y$ represent the observed data. For computational stability, these calculations are done on the log scale.
\begin{align}
  \log p(\theta \mid y) &\propto \log p(y \mid \theta) + \log p(\theta),
  (\#eq:bayes-prop-log)
\end{align}
MCMC algorithms evaluate the right-side of this proportionality at each iteration of the sampler to decide if proposed parameters should be accepted into the sample or rejected.

In the current case, it is the probability of the data $\log p(y \mid \theta)$ that presents a problem. The Binomial log likelihood function as written in Stan will not accept non-integer data, so I rewrite the kernel of the Binomial log likelihood,
\begin{align}
  \log p\left(y \mid \theta\right) \propto s^{*}_{gj} \log \pi_{gj} + \left(n_{gj}^{*} - s^{*}_{gj}\right) \log \left(1 -\pi_{gj}\right)
  (\#eq:weighted-ll-raw)
\end{align}
where the number of trials $n_{gj}^{*}$ and the number of successes $s^{*}_{gj}$ can take non-integer values. This is the same approach that @ghitza-gelman:2013:subgroup-mrp take in a frequentist maximum likelihood context.^[
  They describe this as "simply the weighted log-likelihood approach to fitting generalized linear models with weighted data" [@ghitza-gelman:2013:subgroup-mrp, 765].
]
The results of this function are passed directly to the _log density accumulator_, the variable in Stan that sums the log likelihood and log prior density at every sampler iteration [@carpenter-at-al:2016:stan].

To improve the computational stability of this process, I implement 
Equation&nbsp;\@ref(eq:weighted-ll-raw) 

<!------- TO DO ---------
- come back to this after trying the probit in the code again!
------------------------->


#### Non-Centered Parameterization {#sec:noncentering}

<!-- Non-Centered Parameterization -->
Hierarchical models have posterior distributions whose curvature presents difficulties for sampling algorithms [@betancourt:2015:hamiltonian; @papaspiliopoulos-et-al:2007:parameterizations]. To improve the estimation in Stan,
  <!-- to do: Stan -->
<!-- Flag early on that I'll be setting things up to make things easier in Stan -->
I program the hierarchical models in the "non-centered" rather than the "centered" parameterizations. Whereas the centered parameterization considers $\bar{\theta}_{g}$ as a random draw from its hierarchical distribution (Equation&nbsp;\@ref(eq:theta-g-hlm) above),
the non-centered parameterization considers $\bar{\theta}_{g}$ as a _deterministic_ function of its hypermean regression plus a random variate. 
\begin{align}
  \bar{\theta}_{g} &=
    \mu_{0} + \mathbf{x}^{\intercal}_{d[g]}\beta_{p[g]} + 
    \alpha^{\mathtt{state}}_{s[g]p[g]} + 
    u_{g}\tau_{p[g]}, \\
    u_{g} &\sim \mathrm{Normal}\left(0, 1\right)
  (\#eq:theta-g-noncenter)
\end{align}
where $u_{g}\tau_{p[g]}$ is a group-level error term. It is composed of a standard Normal variate $u_{g}$ and a scalar parameter $\tau_{p}$ that controls the variance of the error term. The non-centered model is _algebraically_ equivalent to centered model, but "factoring out" the hypermean and the error variance from the hierarchical model improves MCMC estimation by de-correlating (or "un-nesting") the parameters that compose the hierarchical distribution. This ameliorates the estimation bias that would result from poor posterior exploration otherwise [@betancourt:2015:hamiltonian].^[
  Stan's Hamiltonian Monte Carlo algorithm generates parameter proposals by proposing a "trajectory" through the parameter space that coasts along the gradient of the distribution. Areas of high curvature in the posterior gradient lead to "divergences" in Stan's HMC algorithm: transitions where the calculated log density diverges substantially from what Stan anticipated when it proposed the transition. Markov chains with many divergent transitions can be severely biased. Non-centered parameterizations are very effective at guarding against divergences in hierarchical models. 
]
This is a crucial extension to the estimation approach developed by @caughey-warshaw:2015:DGIRT, whose model implements a centered parameterization for all of its hierarchical models. 

Equation&nbsp;\@ref(eq:theta-g-noncenter) is an incomplete implementation of the non-centered form; to complete the parameterization, I apply it too all hierarchical components in the regression, including the state and region effects.
\begin{align}
\begin{split}
  \bar{\theta}_{g} &=
    \mu_{0} + 
    \mathbf{x}^{\intercal}_{d[g]}\beta_{p[g]} + 
    u^{\mathtt{group}}_{g}\tau^{\mathtt{group}}_{p[g]} \\
    &\quad +
    \mathbf{z}^{\intercal}_{s[g]}\gamma_{p} +
    u^{\mathtt{state}}_{s[g]p[g]}\tau^{\mathtt{state}}_{p[s]} \\
    &\quad +
    u^{\mathtt{region}}_{r[g]p[g]}\tau^{\mathtt{region}}_{p[g]}
\end{split}
(\#eq:full-noncenter)
\end{align}
The full model places the hypermean regressions and error terms for groups, states, and regions in one deterministic equation. It contains "error terms" for each level of hierarchy---groups, states, and regions---where all parameters are indexed by party. 

<!-- Heteroskedastic model -->
<!-- 
We also have a hierarchical model that predicts the ideal point standard deviation within each group, $\sigma_{g}$. This makes the model "heteroskedastic"---we are modeling the mean ideal point within each group and the variance, conditional on hierarchical covariates. The model for $\sigma_{g}$ in non-centered form is as follows:
\begin{align}
  \log(\sigma_{g}) &= X_{g}\delta_{p} + Z_{s[g]}\eta_{p} + m_{g}\nu + m_{sp}\lambda
  (\#eq:sigma-g-hetero-noncenter)
\end{align}
Where $X_{g}$ and $Z_{sp}$ are the same group- and state-level covariates as the above regression, $\delta_{p}$ and $\eta_{p}$ are party-varying coefficients. The terms $m_{g}\nu$ and $m_{sp}\lambda$ are "factored" error terms for groups and states, where $m_{g}$ and $m_{sp}$ are each distributed $\mathrm{Normal\left(0, 1\right)}$, while $\nu$ and $\lambda$ are scale factors. 
-->


<!------- TO DO ---------
- Read stan files into an appendix?
------------------------->

#### Optimized IRT Model

The matrix expansion trick would go here, if I did it.

<!------- TO DO ---------
- resolve this
------------------------->


### Priors and Identification {#sec:priors-identification}

Latent ideal point models are unidentified without restrictions on the latent space. The polarity of the scale is set by coding all items such that conservative responses are $1$ and liberal responses are $0$. The location of the scale is set by restricting the mean of the $J$ item cutpoints to be $0$. If $\kappa^{*}_{j}$ were the "unscaled" item cutpoint, the cutpoint used in the analysis would be
\begin{align}
  \kappa_{j} &= \kappa^{*}_{j} - \frac{1}{J}\sum\limits_{j = 1}^{J} \kappa^{*}_{j},
  (\#eq:scale-midpoint)
\end{align}
which is performed in every iteration of the sampler. The scale of the latent space is set by restricting the product of the $J$ difficulty parameters to be $1$. Letting $\iota^{*}_{j}$ be the unscaled difficulty parameter...
\begin{align}
\begin{split}
  \iota_{j} &= 
    \iota^{*}_{j} \left(
      \prod\limits_{i = 1}^{J}\iota^{*}_{j} 
    \right)^{\frac{-1}{J}} \\
  &= 
    \iota^{*}_{j} \left(
      \sum\limits_{i = 1}^{J} e^{\log \iota^{*}_{j}} 
    \right)^{\frac{-1}{J}}
\end{split}
(\#eq:scale-difficulty)
\end{align}
The second line shows the calculation on the log scale, which is more computationally robust especially as the number of items increases. Item difficulty is then reparameterized as $\sigma_{j} = \iota_{j}^{-1}$. These restrictions on the item parameters are sufficient to identify $\theta_{g}$.

I specify priors for the unscaled item cutpoints and difficulties that are Normal and LogNormal, respectively. In order to model their joint distribution, I specify a multivariate Normal distribution for the cutpoint and exponentiated difficulty parameter. 
<!------- TO DO ---------
- Come back to This
------------------------->
\begin{align}
  \begin{bmatrix}
    \kappa^{*}_{j} \\
    e^{\iota_{j}^{*}} 
  \end{bmatrix}
  &\sim 
  \mathrm{Normal}\left(\overrightarrow{\bm{\mu}}, \bm{\Sigma}\right)
\end{align}
where $\overrightarrow{\bm{\mu}}$ is a vector of means and $\bm{\Sigma}$ is a variance-covariance matrix. I give independent Normal hyperpriors to the means and an LKJ prior to the variance-covariance matrix [@LKJ:2009:correlation-matrices],
\begin{align}
  \overrightarrow{\bm{\mu}} &\sim 
    \mathrm{Normal}\left( 
      \begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
      \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} 
    \right) \\ 
  \bm{\Sigma} &\sim \mathrm{LKJcorr}(2),
\end{align}
where the LKJ scale parameter of $2$ provides weak regularization against off-diagonal covariance terms, while $\mathrm{LKJcorr(1)}$ would result in a flat prior over the whole matrix. 

Constants and Coefficients for hierarchical regressions are given $\mathrm{Normal}\left(0, 1\right)$ priors. Within-group standard deviations, as well as the scale parameters in the non-centered error terms ($\tau$), are given $\mathrm{LogNormal}\left(0, 1\right)$ priors. 


### Dynamic Model {#sec:dynamic-model}

TBD.

