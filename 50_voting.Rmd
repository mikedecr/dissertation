# District-Party Ideology and Primary Election Outcomes {#ch:voting}

```{r stopper, eval = FALSE, cache = FALSE, include = FALSE}
knitr::knit_exit()
```

```{r knitr-05-1-voting, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

```{r}
library("here")
library("magrittr")
library("tidyverse")
library("broom")
```

## Voting in Primaries

What is the research question

- do more ideological district-parties elect more ideological primary candidates?
    - CF of general election nominee as f(ideology)
        - similar to Clinton (2006)
        - problem: selects on the DV, don't know the alternatives
    - primary winner as f(ideology, CFscore)
        - what's the effect of CFScore on winning (could be nonlinear)
        - how does local ideology change that?
- stratify on primary type?
- stratify in open seats?

What's the plan?

- conditional logit vs. Hainmueller et al. OLS choice?
- interact candidate and group ideal point: how does p(win) change?
- concern: is the CF data shit? 
    - estimate victory among incumbents running for reelection, what's p(win)?
    - is there not enough data?

covariates:

- what is fixed within choice set?
    - ideal points
    - party of incumbent
    - cycle

    
- @thomsen:2014:moderate-candidates
    - moderates less likely to run the less they "fit" the party (farther from CFscore median)
    - ideological positioning in the form of running vs. not.
    - _we don't see variation because of a self-selection process?_
    - candidates are compared to the party, not to partisan voters (district measures of voters are cross-party)
- @porter-treul:2020:primary-experience
    - increasing number of inexperienced candidates running in primaries
    - not an obvious pattern about the ideological extremity of inexperienced candidates, not always more extreme than experienced incumbents.
    - relation weakens because inexperienced candidates aren't great at positioning themselves (but maybe incumbents are better?)
    - when inexperienced candidates win, they tend to be more ideologically extreme than experienced candidates win
- @thomsen:2020:ideology-gender 
    - no (significant) evidence that ideology–victory relationship is different for men and women (but the interaction pt estimates are comparable in size to main effects)
    - Dem women are more liberal than men, slight advantage in primaries


    

## Empirical Approach





### Modeling Candidate Choice

Explain conditional logit

data <- probability <- utility

\begin{align}
\begin{split}
  y_{ir} &\sim \text{Bernoulli}\left(\psi_{ij} \right) \\
  \psi_{ij} &= \frac{\text{exp}\left(u_{ij}\right) }{\sum\limits_{i \in j}u_{ij}}
\end{split}
(\#eq:clogit-likelihood)
\end{align}
  <!-- -->


### Causal and statistical identifiability


DAG: theta -> candidate features

Can't measure direct effect of theta but through candidate features

We don't want it anyway...we want to know how candidate positioning matters given theta?

Model: p(effect of CF | theta = h)

CATE: p(effect | theta)p(theta) 


### Modeling continuous causal heterogeneity

Utility is single-peaked

Required: some nonlinearity that keeps theta in the equation

- no: random effects conditional logit
- no: continuous interaction
- yes: spline model.

For heterogeneous effects: flexibility is good but overfitting is bad.
This is a long-standing tenet of heterogeneous treatment effects literature:

- Regularized tree models for shrinkage prior.
- Conjoint experiments using $p$-values with multiple test corrections.
- Bayesian models gives us both: shrinkage prior for regularization "shrink toward homogeneity," hierarchical model and posterior samples for simultaneous inference.

Spline models are not the most common in political science, but generally have the same benefits for fitting nonlinear relationships but with more flexible function form (than, say, log transformations) and fewer problematic extrapolation behaviors (than, say, polynomial regression).

Reiterate the likelihood.

\begin{align}
\begin{split}
  y_{ir} &\sim \text{Bernoulli}\left(\psi_{ij} \right) \\
  \psi_{ij} &= \text{softmax}\left(u_{ij}, \mathbf{u}_{j}\right) %_
\end{split}
(\#eq:clogit-again)
\end{align}

Utility is a flexible function of CF score and district-party public ideology, with additional controls for candidate-level features.

\begin{align}
  u_{ij} &= f\left(\text{CF}_{ij}, \bar{\theta}_{g[j]}\right) + C_{ij}\beta
  (\#eq:vague-predictor)
\end{align}

We construct $f()$ as a flexible interactive function.
Nonlinear relationship to candidate ideology, such as symmetric utility loss as ideological distance increases.
However, the two ideal points are not on the same scale.
We set up a flexible interactive function $f$ that estimates a common mapping between ideal point scales and measures candidate and district locations in that space affect choice utility.

Begin with a simple assumption that one ideal point space is a linear transformation of the other space.
This is similar to Aldrich–McKelvey.
Construct a new measure $\Delta_{ij}$ that is a linear combination of $\text{CF}_{ij}$ and $\bar{\theta}_{g}$.
\begin{align}
\begin{split} 
  \Delta_{ij} &= \alpha_{1} \text{CF}_{ij} + \alpha_{2}\bar{\theta}_{g[j]} \\
  1 &= \alpha_{1}^{2} + \alpha_{2}^{2}
\end{split}
(\#eq:linear-combo)
\end{align}
The second line restricts the coefficients to have a norm of $1$. 
This places an identifiability restriction on the location and scale of the $\Delta$ space.
Furthermore it enables a direct mapping between $\text{CF}$ space and $\bar{\theta}_{g}$ space, since we can define $\alpha_{2}$ in terms of $\alpha_{1}$...
\begin{align}
\begin{split}
  1 &= \alpha_{1}^{2} + \alpha_{2}^{2} \\
  \alpha_{2} &= \sqrt{(1 - \alpha_{1}^{2})}
\end{split}
\end{align}

So ultimately we're estimating only one effective parameter: the scale factor between the two spaces.

With the linear transformation, we can interpret $\Delta_{ij}$ algebraically as a distance measure between the candidate's CF score and the district-party public ideology in the shared ideology space.
This is possible because even though $\Delta_{ij}$ is the sum $\alpha_{1} \text{CF}_{ij} + \alpha_{2}\bar{\theta}_{g[j]}$, we can construe it as a distance if we simply flip the sign of $\alpha_{2}$, which still satisfies the unit norm.

We then use $\Delta_{ij}$ to construct a set of basis functions, using $4$^th^ order bases with $30$ knots across the range of $\Delta_{ij}$.
Let $b_{k}(\Delta_{i})$ be the $k$^th^ out of $K$ total basis functions, each with a coefficient $\phi_{k}$.
The function $f$ is then just a spline function

\begin{align}
\begin{split}
  f\left(\text{CF}_{ij}, \bar{\theta}_{g[j]}\right) %_
    &= \sum\limits_{k} b_{k}\left(\Delta_{ij}\right)\phi_{k}
\end{split}
(\#eq:spline-function)
\end{align}

In order to penalize an overly flexible spline function, I add a hierarchical prior to the spline coefficients that shrinks coefficients toward zero and regularizes against overfitting.
Each coefficient $\phi_{k}$ is the product of a standard normal variate $\tilde{\phi}_{k}$ and a scale factor $\sigma$, the latter of which has a half-Cauchy prior.
\begin{align}
\begin{split}
  \phi_{k} &= \tilde{\phi}_{k}\sigma \\ %_ 
  \tilde{\phi}_{k} &\sim \text{Normal}\left(0, 1\right) \\ %_ 
  \sigma &\sim \text{Half-Cauchy}\left(0, 1\right) %_
\end{split}
(\#eq:spline-shrinkage)
\end{align}
The marginal prior distribution for $\phi_{k}$ then is a scale mixture of Normal distributions, which pulls coefficients toward zero but has flat tails to allow strong enough data to overcome the prior.

```{r, include = TRUE, echo = FALSE, warning = FALSE}
tibble(
  raw = rnorm(10000),
  sigma = abs(rcauchy(10000)),
  phi = raw*sigma,
) %>%
ggplot() +
  aes(x = phi) +
  geom_histogram(boundary = 0, bins = 500) +
  xlim(c(-10, 10))
```

\begin{align}
  \beta &\sim \text{Normal}\left(0, 1\right)
  (\#eq:clogit-wt-priors)
\end{align}




## Findings



## Discussion

### Other nonparametric Bayes approaches for flexible causal inference

Matching under measurement error in the covariates: <https://amstat.tandfonline.com/doi/full/10.1080/01621459.2015.1122601?casa_token=an-4oMYQN1AAAAAA%3AcmZp5oIxAb5Hs2kkR53JgHrrSXOd1HrLYDwQiuXOw_BaJVWcgG7ADtFcSQrIUQ2qRJTT999k4D-n>

### Causal inference with latent variables and measurement models

latent confounders with proxy measures:
<http://papers.nips.cc/paper/7223-causal-effect-inference-with-deep-latent-variable-models.pdf>

