# Bayesian Approach to Causal Inference in Political Science {#ch:causality}

<!------- TO DO ---------
- This might need to be chapter 2!!!! 
- The model has a discussion of priors that might feel out of place
  without first couching it in a viewpoint of Bayes for the project.
- Is there only one viewpoint? No? 
  The model is a "pragmatic Bayes" (MCMC for fitting, structural priors),
  whereas the causal inf stuff is mixed in its view.
  Sometimes its "normalizing Bayes" (information for sensitivity testing),
  other times is merely "structural" WIPs. 
  Maybe one thing to articulate (to myself, to others) is a position on how
  structural priors make us think about properties of Bayes estimators.
  (in the RDD case, the OLS model is consistent but we aren't in asymptopia.
   How else do we evaluate the properties of the Bayes estimator
   when we include structural information?)
------------------------->

I'm new!


```{r knitr-03-causality, include = FALSE, cache = FALSE}
source(here::here("assets-bookdown", "knitr-helpers.R"))
```

```{r r-03-causality, cache = FALSE}
library("knitr")
library("here")
library("magrittr")
library("tidyverse")
library("extrafont")
library("latex2exp")
library("scales")
library("patchwork")
```


## General Orientation Toward Causal Inference

Taboo against explicit causal inference, (Grosz, Rohrer, Thoemmes)

Whatever the Hernán (2018) article says

The point of causal inference isn't to shame observational studies away from making causal claims. The point is to clarify which designs and assumptions enable certain causal claims and which do not.

Strictly speaking, causality is never observed. It is only inferred with the help of assumptions.


## The Meaning of Parameters in Design and Estimation

What was I trying to say here? 

The causal parameter is a feature of the _causal model_. The causal model is the model if you knew everything. When we impose assumptions for estimating, we can say under which conditions this parameter (or an average of the parameter in a finite sample) can be _estimated_. However, the estimator for the parameter is a different thing.

Bayesian estimation is a different _estimation_ framework given the same causal model. This is easy to see when we think about MLE models as special cases of Bayesian models with all log-likelihoods equally weighted. We have parameters given by some "ideal" model, but the parameters are estimated with a pragmatic penalty term.

Oganisian and Roy (2020) "identification" assumptions vs. "statistical" assumptions. Identification assumptions get you to a place where you can express causal effects in terms of expectations about $Y$ given treatment and confounders, integrating over confounders. "Statistical" assumptions define how we think we can build $E[Y]$

- Bayesian inference _begins_ as technology for thinking about statistical assumptions.
- It eventually becomes technology for experimenting with identification assumptions.

Do battle with the "implied nonparametric estimator" framework.

- Causal models are manipulated to express causal _estimands_.
- the "nonparametric estimators" are usually just averages of treatment and control group outcomes, under ignorability assumptions.
- Parametric models for estimands are usually a byproduct of thinking within an MLE/regression framework. Many causal inf techniques don't do that. Instead they try to simply predict E[y(1) - y(0)] and don't care about the interpretability of the other RHS terms.
- Thinking outside the "typical econometric" framework it's actually kinda easy to see how one flavor of fitting Bayesian models for causal effects. If the technology for prediction is Bayesian, you just get the prediction and the posterior distribution. You then deal with the bias/variance properties of $\hat{y}$ or $\tilde{y}$ (unobserved counterfactuals) but at least your focus is on the predictions rather than coefficients (which, from this view, who cares about those?)

We can think bigger about Bayesian _inference_ for a parameter as distinct from Bayesian _estimation_ of the in-sample quantity. 
This lets us use a nonparametric data-driven estimator for the data, but the "inference" or "generalization" still has a prior.
For instance a sample mean estimates a population mean without a likelihood model for the data, but inference about the population mean often follows a parametric assumption from the Central Limit Theorem that the sampling distribution from the mean is asymptotically normal (but doesn't have to, c.f. bootstrapping).
<!------- TO DO ---------
- Read more about sharp null and an RI framework for inference, 
  since this runs up against the idea of asymptotic normality of the means.
------------------------->
Even if the point estimator we use for a mean is unbiased, we can assimilate external information during the interpretation of the estimator (biasing the inference without biasing the point estimator).
Restated: the posterior distribution is a weighted average of the raw point estimator and external information, rather than biasing the data-driven estimate directly.

Even bigger: Bayesian inference about _models_ (Baldi and Shahbaba 2019). 
This is probably where I have to start my justification for this? 
The _entire point_ of causal inference is to make inferences about counterfactuals given data (Rubin 1978?).
Invoking Bayesian inference is really the only way to say what we _want_ to say about causal effects: what are the plausible causal effects given the model/data. We do _not_ care about the plausibility of data given the null (as a primary QOI). 
- Probably want to use Harrell-esque language? Draw on intuition from clinical research, or even industry. We want our best answer, not a philosophically indirect weird jumble.
- This probably also plays into the Cox/Jeffreys/Jaynes stuff I have open on my computer.
<!------- TO DO ---------
- Corey Yanofsky recommended reading
- Gelman/Shalizi philosophy of Bayes
- Induction/Deduction in Bayes
- etc.
- symposium about "objective" Bayes
- And then also Gelman/Simpson/Betancourt "context of the likelihood"
  => we have priors about the WORLD, not about model parameters.
------------------------->

<!------- TO DO ---------
- soul searching about a philosophy of Bayes to stake out for the project?
- "pragmatic bayesian workflow" - priors are "just a part of the model" that need to be checked w/ prior predictive checking and so on.
- vs. the "philosophical coherence" view.
- damnit don't spend too much time on this because it just needs to be an orientation to stick decisions to.
- We're already leaning in the "pragmatic" direction from the IRT model text.
------------------------->

This presumes an m-closed world(?right?), which maybe we don't like (Navarro, "Devil and Deep Blue Sea").
<!------- TO DO ---------
- Read Navarro CLOSELY!
- Read ARONOW and MILLER: how to square "intra-model agonisticism" with 
  some philosophical view of what Bayes is.
- Maybe this is terminology I should work on! Draw some helpful lines that 
  don't already exist for defining subtypes of agnosticism or other
  epistemic bundles of things.
  (there is also some paper on "types of Bayesians"?
   mentioned in a McElreath talk)
------------------------->
Me debating with myself: how to think about Bayesian model selection vs "doubly robust" estimation ideas...
Maybe some hybrid view in the "quasi-experimental" approach to causal model selection. We estimate two models from the same data—one with a treatment parameter and one where we impose no treatment effect—and compare models using some likelihoodist or Bayes factor measure of evidence.
<!------- TO DO ---------
 - articulate a good beef with the paper's terminology of "quasi-experimental"
 -------------------------> 

What is THIS project going to do?

- Pragmatic view of priors?
- Are we doing more flexible covariate adjustment?
- Maybe we should decide this AFTER we experiment with Ch 4 and 5 data/methods
<!------- TO DO ---------
- pronouncement of project values
------------------------->



## Rubin's Vision: Posterior Distributions for Missing Potential Outcomes

Causal model: $y_{i}(1)$ and $y_{i}(0)$

Observed data: $y_{i} = z_{i}y_{i}(1) + (1 - z_{i})y_{i}(0)$

Unobserved data: $\tilde{y}_{i}$

Inference about $\tilde{y}_{i}$: $p(\tilde{y}_{i} \mid \mathbf{y}) \propto p(\tilde{y}_{i} \mid \theta, \mathbf{y})p(\theta)$


## The Issue of Priors

Various roles that priors take on

- merely facilitate posterior inference
- structural information
- weak information
- regularization / stabilization
- prior knowledge

A general orientation toward priors in this dissertation:

- Not about "stacking the deck" or hazy notions of "prior beliefs"
- inference about the thing we care about
- structural information when we have it
- Causal inference: "agnosticism" is something valuable generally


### Priors and Likelihoods







### Likelihood Parameterization and the Incoherence of Flatness


Gelman "Parameterization" paper: parameterization for convenience often reveals a new family of models. For Bayesian inference, it reveals a new family of priors. For _causal_ inference, we should pay careful attention to the way parameterization bears on the priors that we do and do not feel comfortable using, and the way that parameterization presents opportunities to improve on other work.


Priors on the _world_, not about parameters.


Parameters are themselves a _choice_. 

- parameterization exposes some structures that are easier to specify priors for than others


The incoherence of flatness: 

- "no way to conceptualize an uninformative prior because you can always rearrange the problem through a reparameterization or transforamtion of a parameter"
- examples of transformations having crazy implications/MLE being wild (logit).
- Jeffreys prior: actually a very limited range of priors that satisfy an "invariance" property. My words: such that the "amount of information obtained from data about is invariant to parameterization of the likelihood, for all possible values of the parameter," or, "the only way for the posterior distribution to be exactly the same, given the same data, for all true parameter values (?), is the Jeffreys prior," or, regardless of the data, I will learn the same thing about the generative model regardless of which equivalent parameterization of the generative model is used.  
    - is it worth it to think about the theoretical meaning of information
    - how does flatness reflect information in nonlinear scales?



Suppose we have some posterior distribution which relies on some parameter vector $\overrightarrow{\alpha}$. 
\begin{align}
  p\left(\overrightarrow{\alpha} \mid y\right) &\propto 
    p(y \mid \overrightarrow{\alpha} )
    p( \overrightarrow{\alpha} )
\end{align}
Consider some alternate parameterization of the likelihood parameterized by $\overrightarrow{\beta}$. 

Nonlinear transformation of $\theta$ does not preserve a uniform density over parameters.

```{r diff-means-example}
means_data <- 
  tibble(
    x = seq(-1, 1, .05),
    unif = ifelse(x >= 0, dunif(x, 0, 1), NA),
    triangle = 
      ifelse(
        x < 0, 
        (2 * (x - -1)) / ((1 - -1) * (0 - -1)), 
        (2 * (1 - x)) / ((1 - -1) * (1 - 0))
      )
  ) 

plot_prior <- function(data, x, y) {
  ggplot(data) +
  aes(x = {{x}}, y = {{y}}) +
  geom_ribbon(
    aes(ymin = 0, ymax = {{y}}),
    fill = primary_light
  ) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1.5))
}
```

```{r plot-diff-means-example}
plot_prior(data = means_data, x = x, y = unif) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(
    x = TeX("Group Mean: $\\mu_{z}$"),
    y = "Prior Density",
    title = "Prior Densities for Difference in Means",
    subtitle = "If means have Uniform(0, 1) priors"
  ) +
plot_prior(data = means_data, x = x, y = triangle) +
  labs(x = TeX("Treatment Effect: $\\mu_{1} - \\mu_{0}$")) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) 
```

For instance, consider a simple experiment with a binary outcome variable $y_{i}$ and binary treatment assignment $z_{i} \in \{0, 1\}$. Suppose that the treatment effect of interest is a difference-in-means, $\bar{y}_{z = 1} - \bar{y}_{z = 0}$, estimated from a linear probability model. This linear probability model might be parameterized in two ways. First is a conventional regression setup,
\begin{align}
  y_{i} &= \alpha + \beta z_{i} + \epsilon_{i}
  (\#eq:diff-means-example)
\end{align}
where $\alpha$ is the control group mean, $\alpha + \beta$ is the treatment group mean, $\beta$ represents the difference in means, and $\epsilon_{i}$ is a symmetric error term for unit $i$. With the model parameterized in this way, the researcher must specify priors for $\alpha$ and $\beta$. Suppose that the researcher gives $\beta$ a flat prior to represent ignorance about the treatment effect. An equivalent _likelihood model_ for the data would be to treat each observation as a function of its group mean $\mu_{z}$.
\begin{align}
  y_{i} &= z_{i}\mu_{1} + \left(1 - z_{i}\right)\mu_{0}  + \epsilon_{i}
  (\#eq:separate-means-example)
\end{align}
Although the treatment effect $\beta$ from Equation&nbsp;\@ref(eq:diff-means-example) is equivalent to the difference in means $\mu_{1} - \mu_{0}$ from Equation&nbsp;\@ref(eq:separate-means-example), the parameterization of the model affects the implied prior for the difference in means. If the researcher gives a flat prior to both $\mu_{\mathbf{z}}$ terms, the implied prior for the difference in means will not be flat. Instead, it will be triangular, as shown in Figure&nbsp;\@ref(fig:plot-diff-means-example). The underlying mechanics of this problem are well-known in applied statistics---if we continue adding parameters, the Central Limit Theorem describes how the resulting distribution will converge to Normality---but it takes the explicit specification of priors to shine a light on the consequences of default prior choices in a particular case. In particular it shows how even flat priors, which are popularly regarded as "agnostic" priors because of their implicit connection to maximum likelihood estimators, do not necessarily imply flat priors about the researcher's key quantities of interest. Rather, flat priors can create a variety of unintended prior distributions that do not match the researcher's expectations. I return to this important idea in the discussion about setting priors for a probit model in Section&nbsp;\@ref(sec:probit-prior).
<!------- TO DO ---------
- probit in the model chapter?
- logit in this chapter?
------------------------->

```{r plot-diff-means-example, include = TRUE, fig.height = 4, fig.width = 8, out.width = "100%", fig.cap = "Model parameterization affects prior distributions for quantities of interest that are functions of multiple parameters or transformed parameters. The left panel shows that the difference between two means does not have a flat prior if the two means are given flat priors. Note that the $x$-axes are not fixed across panels."}
```

- equivalent parameterizations





### Structural Priors and Weak Information

Bounds, Hierarchy




### Direct Counterfactual Comparisons

ML approach

More of a "priors to facilitate posteriors" approach, not a prior information view (since there is some formal data peeking involved)




## Examples of Bayesian Causal Inference

### Regression Discontinuity with a Binary Outcome

Parameterization, structural information

### Meta-Analysis of Nonparametric Treatment Effects







